<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Workspace of LionHeart]]></title>
  <link href="http://lionheartwang.github.io/atom.xml" rel="self"/>
  <link href="http://lionheartwang.github.io/"/>
  <updated>2018-05-17T16:32:32+08:00</updated>
  <id>http://lionheartwang.github.io/</id>
  <author>
    <name><![CDATA[Wang Yiguang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tensorflow基础环境安装]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/05/17/tensorflowji-chu-huan-jing-an-zhuang/"/>
    <updated>2018-05-17T16:24:53+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/05/17/tensorflowji-chu-huan-jing-an-zhuang</id>
    <content type="html"><![CDATA[<p>本文介绍如何搭建tensorflow的基础运行环境，包括Cuda、Cudnn与tensorflow gpu版本的安装。</p>

<!-- More -->


<h2>安装Cuda</h2>

<p>在nvida官网寻找Cuda rpm安装包：</p>

<ul>
<li> <a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a></li>
</ul>


<p>选择对应版本和系统后下载。</p>

<p>以Cuda 9.0为例，下载后的安装包为：</p>

<ul>
<li>cuda-repo-rhel7-9-0-local-9.0.176-1.x86_64.rpm</li>
</ul>


<p>执行如下命令安装：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo rpm -i cuda-repo-rhel7-9-0-local-9.0.176-1.x86_64.rpm
</span><span class='line'>sudo yum clean all
</span><span class='line'>sudo yum install cuda
</span></code></pre></td></tr></table></div></figure>


<p>安装nvida-driver：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo yum install -y nvidia-kmod cuda-drivers
</span></code></pre></td></tr></table></div></figure>


<p>重新加载nvidia相关模块：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo rmmod nvidia-uvm
</span><span class='line'>sudo rmmod nvidia
</span><span class='line'>sudo nvidia-modprobe -u -c<span class="o">=</span>0
</span></code></pre></td></tr></table></div></figure>


<p>检验新的驱动是否生效，运行</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>nvidia-smi
</span></code></pre></td></tr></table></div></figure>


<p>应当输出本机GPU信息。</p>

<h2>安装Cudnn</h2>

<p>在nvidia官网寻找Cudnn安装包：</p>

<ul>
<li><a href="https://developer.nvidia.com/rdp/cudnn-download">https://developer.nvidia.com/rdp/cudnn-download</a></li>
</ul>


<p>根据Cuda版本与系统选择对应版本下载。</p>

<p>以Cudnn 7.0为例，下载后的安装包为：</p>

<ul>
<li>cudnn-9.0-linux-x64-v7.tgz</li>
</ul>


<p>解压安装操作如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>tar zxvf cudnn-9.0-linux-x64-v7.tgz
</span><span class='line'>sudo cp cuda/include/cudnn.h /usr/local/cuda/include
</span><span class='line'>sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
</span><span class='line'>sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
</span></code></pre></td></tr></table></div></figure>


<p>设置环境变量：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda/lib64:<span class="nv">$LD_LIBRARY_PATH</span>
</span></code></pre></td></tr></table></div></figure>


<h2>安装Tensorflow</h2>

<p>通过pip安装GPU版本的tensorflow。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pip install tensorflow-gpu
</span></code></pre></td></tr></table></div></figure>


<p>在python中测试tensorflow是否能够正常加载。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="kn">as</span> <span class="nn">tf</span>
</span><span class='line'><span class="k">print</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span>
</span></code></pre></td></tr></table></div></figure>


<p>目前最新版本为1.8.0</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tensorflow架构介绍]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/03/25/tensorflowjia-gou-jie-shao/"/>
    <updated>2018-03-25T17:43:48+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/03/25/tensorflowjia-gou-jie-shao</id>
    <content type="html"><![CDATA[<p>Tensorflow的架构为大规模分布式训练和预测设计，单页为实验新的机器学习模型和进行系统层面上的优化提供了足够多的灵活性。</p>

<p>本文介绍Tensorflow的系统架构设计，展示Tensorflow如何将灵活性与扩展性结合。</p>

<p>本文内容主要从Tensorflow官方文档中翻译整理而得。</p>

<ul>
<li><a href="https://www.tensorflow.org/extend/architecture">https://www.tensorflow.org/extend/architecture</a></li>
</ul>


<p>阅读本文之前需要了解Tenflow变成中像计算流图、算子和session的概念。如果不了解可以参考如下文档：</p>

<ul>
<li><a href="https://www.tensorflow.org/programmers_guide/low_level_intro">https://www.tensorflow.org/programmers_guide/low_level_intro</a></li>
</ul>


<!-- More -->


<h2>概述</h2>

<p>Tensorflow的运行时是一个跨平台的库，下图展示了整体结构。用户级别的代码与核心的运行时的代码通过一套C API分隔开。</p>

<p><img src="http://lionheartwang.github.io/images/blog/43-tensorflow-layers.png"></p>

<p>本文主要介绍如下几层：</p>

<ul>
<li>Client:

<ul>
<li>将计算定义为数据流图的形式。</li>
<li>通过一个session初始化图的执行。</li>
</ul>
</li>
<li>Distributed Master

<ul>
<li>根据Session.run()传递的参数将图裁剪为子图。</li>
<li>将子图拆解为多个部分用于分发到不通的进程和设备上。</li>
<li>分发子图分片到Worker Services</li>
<li>初始化Woker Service待执行的子图分片计算。</li>
</ul>
</li>
<li>Worker Services (每个task一个)

<ul>
<li>调度子图算子的执行，这些算子会调用对应于可用硬件（CPU\GPU等）的kernal实现。</li>
<li>发送算子计算结果以及接收来自其他Worker Service的计算结果。</li>
</ul>
</li>
<li>Kernel Implementations

<ul>
<li>负责实现每个图算子的执行。</li>
</ul>
</li>
</ul>


<p>下图说明了这些组件是如何交互的。</p>

<p><img src="http://lionheartwang.github.io/images/blog/44-tensorflow-components-interact.svg"></p>

<p>解释如下：</p>

<ul>
<li>&ldquo;/job:worker/task:0&rdquo; 和 &ldquo;/job:ps/task:0&rdquo; 都是worker services上的task。</li>
<li>“PS”代表parameter server，是负责存储和更新模型参数的task。其他的task会将训练过程中优化的参数更新发送给parameter server。</li>
<li>这样的职能划分并非必须的，但在分布式训练场景中很常见。</li>
</ul>


<p>注意</p>

<ul>
<li>Distributed Master和Worker Service只在支持分布式版本的tensorflow中存在。</li>
<li>单进程版本的Tensorflow包含一个特殊的Session实现完成Distributed Master做的全部事情但只在本地进程中进行设备间的通信。</li>
</ul>


<h2>组件介绍</h2>

<p>下面以一个示例图的处理过程介绍TensorFlow核心层的更多实现细节。</p>

<h3>Client</h3>

<p>用户编写客户端tensorflow程序来构建计算流图。</p>

<p>客户端程序既可以直接调用各种算子操作组合而成，也可以调用更方便的API库如Estimator等直接构建神经网络和其他高层抽象实现。</p>

<p>Tensorflow支持多种客户端语言，主流的是Python和C++，因为Google的内部使用者对这两种语言更熟悉一些。</p>

<p>大多数训练库仍为Python实现，但C++更高效。</p>

<p>Client会创建一个session，将图的定义以<code>tf.GraphDef</code>这种protobuffer的形式发送给Distributed Master节点。</p>

<p>当client对流图中的一个或多个节点进行求值时，会触发调用让Distributed Master进行计算的初始化操作。</p>

<p>在下图中，Client构建了一个图将权值w附加到一个特征向量x并增加了偏移b，然后在变量中存储计算结果。</p>

<p><img src="http://lionheartwang.github.io/images/blog/45-tensorflow-graph_client.svg"></p>

<p> tf.Session相关代码：</p>

<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/Session">https://www.tensorflow.org/api_docs/python/tf/Session</a></li>
</ul>


<h3>Distributed Master</h3>

<p>Distributed Master负责以下工作：</p>

<ul>
<li>对图进行裁剪来获取满足Client对节点求值必须计算的子图。</li>
<li>将图进行拆分得到每个参与计算的设备负责的图分片。</li>
<li>缓存这些拆分的图分片，以备后续步骤中被重新使用。</li>
</ul>


<p>master节点能够看到每步计算的整体情况，因此可以对计算流程进行一些标准的优化，例如公共子表达式求值、常量折叠等。</p>

<p>随后master会负责协调一系列的task执行来计算优化后的子图。</p>

<p><img src="http://lionheartwang.github.io/images/blog/46-tensorflow-graph_master_cln.svg"></p>

<p>下图展示了示例graph的一种可能的分发方案。Distributed Master将模型参数进行分组从而在Parameter Server中将他们存储在一起。</p>

<p><img src="http://lionheartwang.github.io/images/blog/47-tensorflow-graph_split1.svg"></p>

<p>在图的边被切分的分片中，Distributed Master会插入send和receive节点来在不同的task间传递信息。</p>

<p><img src="http://lionheartwang.github.io/images/blog/48-tensorflow-graph_split2.svg"></p>

<p> 随后Distributed Master将图的分片分发给不同节点上的task。</p>

<p> <img src="http://lionheartwang.github.io/images/blog/50-tensorflow-graph_send_recv.svg"></p>

<p>相关代码：</p>

<ul>
<li>MasterService API 定义：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/protobuf/master_service.proto">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/protobuf/master_service.proto</a></li>
<li>Master接口：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/master_interface.h">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/master_interface.h</a></li>
</ul>


<h3>Worker Service</h3>

<p>每个task的Worker Service负责如下工作：</p>

<ul>
<li>处理Distributed Master发来的请求。</li>
<li>调度执行构成本地局部子图的算子的kernels实现。</li>
<li>协调task之间的直接通信。</li>
</ul>


<p>我们优化了Worker Service，支持以较低的开销运行大规模的流图。</p>

<p>我们目前的实现能够支持每秒数以万计的子图执行，这通常能够支持大量副本细粒度地进行训练。</p>

<p>Worker Service将算子的kernel实现分发给设备并尽可能并行化，例如使用多CPU核或者GPU。</p>

<p>我们针对每对源设备和目的设备的组合情况，定制了相应的Send和Recv算子。包括：</p>

<ul>
<li>本地CPU和GPU设备的数据交换使用cudaMemcpyAsync() API来配合计算和数据转移工作。</li>
<li>两个本地GPU使用端到端的DMA技术来避免通过本机CPU进行代价昂贵的数据拷贝操作。</li>
</ul>


<p>对于task之间的数据交换，tensorflow支持多种协议，包括：</p>

<ul>
<li>基于TCP协议的gRPC。</li>
<li>基于融合以太网的RDMA。</li>
</ul>


<p>另外我们也初步支持了NVIDIA为多GPU通信提供的NCCL库。</p>

<p>相关代码：</p>

<ul>
<li>WorkerService API 定义：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/protobuf/worker_service.proto">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/protobuf/worker_service.proto</a></li>
<li>Worker接口：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/worker_interface.h">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/worker_interface.h</a></li>
<li>远程通信相关(Send和Recv算子实现)：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/rpc/rpc_rendezvous_mgr.h">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/rpc/rpc_rendezvous_mgr.h</a></li>
</ul>


<h3>Kernel</h3>

<p>Tensorflow运行时包含了超过200种标准的算子实现，包括数学相关、数组相关、控制流相关以及状态管理相关的算子。</p>

<p>每一种算子在实现上都针对不同的设备种类进行了优化。</p>

<p>其中，许多算子是使用<code>Eigen::Tensor</code>实现的，它基于C++模板生成了高效的能在多核CPU以及GPU上执行的代码。</p>

<p>另一方面，我们也使用想cuDNN等第三方库尽可能地让kernel的实现更加高效。</p>

<p>如果现有算子无法满足需求，用户可以注册自定义的算子实现到kernel中。</p>

<p>相关代码：</p>

<ul>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/framework/op_kernel.h">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/framework/op_kernel.h</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker资源隔离和限制实现原理]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/03/18/dockerzi-yuan-ge-chi-he-xian-zhi-shi-xian-yuan-li/"/>
    <updated>2018-03-18T00:16:49+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/03/18/dockerzi-yuan-ge-chi-he-xian-zhi-shi-xian-yuan-li</id>
    <content type="html"><![CDATA[<p>本文介绍Docker底层资源隔离和限制的相关知识。Docker本质上是宿主机（Linux）上的进程，通过namespace实现资源隔离，通过cgroups实现资源限制，通过写时复用机制(copy-on-write)实现高效的文件操作。</p>

<!--More-->


<h2>Namespace资源隔离机制</h2>

<p>namespace是Linux提供的资源隔离机制。只有在同一个namespace下的进程可以相互联系，但无法感受到外部进程的存在，营造出处于一个独立的系统环境中的错觉，从而实现了隔离。</p>

<p>Linux提供的操作namespace相关接口介绍如下：</p>

<h3>clone()</h3>

<p>clone()是fork()的一种实现方式。调用fork()函数时系统会创建新进程，为其分配资源，并把原来进程中的值复制到新进程中，可通过fpid区分新进程和父进程。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="c1">// fork_ex.c</span>
</span><span class='line'><span class="cp">#include &lt;stdio.h&gt;</span>
</span><span class='line'><span class="cp">#include &lt;unistd.h&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="kt">int</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
</span><span class='line'>    <span class="kt">pid_t</span> <span class="n">fpid</span><span class="p">;</span>
</span><span class='line'>    <span class="kt">int</span> <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'>    <span class="n">fpid</span> <span class="o">=</span> <span class="n">fork</span><span class="p">();</span>
</span><span class='line'>    <span class="k">if</span><span class="p">(</span><span class="n">fpid</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;fork error!&quot;</span><span class="p">);</span>
</span><span class='line'>    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">fpid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;I am child. pid: %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">getpid</span><span class="p">());</span>
</span><span class='line'>    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
</span><span class='line'>        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;I am parent. pid: %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">getpid</span><span class="p">());</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>运行：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>gcc fork-ex.c <span class="o">&amp;&amp;</span> ./a.out
</span></code></pre></td></tr></table></div></figure>


<p>输出：</p>

<blockquote><p>I am parent. pid: 33390<br>
I am child. pid: 33391</p></blockquote>

<p>参数：</p>

<ul>
<li>child_func：程序主函数</li>
<li>child_stack：栈空间</li>
<li>flags：系统调用参数</li>
</ul>


<h3>setns()</h3>

<p>setns()用于加入已存在的namespace</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="kt">int</span> <span class="nf">setns</span><span class="p">(</span><span class="kt">int</span> <span class="n">fd</span><span class="p">,</span> <span class="kt">int</span> <span class="n">nstype</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>参数</p>

<ul>
<li>fd：要加入namespace的文件描述符(指向/proc/[pid]/ns目录)。</li>
<li>nstype：要加入namespace的类型，用于检查，0表示不检查。</li>
</ul>


<h3>unshare()</h3>

<p>unshare()在原进程上进行namespace隔离。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='c'><span class='line'><span class="kt">int</span> <span class="n">unshare</span><span class="p">(</span><span class="kt">int</span> <span class="n">flags</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>unshare()不启动新进程，但是跳出了原来的namespace。</p>

<h2>Namespace 6项隔离介绍</h2>

<p>namespace提供了以下6项隔离：</p>

<table>
<thead>
<tr>
<th> namespace</th>
<th style="text-align:center;"> 系统调用参数 </th>
<th style="text-align:right;"> 隔离内容 </th>
</tr>
</thead>
<tbody>
<tr>
<td> UTS </td>
<td style="text-align:center;"> CLONE_NEWUTS </td>
<td style="text-align:right;"> 主机和域名 </td>
</tr>
<tr>
<td> IPC </td>
<td style="text-align:center;"> CLONE_NEWIPC </td>
<td style="text-align:right;"> 信号量、消息队列和共享内存 </td>
</tr>
<tr>
<td> PID </td>
<td style="text-align:center;"> CLONE_NEWPID </td>
<td style="text-align:right;"> 进程编号 </td>
</tr>
<tr>
<td> Network </td>
<td style="text-align:center;"> CLONE_NEWNET </td>
<td style="text-align:right;"> 网络设备、网络栈、端口等 </td>
</tr>
<tr>
<td> Mount </td>
<td style="text-align:center;"> CLONE_NEWNS </td>
<td style="text-align:right;"> 挂载点(文件系统) </td>
</tr>
<tr>
<td> User </td>
<td style="text-align:center;"> CLONE_NEWUSER </td>
<td style="text-align:right;"> 用户和用户组 </td>
</tr>
</tbody>
</table>


<h3>Unix Time-sharing System(UTS) namespace</h3>

<p>分时系统(Time-sharing System)中一台主机连接了若干个终端，每个终端有一个用户在使用。</p>

<p>用户交互式地向系统提出命令请求，系统接受每个用户的命令，采用时间片轮转方式处理服务请求，并通过交互方式在终端上向用户显示结果。用户根据上步结果发出下道命令。</p>

<p>分时操作系统将CPU的时间划分成若干个片段，称为时间片。操作系统以时间片为单位，轮流为每个终端用户服务。每个用户轮流使用一个时间片而使每个用户并不感到有别的用户存在。</p>

<p><font color=red><b>UTS(Unix Time-sharing System) namespace提供了主机名和域名的隔离，使每个Docker容器可以拥有独立的主机名和域名，在网络上可以视为独立的节点。</b></font></p>

<h3>Unix Interprocess Communication(IPC) namespace</h3>

<p>Linux进程间通信由System V IPC，基于Socket的IPC和POSIX IPC发展而来，主要的通信手段有:</p>

<ul>
<li>管道(Pipe)

<ul>
<li>无名管道
  -只能用于具有亲缘关系的进程之间的通信（父子进程或者兄弟进程之间）

<ul>
<li>半双工的通信模式，具有固定的读端和写端</li>
<li>是一种特殊的文件，不属于其他任何文件系统并且只存在于内存中</li>
</ul>
</li>
<li>有名管道(FIFO)

<ul>
<li>使互不相关的两个进程间实现彼此通信</li>
<li>可以通过路径名来指出，并且在文件系统中是可见的。在建立了管道之后，两个进程就可以把它当做普通文件一样进行读写操作</li>
<li>先进先出规则</li>
</ul>
</li>
</ul>
</li>
<li>信号(Signal)

<ul>
<li>用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身</li>
</ul>
</li>
<li>消息队列(Message Queue)

<ul>
<li>有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。</li>
<li>克服了信号承载信息量少，管道只能承载无格式字节流（要求管道的读出方和写入方必须事先约定好数据的格式，比如多少字节算作一个消息等）以及缓冲区大小受限等缺点。</li>
</ul>
</li>
<li>共享内存

<ul>
<li>使得多个进程可以访问同一块内存空间。</li>
</ul>
</li>
<li>信号量(semaphore)

<ul>
<li>主要用于进程间以及同一进程不同线程之间的同步。</li>
</ul>
</li>
<li>套接字(Socket)

<ul>
<li>可用于不同机器之间的进程间通信，必须包含

<ul>
<li>地址，由 ip 与 端口组成，像192.168.0.1:80。</li>
<li>协议，socket 所用的传输协议 TCP、UDP、raw IP。</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>IPC资源包括信号量、消息队列和共享内存。</p>

<p>IPC namespace中包含系统IPC标识符以及实现POSIX消息队列的文件系统。</p>

<p>同一个IPC namespace下的进程彼此可见，不同IPC namespace下的进程互相不可见。</p>

<p><font color=red><b>通过IPC namespace可以实现容器与宿主机、容器与容器之间的IPC隔离。 </b></font></p>

<h3>Process identifier(PID) namespace</h3>

<p>PID是大多数操作系统的内核用于唯一标识进程的一个数值。</p>

<p>这一数值可以作为许多函数调用的参数，以使调整进程优先级、杀死进程之类的进程控制行为成为可能。</p>

<p>PID为1的进程是init，作为所有进程的父进程，不会处理来自其他进程的信号（信号屏蔽），并维护一张进程表，当有子进程变成孤儿时会回收其资源并结束进程。</p>

<p><font color=red><b>PID namespace隔离对进程PID重新编号，两个不同namespace下的进程没有关系，因此PID也可以相同。内核为所有的PID namespace维护了一个树状结构。</b></font></p>

<p><img src="http://lionheartwang.github.io/images/blog/42-pidtree.png"></p>

<p>其中：</p>

<ul>
<li>每个PID namespace中的第一个进程拥有特权。</li>
<li>一个namespace中的进程不能影响父节点或兄弟节点namespace中的进程。</li>
<li>root namespace中可以看到所有的进程，包括所有后代节点中的namespace。</li>
<li>在外部可以通过监控Docker daemon所在的PID namespace中的所有进程和子进程来实现对Docker中运行的程序的监控。</li>
</ul>


<h3>Mount namespace</h3>

<p><font color=red><b>mount namespace 通过隔离文件系统挂载点隔离文件系统，它是第一个Linux namespace，标识位为CLONE_NEWNS。隔离之后不同的mount namespace中的文件结构互不影响。</b></font></p>

<p>可以通过/proc/[pid]/mounts查看所有挂载在当前namespace中的文件系统。进程创建mount namespace时把当前文件结构复制给新的namespace。</p>

<p>挂载传播(mount propagation)定义了挂载对象之间的关系，解决了文件结构复制过程中子节点namespace影响父节点namespace文件系统的问题。</p>

<ul>
<li>共享关系(share relationship)：存在挂载关系的两个挂载对象中的事件会双向传播</li>
<li>从属关系(slave relationship)：挂载对象中的事件只能按指向从属对象的方向传播(共享挂载—>从属挂载)</li>
</ul>


<h3>Network namespace</h3>

<p><font color=red><b>network namespace提供了关于网络资源的隔离，包括网络设备、IPv4和IPv6协议栈、IP路由表、防火墙、/proc/net目录、/sys/class/net目录、套接字(socket)等。</b></font></p>

<p>注意：</p>

<ul>
<li>一个物理的网络设备最多存在于一个network namespace中</li>
<li>可以通过veth pair在不同的network namespace中创建通道进行通信。</li>
<li>一般情况下，物理网络设备都分配在最初的root namespace中。</li>
</ul>


<h3>User namespace</h3>

<p><font color=red>user namespace主要隔离了安全相关的标识符和属性(用户ID、用户组ID、root目录、key(密钥)、特殊权限)。</b></font></p>

<p>因此用clone()创建的新进程在新的user namespace中可以拥有不同的用户和用户组，在新进程创建的容器中，它是超级用户，但在容器之外只是普通用户。</p>

<p>Linux中，特权用户的user ID是0，user ID非0的进程启动user namespace后user ID可以变为0</p>

<h2>cgroups资源限制机制</h2>

<p>cgroups是Linux内核提供的一种机制。</p>

<p>这种机制可以根据需求把一系列系统任务及其子任务整合(或分隔)到按资源划分等级的不同组内，限制了被namespace隔离起来的资源，并为资源设置权重、计算使用量(CPU, Memory, IO等)、操控任务启停等。</p>

<p>从而为系统资源管理提供了一个统一的框架。</p>

<h3>cgroup功能</h3>

<p>cgroup提供的主要功能如下：</p>

<ul>
<li>资源限制：限制任务使用的资源总额，并在超过这个配额时发出提示</li>
<li>优先级分配：分配CPU时间片数量及磁盘IO带宽大小、控制任务运行的优先级</li>
<li>资源统计：统计系统资源使用量，如CPU使用时长、内存用量等</li>
<li>任务控制：对任务执行挂起、恢复等操作</li>
</ul>


<h3>cgroup子系统</h3>

<p>cgroups的资源控制系统，每种子系统独立地控制一种资源。</p>

<ul>
<li>cpu：使用调度程序控制任务对CPU的使用。</li>
<li>cpuacct(CPU Accounting)：自动生成cgroup中任务对CPU资源使用情况的报告。</li>
<li>cpuset：为cgroup中的任务分配独立的CPU(多处理器系统时)和内存。</li>
<li>devices：开启或关闭cgroup中任务对设备的访问</li>
<li>freezer：挂起或恢复cgroup中的任务</li>
<li>memory：设定cgroup中任务对内存使用量的限定，并生成这些任务对内存资源使用情况的报告</li>
<li>perf(Linux CPU性能探测器)_event：使cgroup中的任务可以进行统一的性能测试</li>
<li>net_cls(Docker未使用)：通过等级识别符标记网络数据包，从而允许Linux流量监控程序(Traffic Controller)识别从具体cgroup中生成的数据包</li>
</ul>


<h2>参考资料</h2>

<p>本文参考如下博文整理：</p>

<ul>
<li><a href="https://kasheemlew.github.io/2017/05/18/docker-linux/">https://kasheemlew.github.io/2017/05/18/docker-linux/</a></li>
</ul>


<p>namespace的详细介绍可以参考：</p>

<ul>
<li><a href="http://man7.org/linux/man-pages/man7/namespaces.7.html">http://man7.org/linux/man-pages/man7/namespaces.7.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Shuffle工作原理详解]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/03/11/spark-shuffle-implementation/"/>
    <updated>2018-03-11T00:34:09+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/03/11/spark-shuffle-implementation</id>
    <content type="html"><![CDATA[<p>Spark中，数据通过从一个state流向下一个 stage 是通过shuffle过程完成的。</p>

<p>本文介绍Spark中的shuffle过程设计和工作原理。</p>

<!--More-->


<h2>MapReduce 和 Spark Shuffle 过程对比</h2>

<p>Hadoop MapReduce 中的 shuffle 过程，和Spark的shuffle过程有一些区别和联系。</p>

<p>从 high-level 的角度来看，两者都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer。（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。</p>

<p>Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。</p>

<p>从 low-level 的角度来看，两者差别不小。</p>

<p>Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。</p>

<p>这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。</p>

<p>1.2之前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。</p>

<p>如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。</p>

<p>从Spark 1.2起，sort作为默认的Shuffle实现。</p>

<p>从实现角度来看，两者也有不少差别。</p>

<p>Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。</p>

<p>每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。</p>

<p>在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。</p>

<p>如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。</p>

<p>那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read 的处理逻辑？以及两个处理逻辑应该怎么高效实现？</p>

<h2>Shuffle Write</h2>

<p>由于不要求数据有序，shuffle write 的任务很简单：</p>

<ul>
<li>将数据 partition 好，并持久化。</li>
<li>之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</li>
</ul>


<p>shuffle write 的任务很简单，那么实现也很简单：</p>

<ul>
<li>将 shuffle write 的处理逻辑加入到 ShuffleMapStage（ShuffleMapTask 所在的 stage） 的最后，该 stage 的 final RDD 每输出一个 record 就将其 partition 并持久化。</li>
</ul>


<p>图示如下：</p>

<p><img src="http://lionheartwang.github.io/images/blog/29-shuffle-write-no-consolidation.png"></p>

<p>上图有 4 个 ShuffleMapTask 要在同一个 worker node 上运行，CPU core 数为 2，可以同时运行两个 task。</p>

<p>每个 task 的执行结果（该 stage 的 finalRDD 中某个 partition 包含的 records）被逐一写到本地磁盘上。</p>

<p>每个 task 包含 R 个缓冲区，R = reducer 个数（也就是下一个 stage 中 task 的个数）。</p>

<p>缓冲区被称为 bucket，其大小为spark.shuffle.file.buffer.kb ，默认是 32KB（Spark 1.1 版本以前是 100KB）。</p>

<p>其实 bucket 是一个广义的概念，代表 ShuffleMapTask 输出结果经过 partition 后要存放的地方，这里为了细化数据存放位置和数据名称，仅仅用 bucket 表示缓冲区。</p>

<p>ShuffleMapTask 的执行过程很简单：</p>

<ul>
<li>先利用 pipeline 计算得到 finalRDD 中对应 partition 的 records。每得到一个 record 就将其送到对应的 bucket 里，具体是哪个 bucket 由partitioner.partition(record.getKey()))决定。</li>
<li>每个 bucket 里面的数据会不断被写到本地磁盘上，形成一个 ShuffleBlockFile，或者简称 FileSegment。</li>
<li>之后的 reducer 会去 fetch 属于自己的 FileSegment，进入 shuffle read 阶段。</li>
</ul>


<p>这样的实现很简单，但有几个问题：</p>

<ul>
<li>产生的 FileSegment 过多。每个 ShuffleMapTask 产生 R（reducer 个数）个 FileSegment，M 个 ShuffleMapTask 就会产生 M * R 个文件。一般 Spark job 的 M 和 R 都很大，因此磁盘上会存在大量的数据文件。</li>
<li>缓冲区占用内存空间大。每个 ShuffleMapTask 需要开 R 个 bucket，M 个 ShuffleMapTask 就会产生 M R 个 bucket。虽然一个 ShuffleMapTask 结束后，对应的缓冲区可以被回收，但一个 worker node 上同时存在的 bucket 个数可以达到 cores R 个（一般 worker 同时可以运行 cores 个 ShuffleMapTask），占用的内存空间也就达到了cores * R * 32 KB。对于 8 核 1000 个 reducer 来说，占用内存就是 256MB。</li>
</ul>


<p>目前来看，第二个问题还没有好的方法解决，因为写磁盘终究是要开缓冲区的，缓冲区太小会影响 IO 速度。</p>

<p>但第一个问题有一些方法去解决，下面介绍已经在 Spark 里面实现的 FileConsolidation 方法。先上图：</p>

<p><img src="http://lionheartwang.github.io/images/blog/30-shuffle-write-consolidation.png"></p>

<p>可以明显看出，在一个 core 上连续执行的 ShuffleMapTasks 可以共用一个输出文件 ShuffleFile。</p>

<p>先执行完的 ShuffleMapTask 形成 ShuffleBlock i，后执行的 ShuffleMapTask 可以将输出数据直接追加到 ShuffleBlock i 后面，形成 ShuffleBlock i'，每个 ShuffleBlock 被称为 FileSegment。下一个 stage 的 reducer 只需要 fetch 整个 ShuffleFile 就行了。</p>

<p>这样，每个 worker 持有的文件数降为 cores * R。FileConsolidation 功能可以通过spark.shuffle.consolidateFiles=true来开启。</p>

<h2>Shuffle Read</h2>

<p>先看一张包含 ShuffleDependency 的物理执行图，来自 reduceByKey：</p>

<p><img src="http://lionheartwang.github.io/images/blog/31-reduceByKeyStage.png"></p>

<p>很自然地，要计算 ShuffleRDD 中的数据，必须先把 MapPartitionsRDD 中的数据 fetch 过来。那么问题就来了：</p>

<ul>
<li>在什么时候 fetch，parent stage 中的一个 ShuffleMapTask 执行完还是等全部 ShuffleMapTasks 执行完？</li>
<li>边 fetch 边处理还是一次性 fetch 完再处理？</li>
<li>fetch 来的数据存放到哪里？</li>
<li>怎么获得要 fetch 的数据的存放位置？</li>
</ul>


<p>依次解答如下。</p>

<h3>在什么时候 fetch？</h3>

<p>当 parent stage 的所有 ShuffleMapTasks 结束后再 fetch。</p>

<p>理论上讲一个 ShuffleMapTask 结束后就可以 fetch，但是为了迎合 stage 的概念（即一个 stage 如果其 parent stages 没有执行完，自己是不能被提交执行的），还是选择全部 ShuffleMapTasks 执行完再去 fetch。</p>

<p>因为 fetch 来的 FileSegments 要先在内存做缓冲，所以一次 fetch 的 FileSegments 总大小不能太大。Spark 规定这个缓冲界限不能超过 spark.reducer.maxMbInFlight，这里用 softBuffer 表示，默认大小为 48MB。</p>

<p>一个 softBuffer 里面一般包含多个 FileSegment，但如果某个 FileSegment 特别大的话，这一个就可以填满甚至超过 softBuffer 的界限。</p>

<h3>边 fetch 边处理还是一次性 fetch 完再处理？</h3>

<p>边 fetch 边处理。</p>

<p>本质上，MapReduce shuffle 阶段就是边 fetch 边使用 combine() 进行处理，只是 combine() 处理的是部分数据。</p>

<p>MapReduce 为了让进入 reduce() 的 records 有序，必须等到全部数据都 shuffle-sort 后再开始 reduce()。</p>

<p>因为 Spark 不要求 shuffle 后的数据全局有序，因此没必要等到全部数据 shuffle 完成后再处理。那么如何实现边 shuffle 边处理，而且流入的 records 是无序的？</p>

<p>答案是使用可以 aggregate 的数据结构，比如 HashMap。</p>

<p>每 shuffle 得到（从缓冲的 FileSegment 中 deserialize 出来）一个 record，直接将其放进 HashMap 里面。</p>

<p>如果该 HashMap 已经存在相应的 Key，那么直接进行 aggregate 也就是 func(hashMap.get(Key), Value)。</p>

<p>比如上面 WordCount 例子中的 func 就是 hashMap.get(Key) ＋ Value，并将 func 的结果重新 put(key) 到 HashMap 中去。</p>

<p>这个 func 功能上相当于 reduce()，但实际处理数据的方式与 MapReduce reduce() 有差别，差别相当于下面两段程序的差别。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'>  <span class="c1">// MapReduce</span>
</span><span class='line'>  <span class="n">reduce</span><span class="o">(</span><span class="n">K</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">V</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">result</span> <span class="o">=</span> <span class="n">process</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">values</span><span class="o">)</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">result</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// Spark</span>
</span><span class='line'>  <span class="n">reduce</span><span class="o">(</span><span class="n">K</span> <span class="n">key</span><span class="o">,</span> <span class="n">Iterable</span><span class="o">&lt;</span><span class="n">V</span><span class="o">&gt;</span> <span class="n">values</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="n">result</span> <span class="o">=</span> <span class="kc">null</span>
</span><span class='line'>      <span class="nf">for</span> <span class="o">(</span><span class="n">V</span> <span class="n">value</span> <span class="o">:</span> <span class="n">values</span><span class="o">)</span>
</span><span class='line'>          <span class="n">result</span>  <span class="o">=</span> <span class="n">func</span><span class="o">(</span><span class="n">result</span><span class="o">,</span> <span class="n">value</span><span class="o">)</span>
</span><span class='line'>      <span class="k">return</span> <span class="n">result</span>
</span><span class='line'>  <span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>MapReduce 可以在 process 函数里面可以定义任何数据结构，也可以将部分或全部的 values 都 cache 后再进行处理，非常灵活。</p>

<p>而 Spark 中的 func 的输入参数是固定的，一个是上一个 record 的处理结果，另一个是当前读入的 record，它们经过 func 处理后的结果被下一个 record 处理时使用。</p>

<p>因此一些算法比如求平均数，在 process 里面很好实现，直接sum(values)/values.length，而在 Spark 中 func 可以实现sum(values)，但不好实现/values.length。更多的 func 将会在下面的章节细致分析。</p>

<h3>fetch 来的数据存放到哪里？</h3>

<p>刚 fetch 来的 FileSegment 存放在 softBuffer 缓冲区，经过处理后的数据放在内存 + 磁盘上。</p>

<p>这里我们主要讨论处理后的数据，可以灵活设置这些数据是“只用内存”还是“内存＋磁盘”。</p>

<p>如果spark.shuffle.spill = false就只用内存。</p>

<p>内存使用的是AppendOnlyMap ，类似 Java 的HashMap，内存＋磁盘使用的是ExternalAppendOnlyMap，如果内存空间不足时，ExternalAppendOnlyMap可以将 records 进行 sort 后 spill 到磁盘上，等到需要它们的时候再进行归并，后面会详解。</p>

<p>使用“内存＋磁盘”的一个主要问题就是如何在两者之间取得平衡？</p>

<p>在 Hadoop MapReduce 中，默认将 reducer 的 70% 的内存空间用于存放 shuffle 来的数据，等到这个空间利用率达到 66% 的时候就开始 merge-combine()-spill。</p>

<p>在 Spark 中，也适用同样的策略，一旦 ExternalAppendOnlyMap 达到一个阈值就开始 spill，具体细节下面会讨论。</p>

<h3>怎么获得要 fetch 的数据的存放位置？</h3>

<p>在上一章讨论物理执行图中的 stage 划分的时候，我们强调 “一个 ShuffleMapStage 形成后，会将该 stage 最后一个 final RDD 注册到 MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size)，这一步很重要，因为 shuffle 过程需要 MapOutputTrackerMaster 来指示 ShuffleMapTask 输出数据的位置”。</p>

<p>因此，reducer 在 shuffle 的时候是要去 driver 里面的 MapOutputTrackerMaster 询问 ShuffleMapTask 输出的数据位置的。</p>

<p>每个 ShuffleMapTask 完成时会将 FileSegment 的存储位置信息汇报给 MapOutputTrackerMaster。</p>

<p>至此，我们已经讨论了 shuffle write 和 shuffle read 设计的核心思想、算法及某些实现。接下来，我们深入一些细节来讨论。</p>

<h2>典型算子实现</h2>

<h3>reduceByKey(func)</h3>

<p>上面初步介绍了 reduceByKey() 是如何实现边 fetch 边 reduce() 的。需要注意的是虽然 Example(WordCount) 中给出了各个 RDD 的内容，但一个 partition 里面的 records 并不是同时存在的。比如在 ShuffledRDD 中，每 fetch 来一个 record 就立即进入了 func 进行处理。MapPartitionsRDD 中的数据是 func 在全部 records 上的处理结果。从 record 粒度上来看，reduce() 可以表示如下：</p>

<p><img src="http://lionheartwang.github.io/images/blog/32-reduceByKeyRecord.png"></p>

<p>可以看到，fetch 来的 records 被逐个 aggreagte 到 HashMap 中，等到所有 records 都进入 HashMap，就得到最后的处理结果。唯一要求是 func 必须是 commulative 的（参见上面的 Spark 的 reduce() 的代码）。</p>

<p>ShuffledRDD 到 MapPartitionsRDD 使用的是 mapPartitionsWithContext 操作。</p>

<p>为了减少数据传输量，MapReduce 可以在 map 端先进行 combine()，其实在 Spark 也可以实现，只需要将上图 ShuffledRDD => MapPartitionsRDD 的 mapPartitionsWithContext 在 ShuffleMapStage 中也进行一次即可，比如 reduceByKey 例子中 ParallelCollectionRDD => MapPartitionsRDD 完成的就是 map 端的 combine()。</p>

<p>对比 MapReduce 的 map()-reduce() 和 Spark 中的 reduceByKey()：</p>

<p>map 端的区别：map() 没有区别。对于 combine()，MapReduce 先 sort 再 combine()，Spark 直接在 HashMap 上进行 combine()。
reduce 端区别：MapReduce 的 shuffle 阶段先 fetch 数据，数据量到达一定规模后 combine()，再将剩余数据 merge-sort 后 reduce()，reduce() 非常灵活。Spark 边 fetch 边 reduce()（在 HashMap 上执行 func），因此要求 func 符合 commulative 的特性。
从内存利用上来对比：</p>

<p>map 端区别：MapReduce 需要开一个大型环形缓冲区来暂存和排序 map() 的部分输出结果，但 combine() 不需要额外空间（除非用户自己定义）。 Spark 需要 HashMap 内存数据结构来进行 combine()，同时输出 records 到磁盘上时也需要一个小的 buffer（bucket）。
reduce 端区别：MapReduce 需要一部分内存空间来存储 shuffle 过来的数据，combine() 和 reduce() 不需要额外空间，因为它们的输入数据分段有序，只需归并一下就可以得到。在 Spark 中，fetch 时需要 softBuffer，处理数据时如果只使用内存，那么需要 HashMap 来持有处理后的结果。如果使用内存＋磁盘，那么在 HashMap 存放一部分处理后的数据。</p>

<h3>groupByKey(numPartitions)</h3>

<p>ShuffleGroupByKey</p>

<p>与 reduceByKey() 流程一样，只是 func 变成 result = result ++ record.value，功能是将每个 key 对应的所有 values 链接在一起。result 来自 hashMap.get(record.key)，计算后的 result 会再次被 put 到 hashMap 中。与 reduceByKey() 的区别就是 groupByKey() 没有 map 端的 combine()。对于 groupByKey() 来说 map 端的 combine() 只是减少了重复 Key 占用的空间，如果 key 重复率不高，没必要 combine()，否则，最好能够 combine()。</p>

<p><img src="http://lionheartwang.github.io/images/blog/37-ShuffleGroupByKey.png"></p>

<h3>distinct(numPartitions)</h3>

<p>ShuffleDistinct</p>

<p>与 reduceByKey() 流程一样，只是 func 变成 result = result == null? record.value : result，如果 HashMap 中没有该 record 就将其放入，否则舍弃。与 reduceByKey() 相同，在map 端存在 combine()。</p>

<p><img src="http://lionheartwang.github.io/images/blog/33-ShuffleDistinct.png"></p>

<h3>cogroup(otherRDD, numPartitions)</h3>

<p>ShuffleCoGroup</p>

<p>CoGroupedRDD 可能有 0 个、1 个或者多个 ShuffleDependency。但并不是要为每一个 ShuffleDependency 建立一个 HashMap，而是所有的 Dependency 共用一个 HashMap。与 reduceByKey() 不同的是，HashMap 在 CoGroupedRDD 的 compute() 中建立，而不是在 mapPartitionsWithContext() 中建立。</p>

<p>粗线表示的 task 首先 new 出一个 Array[ArrayBuffer(), ArrayBuffer()]，ArrayBuffer() 的个数与参与 cogroup 的 RDD 个数相同。func 的逻辑是这样的：每当从 RDD a 中 shuffle 过来一个 \ record 就将其添加到 hashmap.get(Key) 对应的 Array 中的第一个 ArrayBuffer() 中，每当从 RDD b 中 shuffle 过来一个 record，就将其添加到对应的 Array 中的第二个 ArrayBuffer()。</p>

<p><img src="http://lionheartwang.github.io/images/blog/34-ShuffleCoGroup.png"></p>

<p>CoGroupedRDD => MappedValuesRDD 对应 mapValues() 操作，就是将 [ArrayBuffer(), ArrayBuffer()] 变成 [Iterable[V], Iterable[W]]。</p>

<h3>intersection(otherRDD) 和 join(otherRDD, numPartitions)</h3>

<p><img src="http://lionheartwang.github.io/images/blog/35-ShuffleIntersection.png"></p>

<p>join 这两个操作中均使用了 cogroup，所以 shuffle 的处理方式与 cogroup 一样。</p>

<p><img src="http://lionheartwang.github.io/images/blog/36-ShuffleJoin.png"></p>

<h3>sortByKey(ascending, numPartitions)</h3>

<p><img src="http://lionheartwang.github.io/images/blog/38-ShuffleSortByKey.png"></p>

<p>sortByKey() 中 ShuffledRDD => MapPartitionsRDD 的处理逻辑与 reduceByKey() 不太一样，没有使用 HashMap 和 func 来处理 fetch 过来的 records。</p>

<p>sortByKey() 中 ShuffledRDD => MapPartitionsRDD 的处理逻辑是：将 shuffle 过来的一个个 record 存放到一个 Array 里，然后按照 Key 来对 Array 中的 records 进行 sort。</p>

<h3>coalesce(numPartitions, shuffle = true)</h3>

<p><img src="http://lionheartwang.github.io/images/blog/41-ShuffleCoalesce.png"></p>

<p>coalesce() 虽然有 ShuffleDependency，但不需要对 shuffle 过来的 records 进行 aggregate，所以没有建立 HashMap。每 shuffle 一个 record，就直接流向 CoalescedRDD，进而流向 MappedRDD 中。</p>

<h2>基础数据结构</h2>

<p>HashMap 是 Spark shuffle read 过程中频繁使用的、用于 aggregate 的数据结构。Spark 设计了两种：
- 一种是全内存的 AppendOnlyMap
- 另一种是内存＋磁盘的 ExternalAppendOnlyMap。</p>

<p>下面我们来分析一下两者特性及内存使用情况。</p>

<h3>AppendOnlyMap</h3>

<p>AppendOnlyMap 的官方介绍是 A simple open hash table optimized for the append-only use case, where keys are never removed, but the value for each key may be changed。意思是类似 HashMap，但没有remove(key)方法。其实现原理很简单，开一个大 Object 数组，蓝色部分存储 Key，白色部分存储 Value。如下图：</p>

<p><img src="http://lionheartwang.github.io/images/blog/40-appendonlymap.png"></p>

<p>当要 put(K, V) 时，先 hash(K) 找存放位置，如果存放位置已经被占用，就使用 Quadratic probing 探测方法来找下一个空闲位置。对于图中的 K6 来说，第三次查找找到 K4 后面的空闲位置，放进去即可。get(K6) 的时候类似，找三次找到 K6，取出紧挨着的 V6，与先来的 value 做 func，结果重新放到 V6 的位置。</p>

<p>迭代 AppendOnlyMap 中的元素的时候，从前到后扫描输出。</p>

<p>如果 Array 的利用率达到 70%，那么就扩张一倍，并对所有 key 进行 rehash 后，重新排列每个 key 的位置。</p>

<p>AppendOnlyMap 还有一个 destructiveSortedIterator(): Iterator[(K, V)] 方法，可以返回 Array 中排序后的 (K, V) pairs。实现方法很简单：先将所有 (K, V) pairs compact 到 Array 的前端，并使得每个 (K, V) 占一个位置（原来占两个），之后直接调用 Array.sort() 排序，不过这样做会破坏数组（key 的位置变化了）。</p>

<h3>ExternalAppendOnlyMap</h3>

<p>相比 AppendOnlyMap，ExternalAppendOnlyMap 的实现略复杂，但逻辑其实很简单，类似 Hadoop MapReduce 中的 shuffle-merge-combine-sort 过程：</p>

<p>ExternalAppendOnlyMap 持有一个 AppendOnlyMap，shuffle 来的一个个 (K, V) record 先 insert 到 AppendOnlyMap 中，insert 过程与原始的 AppendOnlyMap 一模一样。</p>

<p>如果 AppendOnlyMap 快被装满时检查一下内存剩余空间是否可以够扩展，够就直接在内存中扩展，不够就 sort 一下 AppendOnlyMap，将其内部所有 records 都 spill 到磁盘上。</p>

<p><img src="http://lionheartwang.github.io/images/blog/39-ExternalAppendOnlyMap.png"></p>

<p>图中 spill 了 4 次，每次 spill 完在磁盘上生成一个 spilledMap 文件，然后重新 new 出来一个 AppendOnlyMap。</p>

<p>最后一个 (K, V) record insert 到 AppendOnlyMap 后，表示所有 shuffle 来的 records 都被放到了 ExternalAppendOnlyMap 中，但不表示 records 已经被处理完，因为每次 insert 的时候，新来的 record 只与 AppendOnlyMap 中的 records 进行 aggregate，并不是与所有的 records 进行 aggregate（一些 records 已经被 spill 到磁盘上了）。</p>

<p>因此当需要 aggregate 的最终结果时，需要对 AppendOnlyMap 和所有的 spilledMaps 进行全局 merge-aggregate。</p>

<p>全局 merge-aggregate 的流程也很简单：</p>

<p>先将 AppendOnlyMap 中的 records 进行 sort，形成 sortedMap。</p>

<p>然后利用 DestructiveSortedIterator 和 DiskMapIterator 分别从 sortedMap 和各个 spilledMap 读出一部分数据（StreamBuffer）放到 mergeHeap 里面。StreamBuffer 里面包含的 records 需要具有相同的 hash(key)，所以图中第一个 spilledMap 只读出前三个 records 进入 StreamBuffer。</p>

<p>mergeHeap 顾名思义就是使用堆排序不断提取出 hash(firstRecord.Key) 相同的 StreamBuffer，并将其一个个放入 mergeBuffers 中，放入的时候与已经存在于 mergeBuffers 中的 StreamBuffer 进行 merge-combine，第一个被放入 mergeBuffers 的 StreamBuffer 被称为 minBuffer，那么 minKey 就是 minBuffer 中第一个 record 的 key。</p>

<p>当 merge-combine 的时候，与 minKey 相同的 records 被 aggregate 一起，然后输出。整个 merge-combine 在 mergeBuffers 中结束后，StreamBuffer 剩余的 records 随着 StreamBuffer 重新进入 mergeHeap。</p>

<p>一旦某个 StreamBuffer 在 merge-combine 后变为空（里面的 records 都被输出了），那么会使用 DestructiveSortedIterator 或 DiskMapIterator 重新装填 hash(key) 相同的 records，然后再重新进入 mergeHeap。</p>

<p>整个 insert-merge-aggregate 的过程有三点需要进一步探讨一下：</p>

<p>内存剩余空间检测</p>

<p>与 Hadoop MapReduce 规定 reducer 中 70% 的空间可用于 shuffle-sort 类似，Spark 也规定 executor 中 spark.shuffle.memoryFraction * spark.shuffle.safetyFraction 的空间（默认是0.3 * 0.8）可用于 ExternalOnlyAppendMap。</p>

<p>Spark 略保守是不是？更保守的是这 24％ 的空间不是完全用于一个 ExternalOnlyAppendMap 的，而是由在 executor 上同时运行的所有 reducer 共享的。</p>

<p>为此，exectuor 专门持有一个 ShuffleMemroyMap: HashMap[threadId, occupiedMemory] 来监控每个 reducer 中 ExternalOnlyAppendMap 占用的内存量。</p>

<p>每当 AppendOnlyMap 要扩展时，都会计算 ShuffleMemroyMap 持有的所有 reducer 中的 AppendOnlyMap 已占用的内存 ＋ 扩展后的内存 是会否会大于内存限制，大于就会将 AppendOnlyMap spill 到磁盘。</p>

<p>有一点需要注意的是前 1000 个 records 进入 AppendOnlyMap 的时候不会启动是否要 spill 的检查，需要扩展时就直接在内存中扩展。</p>

<p>AppendOnlyMap 大小估计</p>

<p>为了获知 AppendOnlyMap 占用的内存空间，可以在每次扩展时都将 AppendOnlyMap reference 的所有 objects 大小都算一遍，然后加和，但这样做非常耗时。</p>

<p>所以 Spark 设计了粗略的估算算法，算法时间复杂度是 O(1)，核心思想是利用 AppendOnlyMap 中每次 insert-aggregate record 后 result 的大小变化及一共 insert 的 records 的个数来估算大小，具体见 SizeTrackingAppendOnlyMap 和 SizeEstimator。</p>

<p>Spill 过程</p>

<p>与 shuffle write 一样，在 spill records 到磁盘上的时候，会建立一个 buffer 缓冲区，大小仍为 spark.shuffle.file.buffer.kb ，默认是 32KB。</p>

<p>另外，由于 serializer 也会分配缓冲区用于序列化和反序列化，所以如果一次 serialize 的 records 过多的话缓冲区会变得很大。Spark 限制每次 serialize 的 records 个数为 spark.shuffle.spill.batchSize，默认是 10000。</p>

<h2>参考资料</h2>

<p>通过本文的介绍可以发现，相比 MapReduce 固定的 shuffle-combine-merge-reduce 策略，Spark 更加灵活，会根据不同的 transformation() 的语义去设计不同的 shuffle-aggregate 策略，再加上不同的内存数据结构来混搭出合理的执行流程。</p>

<p>参考资料：</p>

<ul>
<li>Spark Shuffle过程：<a href="https://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html">https://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html</a></li>
<li>Spark Shuffle进化史：<a href="http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/">http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Flink架构及工作原理介绍]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/03/05/flink-framwork-introduction/"/>
    <updated>2018-03-05T00:54:42+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/03/05/flink-framwork-introduction</id>
    <content type="html"><![CDATA[<p>本文整体介绍Apache Flink流计算框架的特性、概念、组件栈、架构及原理分析。</p>

<p>主要内容参考如下博客整理：</p>

<ul>
<li><a href="http://shiyanjun.cn/archives/1508.html">http://shiyanjun.cn/archives/1508.html</a></li>
</ul>


<!--More-->


<h2>Flink框架整体介绍</h2>

<p>Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台。</p>

<p>它能够基于同一个Flink运行时（Flink Runtime），提供支持流处理和批处理两种类型应用的功能。</p>

<p>现有的开源计算方案，会把流处理和批处理作为两种不同的应用类型，因为他们它们所提供的SLA是完全不相同的：</p>

<ul>
<li>流处理一般需要支持低延迟、Exactly-once保证</li>
<li>批处理需要支持高吞吐、高效处理</li>
</ul>


<p>所以在实现的时候通常是分别给出两套实现方法，或者通过一个独立的开源框架来实现其中每一种处理方案。</p>

<p>例如，实现批处理的开源方案有MapReduce、Tez、Crunch、Spark，实现流处理的开源方案有Samza、Storm。</p>

<p>Flink在实现流处理和批处理时，与传统的一些方案完全不同，它从另一个视角看待流处理和批处理，将二者统一起来：</p>

<ul>
<li>Flink是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；</li>
<li>批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。</li>
<li>基于同一个Flink运行时（Flink Runtime），分别提供了流处理和批处理API，而这两种API也是实现上层面向流处理、批处理类型应用框架的基础。</li>
</ul>


<h3>基本特性</h3>

<p>关于Flink所支持的特性，我这里只是通过分类的方式简单做一下梳理，涉及到具体的一些概念及其原理会在后面的部分做详细说明。</p>

<p>流处理特性</p>

<ul>
<li>支持高吞吐、低延迟、高性能的流处理</li>
<li>支持带有事件时间的窗口（Window）操作</li>
<li>支持有状态计算的Exactly-once语义</li>
<li>支持高度灵活的窗口（Window）操作，支持基于time、count、session，以及data-driven的窗口操作</li>
<li>支持具有Backpressure功能的持续流模型</li>
<li>支持基于轻量级分布式快照（Snapshot）实现的容错</li>
<li>一个运行时同时支持Batch on Streaming处理和Streaming处理</li>
<li>Flink在JVM内部实现了自己的内存管理</li>
<li>支持迭代计算</li>
<li>支持程序自动优化：避免特定情况下Shuffle、排序等昂贵操作，中间结果有必要进行缓存</li>
</ul>


<h3>API支持</h3>

<p>对Streaming数据类应用，提供DataStream API</p>

<p>对批处理类应用，提供DataSet API（支持Java/Scala）</p>

<h3>Libraries支持</h3>

<p>相关上层Library支持情况如下：</p>

<ul>
<li>支持机器学习（FlinkML）</li>
<li>支持图分析（Gelly）</li>
<li>支持关系数据处理（Table）</li>
<li>支持复杂事件处理（CEP）</li>
</ul>


<p>与其他外部系统对接支持如下：</p>

<ul>
<li>支持Flink on YARN</li>
<li>支持HDFS</li>
<li>支持来自Kafka的输入数据</li>
<li>支持Apache HBase</li>
<li>支持Hadoop程序</li>
<li>支持Tachyon</li>
<li>支持ElasticSearch</li>
<li>支持RabbitMQ</li>
<li>支持Apache Storm</li>
<li>支持S3</li>
<li>支持XtreemFS</li>
</ul>


<h2>基本概念</h2>

<h3>Stream &amp; Transformation &amp; Operator</h3>

<p>用户实现的Flink程序是由Stream和Transformation这两个基本构建块组成，其中Stream是一个中间结果数据，而Transformation是一个操作，它对一个或多个输入Stream进行计算处理，输出一个或多个结果Stream。</p>

<p>当一个Flink程序被执行的时候，它会被映射为Streaming Dataflow。</p>

<p>一个Streaming Dataflow是由一组Stream和Transformation Operator组成，它类似于一个DAG图，在启动的时候从一个或多个Source Operator开始，结束于一个或多个Sink Operator。</p>

<p>下面是一个由Flink程序映射为Streaming Dataflow的示意图，如下所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/11-flink-streaming-dataflow-example.png"></p>

<p>上图中，FlinkKafkaConsumer是一个Source Operator，map、keyBy、timeWindow、apply是Transformation Operator，RollingSink是一个Sink Operator。</p>

<h3>Parallel Dataflow</h3>

<p>在Flink中，程序天生是并行和分布式的：</p>

<ul>
<li>一个Stream可以被分成多个Stream分区（Stream Partitions），一个Operator可以被分成多个Operator Subtask，每一个Operator Subtask是在不同的线程中独立执行的。</li>
<li>一个Operator的并行度，等于Operator Subtask的个数，一个Stream的并行度总是等于生成它的Operator的并行度。</li>
</ul>


<p>有关Parallel Dataflow的实例，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/13-flink-parallel-dataflow.png"></p>

<p>上图Streaming Dataflow的并行视图中，展现了在两个Operator之间的Stream的两种模式：</p>

<h4>One-to-one模式</h4>

<p>比如从Source[1]到map()[1]，它保持了Source的分区特性（Partitioning）和分区内元素处理的有序性。</p>

<p>也就是说map()[1]的Subtask看到数据流中记录的顺序，与Source[1]中看到的记录顺序是一致的。</p>

<h4>Redistribution模式</h4>

<p>这种模式改变了输入数据流的分区。</p>

<p>比如从map()[1]、map()[2]到keyBy()/window()/apply()[1]、keyBy()/window()/apply()[2]，上游的Subtask向下游的多个不同的Subtask发送数据，改变了数据流的分区，这与实际应用所选择的Operator有关系。</p>

<p>另外，Source Operator对应2个Subtask，所以并行度为2，而Sink Operator的Subtask只有1个，故而并行度为1。</p>

<h3>Task &amp; Operator Chain</h3>

<p>在Flink分布式执行环境中，会将多个Operator Subtask串起来组成一个Operator Chain，实际上就是一个执行链。</p>

<p>每个执行链会在TaskManager上一个独立的线程中执行，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/14-flink-tasks-chains.png"></p>

<p>上图中上半部分表示的是一个Operator Chain，多个Operator通过Stream连接，而每个Operator在运行时对应一个Task。</p>

<p>图中下半部分是上半部分的一个并行版本，也就是对每一个Task都并行化为多个Subtask。</p>

<h3>Time &amp; Window</h3>

<p>Flink支持基于时间窗口操作，也支持基于数据的窗口操作，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/15-flink-window.png"></p>

<p>上图中，基于时间的窗口操作，在每个相同的时间间隔对Stream中的记录进行处理，通常各个时间间隔内的窗口操作处理的记录数不固定。</p>

<p>而基于数据驱动的窗口操作，可以在Stream中选择固定数量的记录作为一个窗口，对该窗口中的记录进行处理。</p>

<p>有关窗口操作的不同类型，可以分为如下几种：</p>

<ul>
<li>倾斜窗口（Tumbling Windows，记录没有重叠）</li>
<li>滑动窗口（Slide Windows，记录有重叠）</li>
<li>会话窗口（Session Windows）</li>
</ul>


<p>具体可以查阅相关资料。</p>

<p>在处理Stream中的记录时，记录中通常会包含各种典型的时间字段，Flink支持多种时间的处理，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/16-flink-event-ingestion-processing-time.png"></p>

<p>上图描述了在基于Flink的流处理系统中，各种不同的时间所处的位置和含义。</p>

<p>其中：</p>

<ul>
<li>Event Time表示事件创建时间</li>
<li>Ingestion Time表示事件进入到Flink Dataflow的时间</li>
<li>Processing Time表示某个Operator对事件进行处理事的本地系统时间（是在TaskManager节点上）。</li>
</ul>


<p>这里，谈一下基于Event Time进行处理的问题。</p>

<p>通常根据Event Time会给整个Streaming应用带来一定的延迟性，因为在一个基于事件的处理系统中，进入系统的事件可能会基于Event Time而发生乱序现象。</p>

<p>比如事件来源于外部的多个系统，为了增强事件处理吞吐量会将输入的多个Stream进行自然分区，每个Stream分区内部有序，但是要保证全局有序必须同时兼顾多个Stream分区的处理，设置一定的时间窗口进行暂存数据，当多个Stream分区基于Event Time排列对齐后才能进行延迟处理。</p>

<p>所以，设置的暂存数据记录的时间窗口越长，处理性能越差，甚至严重影响Stream处理的实时性。</p>

<p>有关基于时间的Streaming处理，可以参考官方文档，在Flink中借鉴了Google使用的WaterMark实现方式，可以查阅相关资料。</p>

<h2>基本架构</h2>

<p>Flink系统的架构与Spark类似，是一个基于Master-Slave风格的架构，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/17-flink-system-architecture.png"></p>

<p>Flink集群启动时，会启动一个JobManager进程、至少一个TaskManager进程。</p>

<p>在Local模式下，会在同一个JVM内部启动一个JobManager进程和TaskManager进程。</p>

<p>当Flink程序提交后，会创建一个Client来进行预处理，并转换为一个并行数据流，这是对应着一个Flink Job，从而可以被JobManager和TaskManager执行。</p>

<p>在实现上，Flink基于Actor实现了JobManager和TaskManager，所以JobManager与TaskManager之间的信息交换，都是通过事件的方式来进行处理。</p>

<p>如上图所示，Flink系统主要包含如下3个主要的进程：</p>

<h3>JobManager</h3>

<p>JobManager是Flink系统的协调者，它负责接收Flink Job，调度组成Job的多个Task的执行。</p>

<p>同时，JobManager还负责收集Job的状态信息，并管理Flink集群中从节点TaskManager。</p>

<p>JobManager所负责的各项管理功能，它接收到并处理的事件主要包括：</p>

<ul>
<li>RegisterTaskManager：在Flink集群启动的时候，TaskManager会向JobManager注册，如果注册成功，则JobManager会向TaskManager回复消息AcknowledgeRegistration。</li>
<li>SubmitJob：Flink程序内部通过Client向JobManager提交Flink Job，其中在消息SubmitJob中以JobGraph形式描述了Job的基本信息。</li>
<li>CancelJob：请求取消一个Flink Job的执行，CancelJob消息中包含了Job的ID，如果成功则返回消息CancellationSuccess，失败则返回消息CancellationFailure。</li>
<li>UpdateTaskExecutionState：TaskManager会向JobManager请求更新ExecutionGraph中的ExecutionVertex的状态信息，更新成功则返回true。</li>
<li>RequestNextInputSplit：运行在TaskManager上面的Task，请求获取下一个要处理的输入Split，成功则返回NextInputSplit。</li>
<li>JobStatusChanged：ExecutionGraph向JobManager发送该消息，用来表示Flink Job的状态发生的变化，例如：RUNNING、CANCELING、FINISHED等。</li>
</ul>


<h3>TaskManager</h3>

<p>TaskManager也是一个Actor，它是实际负责执行计算的Worker，在其上执行Flink Job的一组Task。</p>

<p>每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。</p>

<p>TaskManager端可以分成两个阶段：</p>

<ul>
<li>注册阶段：TaskManager会向JobManager注册，发送RegisterTaskManager消息，等待JobManager返回AcknowledgeRegistration，然后TaskManager就可以进行初始化过程。</li>
<li>可操作阶段：该阶段TaskManager可以接收并处理与Task有关的消息，如SubmitTask、CancelTask、FailTask。</li>
</ul>


<p>如果TaskManager无法连接到JobManager，这是TaskManager就失去了与JobManager的联系，会自动进入“注册阶段”，只有完成注册才能继续处理Task相关的消息。</p>

<h3>Client</h3>

<p>当用户提交一个Flink程序时，会首先创建一个Client。</p>

<p>该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。</p>

<p>Client会将用户提交的Flink程序组装一个JobGraph， 并且是以JobGraph的形式提交的。</p>

<p>一个JobGraph是一个Flink Dataflow，它由多个JobVertex组成的DAG。</p>

<p>其中，一个JobGraph包含了一个Flink程序的如下信息：JobID、Job名称、配置信息、一组JobVertex等。</p>

<h2>组件栈</h2>

<p>Flink是一个分层架构的系统，每一层所包含的组件都提供了特定的抽象，用来服务于上层组件。Flink分层的组件栈如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/18-flink-component-stack.png"></p>

<p>下面，我们自下而上，分别针对每一层进行解释说明。</p>

<h3>Deployment层</h3>

<p>该层主要涉及了Flink的部署模式，Flink支持多种部署模式：</p>

<ul>
<li>本地、集群（Standalone/YARN）</li>
<li>云（GCE/EC2）</li>
<li>Standalone部署模式与Spark类似。</li>
</ul>


<p>这里，我们看一下Flink on YARN的部署模式，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/19-flink-on-yarn.png"></p>

<p>了解YARN的话，对上图的原理非常熟悉，实际Flink也实现了满足在YARN集群上运行的各个组件：</p>

<ul>
<li>Flink YARN Client负责与YARN RM通信协商资源请求</li>
<li>Flink JobManager和Flink TaskManager分别申请到Container去运行各自的进程。</li>
</ul>


<p>通过上图可以看到，YARN AM与Flink JobManager在同一个Container中，这样AM可以知道Flink JobManager的地址，从而AM可以申请Container去启动Flink TaskManager。</p>

<p>待Flink成功运行在YARN集群上，Flink YARN Client就可以提交Flink Job到Flink JobManager，并进行后续的映射、调度和计算处理。</p>

<h3>Runtime层</h3>

<p>Runtime层提供了支持Flink计算的全部核心实现，比如：</p>

<ul>
<li>支持分布式Stream处理</li>
<li>JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务。</li>
</ul>


<h3>API层</h3>

<p>API层主要实现了面向无界Stream的流处理和面向Batch的批处理API。</p>

<p>其中面向流处理对应DataStream API，面向批处理对应DataSet API。</p>

<h3>Libraries层</h3>

<p>该层也可以称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。</p>

<ul>
<li>面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；</li>
<li>面向批处理支持：FlinkML（机器学习库）、Gelly（图处理）。</li>
</ul>


<h2>内部原理</h2>

<h3>容错机制</h3>

<p>Flink基于Checkpoint机制实现容错，它的原理是不断地生成分布式Streaming数据流Snapshot。</p>

<p>在流处理失败时，通过这些Snapshot可以恢复数据流处理。</p>

<h4>Barrier</h4>

<p>理解Flink的容错机制，首先需要了解一下Barrier这个概念：</p>

<ul>
<li>Stream Barrier是Flink分布式Snapshotting中的核心元素，它会作为数据流的记录被同等看待，被插入到数据流中，将数据流中记录的进行分组，并沿着数据流的方向向前推进。</li>
<li>每个Barrier会携带一个Snapshot ID，属于该Snapshot的记录会被推向该Barrier的前方。因为Barrier非常轻量，所以并不会中断数据流。带有Barrier的数据流。</li>
</ul>


<p>如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/20-flink-stream-barriers.png"></p>

<p>基于上图，我们通过如下要点来说明：</p>

<ul>
<li>出现一个Barrier，在该Barrier之前出现的记录都属于该Barrier对应的Snapshot，在该Barrier之后出现的记录属于下一个Snapshot。</li>
<li>来自不同Snapshot多个Barrier可能同时出现在数据流中，也就是说同一个时刻可能并发生成多个Snapshot。</li>
<li>当一个中间（Intermediate）Operator接收到一个Barrier后，它会发送Barrier到属于该Barrier的Snapshot的数据流中，等到Sink Operator接收到该Barrier后会向Checkpoint Coordinator确认该Snapshot。</li>
<li>直到所有的Sink Operator都确认了该Snapshot，才被认为完成了该Snapshot。</li>
</ul>


<p>这里还需要强调的是，Snapshot并不仅仅是对数据流做了一个状态的Checkpoint，它也包含了一个Operator内部所持有的状态，这样才能够在保证在流处理系统失败时能够正确地恢复数据流处理。</p>

<p>也就是说，如果一个Operator包含任何形式的状态，这种状态必须是Snapshot的一部分。</p>

<h4>Operator State</h4>

<p>Operator的状态包含两种：</p>

<ul>
<li>一种是系统状态，一个Operator进行计算处理的时候需要对数据进行缓冲，所以数据缓冲区的状态是与Operator相关联的，以窗口操作的缓冲区为例，Flink系统会收集或聚合记录数据并放到缓冲区中，直到该缓冲区中的数据被处理完成；</li>
<li>另一种是用户自定义状态（状态可以通过转换函数进行创建和修改），它可以是函数中的Java对象这样的简单变量，也可以是与函数相关的Key/Value状态。</li>
</ul>


<p>对于具有轻微状态的Streaming应用，会生成非常轻量的Snapshot而且非常频繁，但并不会影响数据流处理性能。</p>

<p>Streaming应用的状态会被存储到一个可配置的存储系统中，例如HDFS。</p>

<p>在一个Checkpoint执行过程中，存储的状态信息及其交互过程，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/21-flink-checkpointing.png"></p>

<h4>Stream Aligning</h4>

<p>在Checkpoint过程中，还有一个比较重要的操作——Stream Aligning。</p>

<p>当Operator接收到多个输入的数据流时，需要在Snapshot Barrier中对数据流进行排列对齐，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/22-flink-stream-aligning.png"></p>

<p>具体排列过程如下：</p>

<ul>
<li>Operator从一个incoming Stream接收到Snapshot Barrier n，然后暂停处理，直到其它的incoming Stream的Barrier n（否则属于2个Snapshot的记录就混在一起了）到达该Operator。</li>
<li>接收到Barrier n的Stream被临时搁置，来自这些Stream的记录不会被处理，而是被放在一个Buffer中</li>
<li>一旦最后一个Stream接收到Barrier n，Operator会emit所有暂存在Buffer中的记录，然后向Checkpoint Coordinator发送Snapshot n</li>
<li>继续处理来自多个Stream的记录</li>
</ul>


<p>基于Stream Aligning操作能够实现Exactly Once语义，但是也会给流处理应用带来延迟，因为为了排列对齐Barrier，会暂时缓存一部分Stream的记录到Buffer中。</p>

<p>尤其是在数据流并行度很高的场景下可能更加明显，通常以最迟对齐Barrier的一个Stream为处理Buffer中缓存记录的时刻点。</p>

<p>在Flink中，提供了一个开关，选择是否使用Stream Aligning，如果关掉则Exactly Once会变成At least once。</p>

<h3>调度机制</h3>

<p>在JobManager端，会接收到Client提交的JobGraph形式的Flink Job。</p>

<p>JobManager会将一个JobGraph转换映射为一个ExecutionGraph，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/23-flink-job-and-execution-graph.png"></p>

<p>通过上图可以看出：</p>

<ul>
<li>JobGraph是一个Job的用户逻辑视图表示，将一个用户要对数据流进行的处理表示为单个DAG图（对应于JobGraph）</li>
<li>DAG图由顶点（JobVertex）和中间结果集（IntermediateDataSet）组成，</li>
<li>其中JobVertex表示了对数据流进行的转换操作，比如map、flatMap、filter、keyBy等操作，而IntermediateDataSet是由上游的JobVertex所生成，同时作为下游的JobVertex的输入。</li>
</ul>


<p>而ExecutionGraph是JobGraph的并行表示，也就是实际JobManager调度一个Job在TaskManager上运行的逻辑视图。</p>

<p>它也是一个DAG图，是由ExecutionJobVertex、IntermediateResult（或IntermediateResultPartition）组成</p>

<p>ExecutionJobVertex实际对应于JobGraph图中的JobVertex，只不过在ExecutionJobVertex内部是一种并行表示，由多个并行的ExecutionVertex所组成。</p>

<p>另外，这里还有一个重要的概念，就是Execution，它是一个ExecutionVertex的一次运行Attempt。</p>

<p>也就是说，一个ExecutionVertex可能对应多个运行状态的Execution。</p>

<p>比如，一个ExecutionVertex运行产生了一个失败的Execution，然后还会创建一个新的Execution来运行，这时就对应这个2次运行Attempt。</p>

<p>每个Execution通过ExecutionAttemptID来唯一标识，在TaskManager和JobManager之间进行Task状态的交换都是通过ExecutionAttemptID来实现的。</p>

<p>下面看一下，在物理上进行调度，基于资源的分配与使用的一个例子，来自官网，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/24-flink-scheduled-task-slots.png"></p>

<p>说明如下：</p>

<ul>
<li>左上子图：有2个TaskManager，每个TaskManager有3个Task Slot</li>
<li>左下子图：一个Flink Job，逻辑上包含了1个data source、1个MapFunction、1个ReduceFunction，对应一个JobGraph</li>
<li>左下子图：用户提交的Flink Job对各个Operator进行的配置——data source的并行度设置为4，MapFunction的并行度也为4，ReduceFunction的并行度为3，在JobManager端对应于ExecutionGraph</li>
<li>右上子图：TaskManager 1上，有2个并行的ExecutionVertex组成的DAG图，它们各占用一个Task Slot</li>
<li>右下子图：TaskManager 2上，也有2个并行的ExecutionVertex组成的DAG图，它们也各占用一个Task Slot</li>
</ul>


<p>在2个TaskManager上运行的4个Execution是并行执行的</p>

<h3>迭代机制</h3>

<p>机器学习和图计算应用，都会使用到迭代计算。</p>

<p>Flink通过在迭代Operator中定义Step函数来实现迭代算法，这种迭代算法包括Iterate和Delta Iterate两种类型，在实现上它们反复地在当前迭代状态上调用Step函数，直到满足给定的条件才会停止迭代。</p>

<p>下面，对Iterate和Delta Iterate两种类型的迭代算法原理进行说明：</p>

<h4>Iterate</h4>

<p>Iterate Operator是一种简单的迭代形式：</p>

<ul>
<li>每一轮迭代，Step函数的输入或者是输入的整个数据集，或者是上一轮迭代的结果，通过该轮迭代计算出下一轮计算所需要的输入（也称为Next Partial Solution）</li>
<li>满足迭代的终止条件后，会输出最终迭代结果，具体执行流程如下图所示：</li>
</ul>


<p><img src="http://lionheartwang.github.io/images/blog/25-flink-iterations-iterate-operator.png"></p>

<p>Step函数在每一轮迭代中都会被执行，它可以是由map、reduce、join等Operator组成的数据流。</p>

<p>下面通过官网给出的一个例子来说明Iterate Operator，非常简单直观，如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/26-flink-iterations-iterate-operator-example.png"></p>

<p>上面迭代过程中，输入数据为1到5的数字，Step函数就是一个简单的map函数，会对每个输入的数字进行加1处理，而Next Partial Solution对应于经过map函数处理后的结果。</p>

<p>比如第一轮迭代，对输入的数字1加1后结果为2，对输入的数字2加1后结果为3，直到对输入数字5加1后结果为变为6，这些新生成结果数字2~6会作为第二轮迭代的输入。</p>

<p>迭代终止条件为进行10轮迭代，则最终的结果为11~15。</p>

<h4>Delta Iterate</h4>

<p>Delta Iterate Operator实现了增量迭代，它的实现原理如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/27-flink-iterations-delta-iterate-operator.png"></p>

<p>基于Delta Iterate Operator实现增量迭代，它有2个输入：</p>

<ul>
<li>其中一个是初始Workset，表示输入待处理的增量Stream数据</li>
<li>另一个是初始Solution Set，它是经过Stream方向上Operator处理过的结果。</li>
</ul>


<p>第一轮迭代会将Step函数作用在初始Workset上，得到的计算结果Workset作为下一轮迭代的输入，同时还要增量更新初始Solution Set。</p>

<p>如果反复迭代知道满足迭代终止条件，最后会根据Solution Set的结果，输出最终迭代结果。</p>

<p>比如，我们现在已知一个Solution集合中保存的是，已有的商品分类大类中购买量最多的商品。</p>

<p>而Workset输入的是来自线上实时交易中最新达成购买的商品的人数，经过计算会生成新的商品分类大类中商品购买量最多的结果。</p>

<p>如果某些大类中商品购买量突然增长，它需要更新Solution Set中的结果（原来购买量最多的商品，经过增量迭代计算，可能已经不是最多），最后会输出最终商品分类大类中购买量最多的商品结果集合。</p>

<p>更详细的例子，可以参考官网给出的“Propagate Minimum in Graph”，这里不再累述。</p>

<h3>Backpressure监控机制</h3>

<p>Backpressure在流式计算系统中会比较受到关注。</p>

<p>因为在一个Stream上进行处理的多个Operator之间，它们处理速度和方式可能非常不同，所以就存在上游Operator如果处理速度过快，下游Operator处可能机会堆积Stream记录，严重会造成处理延迟或下游Operator负载过重而崩溃（有些系统可能会丢失数据）。</p>

<p>因此，对下游Operator处理速度跟不上的情况，如果下游Operator能够将自己处理状态传播给上游Operator，使得上游Operator处理速度慢下来就会缓解上述问题，比如通过告警的方式通知现有流处理系统存在的问题。</p>

<p>Flink Web界面上提供了对运行Job的Backpressure行为的监控，它通过使用Sampling线程对正在运行的Task进行堆栈跟踪采样来实现，具体实现方式如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/28-flink-back-pressure-sampling.png"></p>

<p>JobManager会反复调用一个Job的Task运行所在线程的Thread.getStackTrace()。</p>

<p>默认情况下，JobManager会每间隔50ms触发对一个Job的每个Task依次进行100次堆栈跟踪调用，根据调用调用结果来确定Backpressure，Flink是通过计算得到一个比值（Radio）来确定当前运行Job的Backpressure状态。</p>

<p>在Web界面上可以看到这个Radio值，它表示在一个内部方法调用中阻塞（Stuck）的堆栈跟踪次数，例如，radio=0.01，表示100次中仅有1次方法调用阻塞。</p>

<p>Flink目前定义了如下Backpressure状态：</p>

<ul>
<li>OK: 0 &lt;= Ratio &lt;= 0.10</li>
<li>LOW: 0.10 &lt; Ratio &lt;= 0.5</li>
<li>HIGH: 0.5 &lt; Ratio &lt;= 1</li>
</ul>


<p>另外，Flink还提供了3个参数来配置Backpressure监控行为：</p>

<table>
<thead>
<tr>
<th> 参数名称 </th>
<th style="text-align:left;"> 默认值 </th>
<th style="text-align:left;"> 说明 </th>
</tr>
</thead>
<tbody>
<tr>
<td> jobmanager.web.backpressure.refresh-interval </td>
<td style="text-align:left;"> 60000 </td>
<td style="text-align:left;"> 默认1分钟，表示采样统计结果刷新时间间隔 </td>
</tr>
<tr>
<td> jobmanager.web.backpressure.num-samples </td>
<td style="text-align:left;"> 100 </td>
<td style="text-align:left;"> 评估Backpressure状态，所使用的堆栈跟踪调用次数 </td>
</tr>
<tr>
<td> jobmanager.web.backpressure.delay-between-samples </td>
<td style="text-align:left;"> 50 </td>
<td style="text-align:left;"> 默认50毫秒，表示对一个Job的每个Task依次调用的时间间隔 </td>
</tr>
</tbody>
</table>


<p>通过上面个定义的Backpressure状态，以及调整相应的参数，可以确定当前运行的Job的状态是否正常，并且保证不影响JobManager提供服务。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建Ray集群步骤]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/02/08/da-jian-rayji-qun-bu-zou/"/>
    <updated>2018-02-08T10:29:16+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/02/08/da-jian-rayji-qun-bu-zou</id>
    <content type="html"><![CDATA[<p>本文介绍如何搭建Ray 0.3集群环境。</p>

<p>可参考官方文档：</p>

<ul>
<li><a href="https://ray.readthedocs.io/en/latest/using-ray-on-a-cluster.html">https://ray.readthedocs.io/en/latest/using-ray-on-a-cluster.html</a></li>
</ul>


<!-- More -->


<h2>安装Ray</h2>

<p>首先在每台机器上安装如下组件。</p>

<h3>安装Anaconda</h3>

<p>首先安装Anaconda，下载：</p>

<ul>
<li>Anaconda2-4.3.0-Linux-x86_64.sh</li>
</ul>


<p>按提示执行安装即可。</p>

<h3>安装Ray依赖</h3>

<p>ray依赖如下库：</p>

<ul>
<li>six (>=1.0.0)</li>
<li>redis</li>
<li>pytest</li>
<li>psutil</li>
<li>numpy</li>
<li>funcsigs</li>
<li>flatbuffers</li>
<li>colorama</li>
<li>cloudpickle (==0.5.2)</li>
<li>click</li>
</ul>


<p>注意：</p>

<ul>
<li>如果机器环境通pip源则直接pip install即可。</li>
<li>如果不通可以在 <a href="https://pypi.python.org/pypi/ray/0.3.0">https://pypi.python.org/pypi/ray/0.3.0</a> 下载.whl包后上传到机器pip本地安装。</li>
</ul>


<h3>安装Ray 0.3</h3>

<p>如果环境通pip源</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>pip install ray
</span></code></pre></td></tr></table></div></figure>


<p>如果不通则在 <a href="https://pypi.python.org/pypi/ray/0.3.0">https://pypi.python.org/pypi/ray/0.3.0</a> 下载 ：</p>

<ul>
<li>ray-0.3.0-cp27-cp27mu-manylinux1_x86_64.whl</li>
</ul>


<p>然后执行：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>pip install ray-0.3.0-cp27-cp27mu-manylinux1_x86_64.whl
</span></code></pre></td></tr></table></div></figure>


<h2>搭建集群</h2>

<p>假设集群IP如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>192.168.0.1
</span><span class='line'>192.168.0.2
</span><span class='line'>192.168.0.3
</span><span class='line'>192.168.0.4
</span><span class='line'>192.168.0.5
</span><span class='line'>192.168.0.6
</span><span class='line'>192.168.0.7
</span><span class='line'>192.168.0.8
</span><span class='line'>192.168.0.9
</span><span class='line'>192.168.0.10
</span></code></pre></td></tr></table></div></figure>


<p>搭建集群环境如下：</p>

<h3>启动Head节点</h3>

<p>选一个节点作为Head节点，例如IP为：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>192.168.0.1
</span></code></pre></td></tr></table></div></figure>


<p>在head节点执行：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ray start --head --node-ip-address 192.168.0.1 --redis-port<span class="o">=</span>6379
</span></code></pre></td></tr></table></div></figure>


<p>执行后会启动Head节点相关的服务。</p>

<h3>启动Worker节点</h3>

<p>Worker节点IP为：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>192.168.0.2
</span><span class='line'>192.168.0.3
</span><span class='line'>192.168.0.4
</span><span class='line'>192.168.0.5
</span><span class='line'>192.168.0.6
</span><span class='line'>192.168.0.7
</span><span class='line'>192.168.0.8
</span><span class='line'>192.168.0.9
</span><span class='line'>192.168.0.10
</span></code></pre></td></tr></table></div></figure>


<p>在每台Worker节点上执行：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ray start --redis-address :6379 192.168.0.x --num-cpus 10
</span></code></pre></td></tr></table></div></figure>


<p>执行后会启动Worker节点相关服务，其中：</p>

<ul>
<li>192.168.0.x 为对应节点IP</li>
<li>num-cpu选项可以用于设置每台节点可用的cpu数，默认为机器总的cpu数。</li>
</ul>


<h3>停止集群</h3>

<p>Head节点与Worker节点服务的停止命令相同，执行：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ray stop
</span></code></pre></td></tr></table></div></figure>


<h2>连接集群</h2>

<p>使用如下方法建立连接：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">ray</span>
</span><span class='line'>
</span><span class='line'><span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">redis_address</span><span class="o">=</span><span class="s">&quot;192.168.0.1:6379&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tensorflow模型保存与加载方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa/"/>
    <updated>2017-12-10T16:57:13+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa</id>
    <content type="html"><![CDATA[<p>本文档介绍如何保存和读取Tensorflow变量和模型。</p>

<p>官方文档可参考：</p>

<ul>
<li><a href="https://www.tensorflow.org/programmers_guide/saved_model">https://www.tensorflow.org/programmers_guide/saved_model</a></li>
</ul>


<!--More-->


<h2>保存/读取变量</h2>

<p>本节介绍如何存取Tensorflow变量。注意Estimators会自动在model_dir中存取变量。</p>

<p><code>tf.train.Saver</code> 提供了存取模型的接口，其构造函数会在图中增加存取模型的op。</p>

<p>示例化的Saver对象提供方法来运行这些op并，设置checkpoint文件用于保存和恢复模型信息。</p>

<p>Saver关于将模型中定义的所有变量保存，如果你不了解加载模型的graph结构，可以参考后面的保存恢复模型一节。</p>

<p>TensorFlow将变量以二进制形式存储在文件中，保存的信息主要是变量名以及对应的Tensor的值。</p>

<h3>保存变量</h3>

<p>调用tf.train.Saver()来管理模型变量，示例代码：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># Create some variables.</span>
</span><span class='line'><span class="n">v1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;v1&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
</span><span class='line'><span class="n">v2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;v2&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">inc_v1</span> <span class="o">=</span> <span class="n">v1</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">v1</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'><span class="n">dec_v2</span> <span class="o">=</span> <span class="n">v2</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">v2</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Add an op to initialize the variables.</span>
</span><span class='line'><span class="n">init_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Add ops to save and restore all the variables.</span>
</span><span class='line'><span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Later, launch the model, initialize the variables, do some work, and save the</span>
</span><span class='line'><span class="c"># variables to disk.</span>
</span><span class='line'><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</span><span class='line'>  <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">init_op</span><span class="p">)</span>
</span><span class='line'>  <span class="c"># Do some work with the model.</span>
</span><span class='line'>  <span class="n">inc_v1</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</span><span class='line'>  <span class="n">dec_v2</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</span><span class='line'>  <span class="c"># Save the variables to disk.</span>
</span><span class='line'>  <span class="n">save_path</span> <span class="o">=</span> <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">&quot;/tmp/model.ckpt&quot;</span><span class="p">)</span>
</span><span class='line'>  <span class="k">print</span><span class="p">(</span><span class="s">&quot;Model saved in file: </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">save_path</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<h3>恢复变量</h3>

<p>同样使用tf.train.Saver来从checkpoint中恢复变量，示例代码：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Create some variables.</span>
</span><span class='line'><span class="n">v1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;v1&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</span><span class='line'><span class="n">v2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;v2&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Add ops to save and restore all the variables.</span>
</span><span class='line'><span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Later, launch the model, use the saver to restore variables from disk, and</span>
</span><span class='line'><span class="c"># do some work with the model.</span>
</span><span class='line'><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</span><span class='line'>  <span class="c"># Restore variables from disk.</span>
</span><span class='line'>  <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">&quot;/tmp/model.ckpt&quot;</span><span class="p">)</span>
</span><span class='line'>  <span class="k">print</span><span class="p">(</span><span class="s">&quot;Model restored.&quot;</span><span class="p">)</span>
</span><span class='line'>  <span class="c"># Check the values of the variables</span>
</span><span class='line'>  <span class="k">print</span><span class="p">(</span><span class="s">&quot;v1 : </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">v1</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
</span><span class='line'>  <span class="k">print</span><span class="p">(</span><span class="s">&quot;v2 : </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">v2</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure>


<h3>存取指定的变量</h3>

<p>有时可能只需要存取模型graph中的部分变量，可以list或者dict两种形式传给tf.train.Saver()来指定需要存取的变量。</p>

<ul>
<li>list形式，模型的变量列表。</li>
<li>dict形式，name为新的保存后的变量名，value为模型中的变量名。</li>
</ul>


<p>示例代码：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">tf</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
</span><span class='line'><span class="c"># Create some variables.</span>
</span><span class='line'><span class="n">v1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;v1&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
</span><span class='line'><span class="n">v2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">&quot;v2&quot;</span><span class="p">,</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Add ops to save and restore only `v2` using the name &quot;v2&quot;</span>
</span><span class='line'><span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">({</span><span class="s">&quot;v2&quot;</span><span class="p">:</span> <span class="n">v2</span><span class="p">})</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Use the saver object normally after that.</span>
</span><span class='line'><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</span><span class='line'>  <span class="c"># Initialize v1 since the saver will not.</span>
</span><span class='line'>  <span class="n">v1</span><span class="o">.</span><span class="n">initializer</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</span><span class='line'>  <span class="n">saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="s">&quot;/tmp/model.ckpt&quot;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">print</span><span class="p">(</span><span class="s">&quot;v1 : </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">v1</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
</span><span class='line'>  <span class="k">print</span><span class="p">(</span><span class="s">&quot;v2 : </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">v2</span><span class="o">.</span><span class="n">eval</span><span class="p">())</span>
</span></code></pre></td></tr></table></div></figure>


<p>注意：</p>

<ul>
<li>可以创建多个模型Saver对象来保存模型变量，同样的变量可被不同的Saver保存多次，restore操作用于从checkpoint中恢复变量的值。</li>
<li>如果checkpoint中只保存了部分变量，那么恢复后，graph中其他的变量仍需要被初始化。</li>
<li>查看checkpoint中的变量可以使用inspect_checkpoint库，特别是print_tensors_in_checkpoint_file函数。</li>
<li>Saver默认使用tf.Variable.name属性作为每个变量的变量名，然而你可以在Saver对象中未变量指定存储在checkpoint中的新名字。</li>
</ul>


<h2>保存/读取模型概述</h2>

<p>当你想要保存整个模型(变量、模型graph以及graph的meta信息)时，我们推荐使用<code>SavedModel</code>。</p>

<p>SavedModel是一种面向多语言的，可恢复的高度序列化封装的格式。</p>

<p>SavedModel运行上层系统或工具来生产、消费或者转换Tensorflow模型。</p>

<p>Tensorflow提供了多种机制来同SavedModel进行交互，包括tf.saved_model API, Estimator API 以及 CLI方式。</p>

<h2>操作SavedModel API</h2>

<p>本节聚焦在使用底层Tensorflow API时需要用到的保存或加载SavedModel的API。</p>

<h3>构建SavedModel</h3>

<p>我们提供了SavedModel builder的python实现。SavedModelBuilder提供保存MetaGraphDef结构的功能。</p>

<ul>
<li>MetaGraphDef是MetaGraph的proto buffer表达形式。</li>
<li>MetaGraph是一个数据流图，以及相关的变量、资源和signatures。</li>
<li>signature是一个graph的输入与输出的集合。</li>
</ul>


<p>每个加入到 SavedModel中的MetaGraphDef需要以用户指定的tag标注。tag提供了区分特定MetaGraphDef的方法。通常这些tag会标注MetaGraphDef的功能以及一些可选的硬件相关的信息。</p>

<p>SavedModelBuilder的使用示例代码如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">export_dir</span> <span class="o">=</span> <span class="o">...</span>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="n">builder</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">saved_model_builder</span><span class="o">.</span><span class="n">SavedModelBuilder</span><span class="p">(</span><span class="n">export_dir</span><span class="p">)</span>
</span><span class='line'><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">())</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</span><span class='line'>  <span class="o">...</span>
</span><span class='line'>  <span class="n">builder</span><span class="o">.</span><span class="n">add_meta_graph_and_variables</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span>
</span><span class='line'>                                       <span class="p">[</span><span class="n">tag_constants</span><span class="o">.</span><span class="n">TRAINING</span><span class="p">],</span>
</span><span class='line'>                                       <span class="n">signature_def_map</span><span class="o">=</span><span class="n">foo_signatures</span><span class="p">,</span>
</span><span class='line'>                                       <span class="n">assets_collection</span><span class="o">=</span><span class="n">foo_assets</span><span class="p">)</span>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="c"># Add a second MetaGraphDef for inference.</span>
</span><span class='line'><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">())</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</span><span class='line'>  <span class="o">...</span>
</span><span class='line'>  <span class="n">builder</span><span class="o">.</span><span class="n">add_meta_graph</span><span class="p">([</span><span class="n">tag_constants</span><span class="o">.</span><span class="n">SERVING</span><span class="p">])</span>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="n">builder</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>


<h3>加载SaveModel</h3>

<p>调用python版本的SaveModel loader需要提供一下信息：</p>

<ul>
<li>保存graph定义和变量的session。</li>
<li>用来标识MetaGraphDef的tag。</li>
<li>SavedModel对应的目录位置。</li>
</ul>


<p>示例代码如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">export_dir</span> <span class="o">=</span> <span class="o">...</span>
</span><span class='line'><span class="o">...</span>
</span><span class='line'><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">())</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
</span><span class='line'>  <span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="p">[</span><span class="n">tag_constants</span><span class="o">.</span><span class="n">TRAINING</span><span class="p">],</span> <span class="n">export_dir</span><span class="p">)</span>
</span><span class='line'>  <span class="o">...</span>
</span></code></pre></td></tr></table></div></figure>


<p>C++版本的SavedModel loader也提供从指定目录恢复模型的API，并支持指定SessionOptions和RunOptions参数。</p>

<p>同样需要指定taq参数，被加载的SavedModel被SavedModelBundle引用，包含了MetaGraphDef以及加载它的session信息。</p>

<p>示例代码：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='c++'><span class='line'><span class="k">const</span> <span class="n">string</span> <span class="n">export_dir</span> <span class="o">=</span> <span class="p">...</span>
</span><span class='line'><span class="n">SavedModelBundle</span> <span class="n">bundle</span><span class="p">;</span>
</span><span class='line'><span class="p">...</span>
</span><span class='line'><span class="n">LoadSavedModel</span><span class="p">(</span><span class="n">session_options</span><span class="p">,</span> <span class="n">run_options</span><span class="p">,</span> <span class="n">export_dir</span><span class="p">,</span> <span class="p">{</span><span class="n">kSavedModelTagTrain</span><span class="p">},</span>
</span><span class='line'>               <span class="o">&amp;</span><span class="n">bundle</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>另外Tensorflow提供了一组MetaGraphDef和SignatureDef相关的常量供用户使用。</p>

<p>MetaGraphDef常量：</p>

<ul>
<li>python：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/tag_constants.py">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/tag_constants.py</a></li>
<li>c++：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/tag_constants.h">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/tag_constants.h</a></li>
</ul>


<p>SignatureDef常量</p>

<ul>
<li>python：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/signature_constants.py">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/signature_constants.py</a></li>
<li>c++：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/signature_constants.h">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/signature_constants.h</a></li>
</ul>


<h2>在Estimators中使用SaveModel</h2>

<p>当训练好Estimator模型后，你可能需要部署预测服务。你可以选择在本机启动一个本地服务，或在云端进行扩展。</p>

<p>部署Estimator训练出的模型需要先把模型导出为SaveModel格式，本节介绍：</p>

<ul>
<li>如何制定输出节点以及相关的API</li>
<li>使用SavedModel导出模型</li>
<li>发请求给本地模型预测服务</li>
</ul>


<h3>准备Serving输入</h3>

<p>训练过程中 <code>input_fn()</code>用于提供数据输入，类似地，预测阶段输入数据由 <code>serving_input_receiver_fn</code>提供。</p>

<p>serving_input_receiver_fn有如下两个功能：</p>

<ul>
<li>将预测需要输入数据的placeholder添加到graph中。</li>
<li>添加额外的用于将输入数据格式转换为feature Tensors格式的op。</li>
</ul>


<p>函数返回 <code>tf.estimator.export.ServingInputReceiver</code>对象，封装了placeholders以及feature Tensor。</p>

<p>当编写 <code>serving_input_receiver_fn</code>时，需要提供一个 tf.parse_example的特定parser描述来说明数据解析的方式。</p>

<p>Parser说明是一个dict的形式，包含：</p>

<ul>
<li>tf.FixedLenFeature</li>
<li>tf.VarLenFeature</li>
<li>tf.SparseFeature</li>
</ul>


<p>示例代码：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">feature_spec</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;foo&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">FixedLenFeature</span><span class="p">(</span><span class="o">...</span><span class="p">),</span>
</span><span class='line'>                <span class="s">&#39;bar&#39;</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">VarLenFeature</span><span class="p">(</span><span class="o">...</span><span class="p">)}</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">serving_input_receiver_fn</span><span class="p">():</span>
</span><span class='line'>  <span class="sd">&quot;&quot;&quot;An input receiver that expects a serialized tf.Example.&quot;&quot;&quot;</span>
</span><span class='line'>  <span class="n">serialized_tf_example</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span>
</span><span class='line'>                                         <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">default_batch_size</span><span class="p">],</span>
</span><span class='line'>                                         <span class="n">name</span><span class="o">=</span><span class="s">&#39;input_example_tensor&#39;</span><span class="p">)</span>
</span><span class='line'>  <span class="n">receiver_tensors</span> <span class="o">=</span> <span class="p">{</span><span class="s">&#39;examples&#39;</span><span class="p">:</span> <span class="n">serialized_tf_example</span><span class="p">}</span>
</span><span class='line'>  <span class="n">features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">parse_example</span><span class="p">(</span><span class="n">serialized_tf_example</span><span class="p">,</span> <span class="n">feature_spec</span><span class="p">)</span>
</span><span class='line'>  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">export</span><span class="o">.</span><span class="n">ServingInputReceiver</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">receiver_tensors</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>tf.estimator.export.build_parsing_serving_input_receiver_fn 工具函数给出了一个通用实现。</p>

<h3>导出模型</h3>

<p>调用 <code>tf.estimator.Estimator.export_savedmodel</code> ，提供导出路径以及serving_input_receiver_fn进行模型导出：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">estimator</span><span class="o">.</span><span class="n">export_savedmodel</span><span class="p">(</span><span class="n">export_dir_base</span><span class="p">,</span> <span class="n">serving_input_receiver_fn</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>该方法创建一个新的graph并调用serving_input_receiver_fn来获取输入tensor，随后调用Estimator的<code>model_fn()</code> 来产生模型的graph。</p>

<p>最终会创建一个带时间戳的目录(export_dir_base/<timestamp>)并将模型导出为SavedModel。</p>

<h3>指定模型输出</h3>

<p>通过export_outputs指定，其类型为 <code>tf.estimator.EstimatorSpec</code>，是一个形如 {name: output} 的dict，用于描述预测阶段的输出。</p>

<p>预测输出的值类型必须为 <code>ExportOutput</code> 的某个实现，例如：</p>

<ul>
<li>tf.estimator.export.ClassificationOutput,</li>
<li>tf.estimator.export.RegressionOutput</li>
<li>tf.estimator.export.PredictOutput.</li>
</ul>


<h3>部署本地预测服务</h3>

<p>本地部署预测服务需要使用TensorFlow Serving。</p>

<p>TensorFlow Serving是一个独立的开源项目，功能是加载SavedModel模型并对外提供gRPC服务。</p>

<p>首先安装Tensorflow Serving。</p>

<p>部署服务命令如下，将 <code>$export_dir_base</code> 替换为SavedModel导出的目录。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>bazel build //tensorflow_serving/model_servers:tensorflow_model_server
</span><span class='line'>bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port<span class="o">=</span><span class="m">9000</span> --model_base_path<span class="o">=</span><span class="nv">$export_dir_base</span>
</span></code></pre></td></tr></table></div></figure>


<p>执行后在9000端口会启动一个gRPC预测服务。</p>

<h3>向本地Server发送请求</h3>

<p>发送预测请求需要通过PredictionService gRPC API。相关API依赖：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>  <span class="nv">deps</span> <span class="o">=</span> <span class="o">[</span>
</span><span class='line'>    <span class="s2">&quot;//tensorflow_serving/apis:classification_proto_py_pb2&quot;</span>,
</span><span class='line'>    <span class="s2">&quot;//tensorflow_serving/apis:regression_proto_py_pb2&quot;</span>,
</span><span class='line'>    <span class="s2">&quot;//tensorflow_serving/apis:predict_proto_py_pb2&quot;</span>,
</span><span class='line'>    <span class="s2">&quot;//tensorflow_serving/apis:prediction_service_proto_py_pb2&quot;</span>
</span><span class='line'>  <span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>python代码中可以像如下示例使用：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">tensorflow_serving.apis</span> <span class="kn">import</span> <span class="n">classification_pb2</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">tensorflow_serving.apis</span> <span class="kn">import</span> <span class="n">regression_pb2</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">tensorflow_serving.apis</span> <span class="kn">import</span> <span class="n">predict_pb2</span>
</span><span class='line'><span class="kn">from</span> <span class="nn">tensorflow_serving.apis</span> <span class="kn">import</span> <span class="n">prediction_service_pb2</span>
</span></code></pre></td></tr></table></div></figure>


<p>请求的数据会以proto buffer的形式发送，发送请求的示例代码：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">grpc.beta</span> <span class="kn">import</span> <span class="n">implementations</span>
</span><span class='line'>
</span><span class='line'><span class="n">channel</span> <span class="o">=</span> <span class="n">implementations</span><span class="o">.</span><span class="n">insecure_channel</span><span class="p">(</span><span class="n">host</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">port</span><span class="p">))</span>
</span><span class='line'><span class="n">stub</span> <span class="o">=</span> <span class="n">prediction_service_pb2</span><span class="o">.</span><span class="n">beta_create_PredictionService_stub</span><span class="p">(</span><span class="n">channel</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="n">request</span> <span class="o">=</span> <span class="n">classification_pb2</span><span class="o">.</span><span class="n">ClassificationRequest</span><span class="p">()</span>
</span><span class='line'><span class="n">example</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">input</span><span class="o">.</span><span class="n">example_list</span><span class="o">.</span><span class="n">examples</span><span class="o">.</span><span class="n">add</span><span class="p">()</span>
</span><span class='line'><span class="n">example</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">feature</span><span class="p">[</span><span class="s">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">float_list</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">))</span>
</span><span class='line'>
</span><span class='line'><span class="n">result</span> <span class="o">=</span> <span class="n">stub</span><span class="o">.</span><span class="n">Classify</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>  <span class="c"># 10 secs timeout</span>
</span></code></pre></td></tr></table></div></figure>


<p>本例中的返回值是一个 <code>ClassificationResponse</code>格式的PB数据。</p>

<h2>SavedModel目录结构</h2>

<p>Tensorflow为每个SavedModel组织目录结构形式如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>assets/
</span><span class='line'>assets.extra/
</span><span class='line'>variables/
</span><span class='line'>    variables.data-?????-of-?????
</span><span class='line'>    variables.index
</span><span class='line'>saved_model.pb<span class="p">|</span>saved_model.pbtxt
</span></code></pre></td></tr></table></div></figure>


<p>说明如下：</p>

<ul>
<li>assets：是一个子目录，包含了一些外部文件，例如词表等，这些资源文件会被特定的MetaGraphDef读取使用。</li>
<li>assets.extra：是一个子目录，用于上层应用或者用户添加一些自己的资源文件，但不会被模型的graph加载。该目录不由SavedModel管理。</li>
<li>variables：是一个子目录，用于存储tf.train.Saver的输出。</li>
<li>saved_model.pb/saved_model.pbtxt SavedModel的Proto Buffer描述。包含了MetaGraphDef的proto buffer形式的定义。</li>
</ul>


<p>一个单独的SavedModel可以表达多个graph，SavedModel的多个graph共享一组checkpoint(变量和资源文件)。</p>

<p>组织形式如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/10-savemodel.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[论文阅读: On-line Random Forest]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/11/30/on-line-random-forest-paper/"/>
    <updated>2017-11-30T21:25:38+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/11/30/on-line-random-forest-paper</id>
    <content type="html"><![CDATA[<p>2009年的一篇论文, 提出了一种在线随机森林算法（ORF）。</p>

<p>原文链接：<a href="http://ieeexplore.ieee.org/abstract/document/5457447/?reload=true">http://ieeexplore.ieee.org/abstract/document/5457447/?reload=true</a></p>

<!-- More -->


<h2>本文解决的问题</h2>

<p>经典的随机森林算法是离线训练，每次要基于全局数据生成一系列决策树，但在线环境下难以获得全局数据，因而无法直接使用。</p>

<p>本文提出的算法设计参考了online-bagging的思路，效果接近离线版本的算法。</p>

<h2>核心内容</h2>

<p>本文提出的在线online random forest算法中，每棵树可以在线分裂。样本采样时每棵树采样次数基于泊松分布，每个叶子分裂的条件是预测的数量要达到一定的值和每个叶子节点信息。</p>

<p>每个树的生长主要通过在线接收的实时样本数据，每棵树的叶子节点分裂主要根据该节点的熵或Gini系数。</p>

<p>算法流程如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/07-online_random_forest.png"></p>

<p>说明如下：</p>

<ul>
<li>步骤3. 用个possion分布确定从采样的次数，其原理见online boosting： <a href="http://www.cnblogs.com/liqizhou/archive/2012/05/10/2494145.html">http://www.cnblogs.com/liqizhou/archive/2012/05/10/2494145.html</a></li>
<li>步骤6. u代表分类的类别。</li>
<li>步骤7. j代表第t棵树上叶子节点。</li>
<li>步骤8. 统计第j个叶子节点的数目和计算Gini。</li>
<li>步骤9. 判断条件是否分裂的二个条件。</li>
<li>步骤10. 在符合条件的叶子节点中，选择一个Gini最大的叶子节点作为分类节点。</li>
</ul>


<p>有时在线训练过程中希望丢弃较旧的信息，需要适当丢弃随机森林中的某些树。针对这种需求，原文给出了随机森林中树的选择性丢弃方案。</p>

<p>算法流程如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/08-orf_temporal_knowledge_weighting.png"></p>

<p>附C++源码实现：</p>

<ul>
<li><a href="https://github.com/amirsaffari/online-random-forests">https://github.com/amirsaffari/online-random-forests</a></li>
</ul>


<h2>本文解决方案效果</h2>

<p>随着样本数量增加，算法效果逼近离线随机森林算法。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Landslide基于MarkDown制作在线Slide]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/11/26/shi-yong-landslideji-yu-markdownzhi-zuo-zai-xian-slide/"/>
    <updated>2017-11-26T19:05:19+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/11/26/shi-yong-landslideji-yu-markdownzhi-zuo-zai-xian-slide</id>
    <content type="html"><![CDATA[<p>本文介绍使用Landslide工具基于Markdown语法制作在线Slide的方法。</p>

<!-- More -->


<h2>Landsilde工具使用</h2>

<p>Landslide是基于Google的<code>html5slides</code>的一个Slide生成工具，可将markdown、ReST 或者 textile文件转化成HTML5的slide。</p>

<p>该转化支持内联模式，即生成一个具有完整功能的HTML文件，将依赖的css等东西放入其中，很容易用来分享。</p>

<p>类似的还有工具还有 remark，相关gitlab项目主页：</p>

<ul>
<li><a href="https://github.com/adamzap/landslide">https://github.com/adamzap/landslide</a></li>
<li><a href="https://github.com/gnab/remark/">https://github.com/gnab/remark/</a></li>
</ul>


<h3>安装landslide</h3>

<p>  方案一 pip安装：</p>

<pre><code class="``bash"> $ pip install landslide
</code></pre>

<p> 方案二 源码安装：</p>

<pre><code class="`bash">$ git clone https://github.com/adamzap/landslide.git
$ cd landslide
$ python setup.py build
$ sudo python setup.py install
</code></pre>

<h3>Markdown书写Slide内容</h3>

<p>以markdown语法书写，可以参考landslide提供的示例：</p>

<ul>
<li><a href="https://github.com/adamzap/landslide/blob/master/examples/markdown/slides.md">https://github.com/adamzap/landslide/blob/master/examples/markdown/slides.md</a></li>
</ul>


<h3>生成PPT页面</h3>

<p>以官方提供的markdown文本为例，文件命名为test.md，完成后执行如下命令生成HTML内容：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>landslide file.md -i -o &gt; test.html
</span></code></pre></td></tr></table></div></figure>


<p>命令行参数说明详见下文介绍。</p>

<p>可以直接在浏览器中打开观察效果：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>open test.html
</span></code></pre></td></tr></table></div></figure>


<p>支持快捷键，可左右切换slide，详见下文快捷键介绍。</p>

<p><img src="http://lionheartwang.github.io/images/blog/06-landslide_demo.png"></p>

<h2>使用PrinceXML生成PDF</h2>

<p>PrinceXML是一款将html转换为pdf的工具，提供免费带水印版试用。</p>

<p>下载地址：</p>

<ul>
<li><a href="http://www.princexml.com/download/">http://www.princexml.com/download/</a></li>
</ul>


<p>选择对应版本解压后执行脚本安装：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>sh insatll.sh
</span></code></pre></td></tr></table></div></figure>


<p>指定路径安装完毕后即可使用prince命令。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$ </span>prince test.html -o test.pdf
</span></code></pre></td></tr></table></div></figure>


<p>即可生成需要的PDF PPT文件。</p>

<h2>Landslide快捷键及命令参数介绍</h2>

<h3>PPT页面快捷键</h3>

<p>快捷键如下：</p>

<blockquote><p>Press h to toggle display of help</p>

<p>Press left arrow and right arrow to navigate</p>

<p>Press t to toggle a table of contents for your presentation. Slide titles are links</p>

<p>Press ESC to display the presentation overview (Exposé)</p>

<p>Press n to toggle slide number visibility</p>

<p>Press b to toggle screen blanking</p>

<p>Press c to toggle current slide context (previous and next slides)</p>

<p>Press e to make slides filling the whole available space within the document body</p>

<p>Press S to toggle display of link to the source file for each slide</p>

<p>Press &lsquo;2&rsquo; to toggle notes in your slides (specify with the .notes macro)</p>

<p>Press &lsquo;3&rsquo; to toggle pseudo-3D display (experimental)</p>

<p>Browser zooming is supported</p></blockquote>

<h3>命令行参数介绍</h3>

<p>landslide命令行参数介绍如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>-h, --help            show this <span class="nb">help </span>message and <span class="nb">exit</span>
</span><span class='line'>-c, --copy-theme      Copy theme directory into current presentation <span class="nb">source </span>directory
</span><span class='line'>-b, --debug           Will display any exception trace to stdin
</span><span class='line'>-d FILE, --destination<span class="o">=</span>FILE
</span><span class='line'>                      The path to the to the destination file: .html or .pdf
</span><span class='line'>                      extensions allowed <span class="o">(</span>default: presentation.html<span class="o">)</span>
</span><span class='line'>-e ENCODING, --encoding<span class="o">=</span>ENCODING
</span><span class='line'>                      The encoding of your files <span class="o">(</span>defaults to utf8<span class="o">)</span>
</span><span class='line'>-i, --embed     Embed stylesheet and javascript contents,
</span><span class='line'>                      base64-encoded images in presentation to make a
</span><span class='line'>                      standalone document
</span><span class='line'>-l LINENOS, --linenos<span class="o">=</span>LINENOS
</span><span class='line'>                      How to output linenos in <span class="nb">source </span>code. Three options
</span><span class='line'>                      availables: no <span class="o">(</span>no line numbers<span class="o">)</span><span class="p">;</span> inline inside pre tag
</span><span class='line'>                      table <span class="o">(</span>lines numbers in another cell, copy-paste friendly<span class="o">)</span>
</span><span class='line'>-o, --direct-output   Prints the generated HTML code to stdin<span class="p">;</span> won<span class="s1">&#39;t work with PDF export</span>
</span><span class='line'><span class="s1">-q, --quiet           Won&#39;</span>t write anything to stdin <span class="o">(</span>silent mode<span class="o">)</span>
</span><span class='line'>-r, --relative   Make your presentation asset links relative to current
</span><span class='line'>                      <span class="nb">pwd</span><span class="p">;</span> This may be useful <span class="k">if</span> you intend to publish your
</span><span class='line'>                      html presentation online.
</span><span class='line'>-t THEME, --theme<span class="o">=</span>THEME
</span><span class='line'>                      A theme name, or path to a landlside theme directory
</span><span class='line'>-v, --verbose  Write informational messages to stdin <span class="o">(</span>enabled by default<span class="o">)</span>
</span><span class='line'>-w, --watch    Watch the <span class="nb">source </span>directory <span class="k">for</span> changes and auto-regenerate the presentation
</span><span class='line'>-x EXTENSIONS, --extensions<span class="o">=</span>EXTENSIONS
</span><span class='line'>                      Comma-separated list of extensions <span class="k">for</span> Markdown
</span><span class='line'>-m, --math-output     Enable mathematical output using mathjax
</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Fuse挂载HDFS到本地目录方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/11/14/gua-zai-hdfsdao-ben-di-mu-lu-fang-fa/"/>
    <updated>2017-11-14T00:10:27+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/11/14/gua-zai-hdfsdao-ben-di-mu-lu-fang-fa</id>
    <content type="html"><![CDATA[<p>网上关于挂载HDFS到本地的介绍大多基于较早版本的Hadoop。
本文以Hadoop-2.8.0为例，介绍通过Fuse挂载HDFS到本地的方法。</p>

<!--more-->


<h2>安装Fuse</h2>

<p>对每台节点，执行如下命令一键安装</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo yum -y install fuse fuse-libs
</span></code></pre></td></tr></table></div></figure>


<h2>编译fuse-dfs工具</h2>

<p>下载hadoop-2.8.0源码，解压编译</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>tar zxvf hadoop-2.8.0.tar.gz
</span><span class='line'><span class="nb">cd </span>hadoop-2.8.0
</span><span class='line'>mvn package -Drequire.fuse<span class="o">=</span><span class="nb">true</span> -DskipTests -Dmaven.javadoc.skip<span class="o">=</span><span class="nb">true</span> -Dtar
</span></code></pre></td></tr></table></div></figure>


<p>编译后会生成fuse_dfs的可执行文件，位于</p>

<blockquote><p>./hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs/fuse_dfs</p></blockquote>

<p>另外会生成一个对该可执行程序的封装脚本，位于</p>

<blockquote><p>./hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh</p></blockquote>

<h2>配置环境变量</h2>

<p>可以为fuse_dfs_wrapper.sh建立软链接到当前目录方便后续使用。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ln -s /&lt;Hadoop源码路径&gt;/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh .
</span></code></pre></td></tr></table></div></figure>


<p>编辑fuse_dfs_wrapper.sh内容，有一些需要根据具体情况修改：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">HADOOP_HOME</span><span class="o">=</span>/path/to/your/hadoop
</span><span class='line'><span class="nv">HADOOP_PREFIX</span><span class="o">=</span>/path/to/your/hadoop/src/
</span><span class='line'>
</span><span class='line'><span class="nb">export </span><span class="nv">FUSEDFS_PATH</span><span class="o">=</span><span class="s2">&quot;$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs&quot;</span>
</span><span class='line'><span class="nb">export </span><span class="nv">LIBHDFS_PATH</span><span class="o">=</span><span class="s2">&quot;$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs-native-client/target/native/target/usr/local/lib&quot;</span>
</span><span class='line'><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/etc/hadoop
</span><span class='line'>
</span><span class='line'><span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;$OS_ARCH&quot;</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span><span class='line'>  <span class="nb">export </span><span class="nv">OS_ARCH</span><span class="o">=</span>amd64
</span><span class='line'><span class="k">fi</span>
</span><span class='line'>
</span><span class='line'><span class="c"># 这里需要替换为JDK的安装路径</span>
</span><span class='line'><span class="nv">JAVA_HOME</span><span class="o">=</span>/home/yiguang.wyg/tools/jdk1.8.0_121
</span><span class='line'>
</span><span class='line'><span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;$LD_LIBRARY_PATH&quot;</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
</span><span class='line'>  <span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$JAVA_HOME</span>/jre/lib/<span class="nv">$OS_ARCH</span>/server:/usr/local/lib
</span><span class='line'><span class="k">fi</span>
</span><span class='line'>
</span><span class='line'><span class="nv">JARS</span><span class="o">=</span><span class="sb">`</span>find <span class="s2">&quot;$HADOOP_PREFIX/hadoop-hdfs-project&quot;</span> -name <span class="s2">&quot;*.jar&quot;</span> <span class="p">|</span> xargs<span class="sb">`</span>
</span><span class='line'><span class="k">for</span> jar in <span class="nv">$JARS</span><span class="p">;</span> <span class="k">do</span>
</span><span class='line'>  <span class="nv">CLASSPATH</span><span class="o">=</span><span class="nv">$jar</span>:<span class="nv">$CLASSPATH</span>
</span><span class='line'><span class="k">done</span>
</span><span class='line'>
</span><span class='line'><span class="nv">JARS</span><span class="o">=</span><span class="sb">`</span>find <span class="s2">&quot;$HADOOP_PREFIX/hadoop-client&quot;</span> -name <span class="s2">&quot;*.jar&quot;</span> <span class="p">|</span> xargs<span class="sb">`</span>
</span><span class='line'><span class="k">for</span> jar in <span class="nv">$JARS</span><span class="p">;</span> <span class="k">do</span>
</span><span class='line'>  <span class="nv">CLASSPATH</span><span class="o">=</span><span class="nv">$jar</span>:<span class="nv">$CLASSPATH</span>
</span><span class='line'><span class="k">done</span>
</span><span class='line'>
</span><span class='line'><span class="nb">export </span><span class="nv">CLASSPATH</span><span class="o">=</span><span class="nv">$HADOOP_CONF_DIR</span>:<span class="nv">$CLASSPATH</span>
</span><span class='line'><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$FUSEDFS_PATH</span>:<span class="nv">$PATH</span>
</span><span class='line'><span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LIBHDFS_PATH</span>:<span class="nv">$JAVA_HOME</span>/jre/lib/<span class="nv">$OS_ARCH</span>/server:<span class="nv">$LD_LIBRARY_PATH</span>
</span><span class='line'>
</span><span class='line'>fuse_dfs <span class="s2">&quot;$@&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>重点需要配置好HADOOP_HOME和HADOOP_PREFIX，分别为hadoop安装路径和hadoop源码路径。</p>

<h2>挂载HDFS</h2>

<p>挂载HDFS之前需要确保HDFS已经启动。</p>

<p>创建挂载目录</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo mkdir /mnt/hdfs
</span></code></pre></td></tr></table></div></figure>


<p>执行：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo sh fuse_dfs_wrapper.sh hdfs://&lt;hdfs路径&gt; /mnt/hdfs
</span></code></pre></td></tr></table></div></figure>


<p>输出：</p>

<blockquote><p>INFO /&hellip;/hadoop-2.8.0-src/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_options.c:164 Adding FUSE arg /mnt/hdfs</p></blockquote>

<p>进入挂载目录，如果能访问到HDFS中的内容，说明挂载成功。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nb">cd</span> /mnt/hdfs
</span><span class='line'>ls
</span></code></pre></td></tr></table></div></figure>


<p>挂载成功后，就可以将HDFS当做本地路径使用了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux NFS配置及使用方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/10/24/linux-nfs-configuration/"/>
    <updated>2017-10-24T00:27:55+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/10/24/linux-nfs-configuration</id>
    <content type="html"><![CDATA[<p>NFS是Network  File System（网络文件系统）。主要功能是通过网络让不同的服务器之间可以共享文件或者目录。</p>

<p>本文以CentOS系统为例介绍Linux下NFS的配置和使用方法。</p>

<p>NFS在文件传送过程中依赖与RPC（远程过程调用）协议，配置步骤介绍如下。</p>

<!-- more -->


<h2>安装软件包</h2>

<p>安装nfs软件包，server和client机器都需要安装</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo yum -y install nfs-utils nfs-utils-lib
</span></code></pre></td></tr></table></div></figure>


<h2>配置NFS Server</h2>

<p>假设用来共享目录的NFS Server机器是192.168.0.1，需要登录该机器配置挂载目录。</p>

<p>登录后编辑/etc/exports文件内容，输入需要挂载的机器IP，以及服务端共享的路径。</p>

<p>假设需要访问NFS共享目录的机器为192.168.0.2 ~ 192.168.0.6，配置/etc/exports如下所示：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>/path/to/mount 192.168.0.2<span class="o">(</span>rw,sync,no_root_squash<span class="o">)</span>
</span><span class='line'>/path/to/mount 192.168.0.3<span class="o">(</span>rw,sync,no_root_squash<span class="o">)</span>
</span><span class='line'>/path/to/mount 192.168.0.4<span class="o">(</span>rw,sync,no_root_squash<span class="o">)</span>
</span><span class='line'>/path/to/mount 192.168.0.5<span class="o">(</span>rw,sync,no_root_squash<span class="o">)</span>
</span><span class='line'>/path/to/mount 192.168.0.6<span class="o">(</span>rw,sync,no_root_squash<span class="o">)</span>
</span><span class='line'>...
</span></code></pre></td></tr></table></div></figure>


<p>其中/path/to/mount根据需要配置为欲挂载的路径即可。</p>

<h2>启动NFS Server</h2>

<p>在192.168.0.1上执行如下命令启动NFS Server</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo service rpcbind restart
</span><span class='line'>sudo service nfs restart
</span></code></pre></td></tr></table></div></figure>


<p>执行如下命令，则会显示挂载的机器IP列表：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>showmount -e 192.168.0.1
</span></code></pre></td></tr></table></div></figure>


<h2>配置NFS Client</h2>

<p>分别登陆每台需要挂载的client机器，运行如下命令进行共享文件夹挂载。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo mkdir /target/to/mount
</span><span class='line'>sudo mount -t nfs 192.168.0.1:/path/to/mount /target/to/mount
</span></code></pre></td></tr></table></div></figure>


<p>其中：</p>

<ul>
<li>/path/to/mount为之前配置的NFS Server上挂载的路径。</li>
<li>/target/to/mount为本地机器欲挂载到的目标路径。</li>
</ul>


<p>挂载完成后，在192.168.0.2 ~ 192.168.0.6上就可以向访问本地路径一样访问192.168.0.1的/path/to/mount目录了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker常用命令介绍]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/06/18/dockerchang-yong-ming-ling-jie-shao/"/>
    <updated>2017-06-18T22:52:05+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/06/18/dockerchang-yong-ming-ling-jie-shao</id>
    <content type="html"><![CDATA[<p>本文介绍常用的docker命令。</p>

<!--more-->


<h2>镜像相关</h2>

<p>查看本地具有的镜像</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo docker images</span></code></pre></td></tr></table></div></figure>


<p>登录远程镜像仓库</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker login &lt;镜像仓库地址&gt;</span></code></pre></td></tr></table></div></figure>


<p>例如：</p>

<blockquote><p>docker login test.lionheart.com</p></blockquote>

<p>命名镜像</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker tag &lt;镜像id&gt; &lt;镜像仓库地址&gt;/&lt;镜像空间名&gt;/&lt;镜像名&gt;:&lt;镜像tag&gt;
</span></code></pre></td></tr></table></div></figure>


<p>命名镜像后可推送镜像到远端仓库：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker push &lt;镜像仓库地址&gt;/&lt;镜像空间名&gt;/&lt;镜像名&gt;:&lt;镜像tag&gt;
</span></code></pre></td></tr></table></div></figure>


<p>例如：</p>

<blockquote><p>docker tag 12ab34cd56ef test.lionheart.com/wangyiguang/dockertest:1.0</p>

<p>docker push test.lionheart.com/wangyiguang/dockertest:1.0</p></blockquote>

<h2>容器相关</h2>

<p>启动container：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo docker run -tid --name &lt;镜像名&gt; --net<span class="o">=</span>host -l <span class="s2">&quot;&lt;资源配置&gt;&quot;</span> &lt;镜像id&gt; /bin/bash
</span></code></pre></td></tr></table></div></figure>


<p>示例：</p>

<blockquote><p>sudo docker run -tid &ndash;name lionhearttest &ndash;net=host  \</p>

<p>-l &ldquo;GpuCount=1&rdquo; -l &ldquo;PerGpuCache=100000&rdquo; 12ab34cd56ef /bin/bash</p></blockquote>

<p>查看所有container：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo docker ps -a
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>docker tag image_id <span class="nv">$image_center_addr</span>/<span class="nv">$image_namespace</span>/<span class="nv">$IMAGE_NAME</span>:<span class="nv">$TAG</span>
</span></code></pre></td></tr></table></div></figure>


<p>以当前容器创建镜像：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>sudo docker commit &lt;容器id&gt; &lt;镜像名&gt;
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用InteliJ远程调试程序]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/04/17/remote-debug-via-intelij/"/>
    <updated>2017-04-17T18:50:33+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/04/17/remote-debug-via-intelij</id>
    <content type="html"><![CDATA[<p>本文介绍如何使用InteliJ远程调试Java/Scala程序。</p>

<!--more-->


<h2>以调试模式启动程序</h2>

<p>JVM添加如下启动参数：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">JAVA_DEBUG_OPTS</span><span class="o">=</span><span class="s2">&quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=&lt;debugPort&gt;&quot;</span>
</span><span class='line'>java <span class="nv">$JAVA_DEBUG_OPTS</span> &lt;className&gt;
</span></code></pre></td></tr></table></div></figure>


<p>其中：</p>

<ul>
<li>debugPort为调试服务端口，供InteliJ调试器连接，后面会用到。</li>
<li>className为待执行的程序类。</li>
</ul>


<p>启动后会提示如下信息：</p>

<blockquote><p>Listening for transport dt_socket at address: xxxx</p></blockquote>

<h2>调试器远程连接</h2>

<p>在InteliJ菜单选择<code>Run</code> -> <code>Edit Configuration</code>，进入运行配置界面。</p>

<p>选择<code>+</code>添加一项新的配置，内容如下图所示：</p>

<p><img src="http://lionheartwang.github.io/images/blog/04-remoteDebug01.png"></p>

<p>填好配置名称，IP、端口即可。IP、端口要和之前远程启动的程序一致。</p>

<p>然后就可以在源码中设置断点，在InteliJ中进行调试了。</p>

<p><img src="http://lionheartwang.github.io/images/blog/05-remoteDebug02.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mac安装Thrift方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/03/14/intall-thrift-on-mac/"/>
    <updated>2017-03-14T17:25:31+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/03/14/intall-thrift-on-mac</id>
    <content type="html"><![CDATA[<p>新版mac使用brew默认安装的是最新版本的thrift 0.10.1</p>

<p>但现有项目主要依赖0.9.x，因此需要在mac上手动安装thrift。</p>

<p>本文介绍如何在新版本Mac上安装thrift 0.9.3.</p>

<!--more-->


<h2>安装依赖</h2>

<p>确保安装了如下依赖：</p>

<ul>
<li>openssl</li>
<li>libevent</li>
<li>bison 版本> 2.5</li>
</ul>


<p>注意：</p>

<p>mac高版本安装openssl之后可能提示openssl为keg-only的版本</p>

<p>需要记下其安装目录，默认为/usr/local/opt/openssl.</p>

<h2>下载thrift</h2>

<p>以0.9.3为例：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$wget</span> http://archive.apache.org/dist/thrift/0.9.3/thrift-0.9.3.tar.gz
</span></code></pre></td></tr></table></div></figure>


<p>下载后解压：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$tar</span> zxvf thrift-0.9.3.tar.gz
</span></code></pre></td></tr></table></div></figure>


<h2>编译安装thrift</h2>

<p>运行configure进行配置，注意这里指定了前面记下的openssl的路径：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$.</span>/configure --with-openssl<span class="o">=</span>/usr/local/opt/openssl --without-perl --without-php
</span></code></pre></td></tr></table></div></figure>


<p>编译：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$.</span>/make
</span></code></pre></td></tr></table></div></figure>


<p>安装：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$.</span>/make install
</span></code></pre></td></tr></table></div></figure>


<p>安装后运行：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$thrift</span> --version
</span></code></pre></td></tr></table></div></figure>


<p>输出：Thrift version 0.9.3</p>

<p>表明<font color=green><b>安装成功</b></font>。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ZooKeeper安装使用指南]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/01/20/zookeeperan-zhuang-shi-yong-zhi-nan/"/>
    <updated>2017-01-20T15:56:22+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/01/20/zookeeperan-zhuang-shi-yong-zhi-nan</id>
    <content type="html"><![CDATA[<p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务。</p>

<p>作为分布式应用提供一致性服务的软件，ZooKeeper 封装了易错的关键服务，提供简单高效、功能稳定接口给用户</p>

<p>本文介绍 ZooKeeper 的配置方法和客户端使用方法。</p>

<!--more-->


<h2>ZooKeeper 安装</h2>

<p>以ZooKeeper 3.4.8为例，下载 <a href="http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.8/">ZooKeeper 3.4.8</a>
下载解压后配置conf/zoo.cfg，配置clientPort，dataDir等。
示例配置：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># The number of milliseconds of each tick</span>
</span><span class='line'><span class="nv">tickTime</span><span class="o">=</span>2000
</span><span class='line'><span class="c"># The number of ticks that the initial synchronization phase can take</span>
</span><span class='line'><span class="nv">initLimit</span><span class="o">=</span>10
</span><span class='line'><span class="c"># The number of ticks that can pass between sending a request and getting an acknowledgement</span>
</span><span class='line'><span class="nv">syncLimit</span><span class="o">=</span>5
</span><span class='line'><span class="c"># the directory where the snapshot is stored. do not use /tmp for storage, /tmp here is just example sakes.</span>
</span><span class='line'><span class="nv">dataDir</span><span class="o">=</span>/tmp/zookeeper
</span><span class='line'><span class="c"># the port at which the clients will connect</span>
</span><span class='line'><span class="nv">clientPort</span><span class="o">=</span>2181
</span></code></pre></td></tr></table></div></figure>


<h2>ZooKeeper 使用</h2>

<p>配置好Zk后需要先启动ZkServer，然后可以用Zk Client直接以命令行的方式操作Zk。</p>

<h3>Server端</h3>

<p>配置好后启动zk：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$sh</span> bin/zkServer.sh start &gt; zookeeper.out
</span></code></pre></td></tr></table></div></figure>


<h3>Client端</h3>

<p>ZooKeeper客户端的使用非常简单，启动：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># ip和端口根据启动情况修改</span>
</span><span class='line'><span class="nv">$sh</span> bin/zkCli.sh -server 127.0.0.1:2181
</span></code></pre></td></tr></table></div></figure>


<p>之后可以用ls、delete、get等命令查询或修改各ZK节点的值。命令帮助如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ZooKeeper -server host:port cmd args
</span><span class='line'>  connect host:port
</span><span class='line'>  get path <span class="o">[</span>watch<span class="o">]</span>
</span><span class='line'>  ls path <span class="o">[</span>watch<span class="o">]</span>
</span><span class='line'>  <span class="nb">set </span>path data <span class="o">[</span>version<span class="o">]</span>
</span><span class='line'>  rmr path
</span><span class='line'>  delquota <span class="o">[</span>-n<span class="p">|</span>-b<span class="o">]</span> path
</span><span class='line'>  quit
</span><span class='line'>  printwatches on<span class="p">|</span>off
</span><span class='line'>  create <span class="o">[</span>-s<span class="o">]</span> <span class="o">[</span>-e<span class="o">]</span> path data acl
</span><span class='line'>  stat path <span class="o">[</span>watch<span class="o">]</span>
</span><span class='line'>  close
</span><span class='line'>  ls2 path <span class="o">[</span>watch<span class="o">]</span>
</span><span class='line'>  <span class="nb">history</span>
</span><span class='line'><span class="nb"> </span>listquota path
</span><span class='line'>  setAcl path acl
</span><span class='line'>  getAcl path
</span><span class='line'>  sync path
</span><span class='line'>  redo cmdno
</span><span class='line'>  addauth scheme auth
</span><span class='line'>  delete path <span class="o">[</span>version<span class="o">]</span>
</span><span class='line'>  setquota -n<span class="p">|</span>-b val path
</span></code></pre></td></tr></table></div></figure>


<h2>ZooKeeper API</h2>

<p>除了通过客户端操作ZooKeeper，还可以调用ZooKeeper提供的API操作ZooKeeper的节点。</p>

<p>这里以ZooKeeper 3.4.5为例，介绍常用的几个Java API。</p>

<h3>建立连接</h3>

<p>在应用程序中使用Zk需要先创建ZooKeeper对象，后续的操作都是基于该对象进行的。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="nf">ZooKeeper</span><span class="o">(</span><span class="n">String</span> <span class="n">connectString</span><span class="o">,</span> <span class="kt">int</span> <span class="n">sessionTimeout</span><span class="o">,</span> <span class="n">Watcher</span> <span class="n">watcher</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span>
</span></code></pre></td></tr></table></div></figure>


<p>参数说明：</p>

<ul>
<li>connectString： zookeeper server列表, 以逗号隔开。ZooKeeper对象初始化后, 将从列表中选择一个server, 并尝试建立连接。如果失败,则会从剩余项中选择并再次尝试建立连接。</li>
<li>sessionTimeout：指定连接的超时时间.</li>
<li>watcher： 事件回调接口。</li>
</ul>


<h3>创建/删除znode</h3>

<p>ZooKeeper对象的create/delete方法用于创建/删除 znode。如果该node存在, 则返回该node的状态信息, 否则返回null。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="n">String</span> <span class="nf">create</span><span class="o">(</span><span class="n">String</span> <span class="n">path</span><span class="o">,</span> <span class="kt">byte</span><span class="o">[]</span> <span class="n">data</span><span class="o">,</span> <span class="n">List</span> <span class="n">acl</span><span class="o">,</span> <span class="n">CreateMode</span> <span class="n">createMode</span><span class="o">);</span>
</span><span class='line'><span class="kd">public</span> <span class="kt">void</span> <span class="nf">delete</span><span class="o">(</span><span class="kd">final</span> <span class="n">String</span> <span class="n">path</span><span class="o">,</span> <span class="kt">int</span> <span class="n">version</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>参数说明：</p>

<ul>
<li>path： znode的路径。</li>
<li>data：与znode关联的数据。</li>
<li>acl：指定权限信息</li>
<li>createMode：指定znode类型，按持久化节点与临时节点，以及自动编号节点与非自动编号节点两个维度划分，共4类。</li>
<li>version：指定要更新的数据的版本, 如果version和真实的版本不同, 更新操作将失败.。指定version为-1则忽略版本检查。</li>
</ul>


<h3>获取子znode列表</h3>

<p>ZooKeeper对象的getChildren方法用于获取子node列表。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="n">List</span> <span class="nf">getChildren</span><span class="o">(</span><span class="n">String</span> <span class="n">path</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">watch</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>参数说明：</p>

<ul>
<li>path： znode的路径。</li>
<li>watch参数用于指定是否监听path node的创建, 删除事件, 以及数据更新事件。</li>
</ul>


<h3>判断znode是否存在</h3>

<p>ZooKeeper对象的exists方法用于判断指定znode是否存在。如果该node存在, 则返回该node的状态信息, 否则返回null。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="n">Stat</span> <span class="nf">exists</span><span class="o">(</span><span class="n">String</span> <span class="n">path</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">watch</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>参数说明：</p>

<ul>
<li>path： znode的路径。</li>
<li>watch：用于指定是否监听path node的创建, 删除事件, 以及数据更新事件。</li>
</ul>


<h3>获取/更新znode数据</h3>

<p>ZooKeeper对象的getData/setData方法用于获取/更新 znode关联的数据。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='java'><span class='line'><span class="kd">public</span> <span class="kt">byte</span><span class="o">[]</span> <span class="nf">getData</span><span class="o">(</span><span class="n">String</span> <span class="n">path</span><span class="o">,</span> <span class="kt">boolean</span> <span class="n">watch</span><span class="o">,</span> <span class="n">Stat</span> <span class="n">stat</span><span class="o">);</span>
</span><span class='line'><span class="kd">public</span> <span class="n">Stat</span> <span class="nf">setData</span><span class="o">(</span><span class="kd">final</span> <span class="n">String</span> <span class="n">path</span><span class="o">,</span> <span class="kt">byte</span> <span class="n">data</span><span class="o">[],</span> <span class="kt">int</span> <span class="n">version</span><span class="o">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>参数说明：</p>

<ul>
<li>path： znode的路径。</li>
<li>stat：传出参数, getData方法会将path node的状态信息设置到该参数中。</li>
<li>data：与znode关联的数据。</li>
<li>watch：用于指定是否监听path node的创建, 删除事件, 以及数据更新事件。</li>
<li>version：指定要更新的数据的版本, 如果version和真实的版本不同, 更新操作将失败.。指定version为-1则忽略版本检查。</li>
</ul>


<p>更全的API介绍参考 ZooKeeper 3.4.5 API</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Core 消息队列机制]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/11/28/spark-core-message-queue-mechanism/"/>
    <updated>2016-11-28T14:31:03+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/11/28/spark-core-message-queue-mechanism</id>
    <content type="html"><![CDATA[<p>本文介绍Spark中的消息队列机制，首先SparkListenerEvent，SparkListener和SparkListenerBus等基本数据结构实现。</p>

<p>重点介绍了异步消息总线LiveListenerBus的实现。随后介绍了Spark消息队列的整体工作流程。</p>

<!--more-->


<h2>SparkListenerEvent</h2>

<p>Spark中的消息由SparkListenerEvent表示。其本身定义只是一个接口：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">trait</span> <span class="nc">SparkListenerEvent</span> <span class="o">{</span>
</span><span class='line'>  <span class="cm">/* Whether output this event to the event log */</span>
</span><span class='line'>  <span class="k">protected</span><span class="o">[</span><span class="kt">spark</span><span class="o">]</span> <span class="k">def</span> <span class="n">logEvent</span><span class="k">:</span> <span class="kt">Boolean</span> <span class="o">=</span> <span class="kc">true</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>SparkListenerEvent有多个具体的实现，每种实现代表了Spark运行过程中的一种事件。</p>

<ul>
<li>SparkListenerStageSubmitted</li>
<li>SparkListenerStageCompleted</li>
<li>SparkListenerTaskStart</li>
<li>SparkListenerTaskGettingResult</li>
<li>SparkListenerTaskEnd</li>
<li>SparkListenerJobStart</li>
<li>SparkListenerJobEnd</li>
<li>SparkListenerEnvironmentUpdate</li>
<li>SparkListenerBlockManagerAdded</li>
<li>SparkListenerBlockManagerRemoved</li>
<li>SparkListenerUnpersistRDD</li>
<li>SparkListenerExecutorAdded</li>
<li>SparkListenerExecutorRemoved</li>
<li>SparkListenerBlockUpdated</li>
<li>SparkListenerExecutorMetricsUpdate</li>
<li>SparkListenerApplicationStart</li>
<li>SparkListenerApplicationEnd</li>
<li>SparkListenerLogStart</li>
</ul>


<p>根据名称可以知道每一种事件代表的含义。</p>

<h2>SparkListener</h2>

<p>SparkListeners负责监听SparkListenerEvents。</p>

<p>所有Spark消息SparkListenerEvents 被异步的发送给已经注册过的SparkListeners。</p>

<p>SparkListenerInterface定义了SparkListener的接口：</p>

<ul>
<li>onStageCompleted</li>
<li>onStageSubmitted</li>
<li>onTaskStart</li>
<li>onTaskGettingResult</li>
<li>onTaskEnd</li>
<li>onJobStart</li>
<li>onJobEnd</li>
<li>onEnvironmentUpdate</li>
<li>onBlockManagerAdded</li>
<li>onBlockManagerRemoved</li>
<li>onUnpersistRDD</li>
<li>onApplicationStart</li>
<li>onApplicationEnd</li>
<li>onExecutorMetricsUpdate</li>
<li>onExecutorAdded</li>
<li>onExecutorRemoved</li>
<li>onBlockUpdated</li>
<li>onOtherEvent</li>
</ul>


<p>根据名称可以知道每一个方法是对应事件消息的响应函数。SparkListener的实现：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">abstract</span> <span class="k">class</span> <span class="nc">SparkListener</span> <span class="k">extends</span> <span class="nc">SparkListenerInterface</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onStageCompleted</span><span class="o">(</span><span class="n">stageCompleted</span><span class="k">:</span> <span class="kt">SparkListenerStageCompleted</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onStageSubmitted</span><span class="o">(</span><span class="n">stageSubmitted</span><span class="k">:</span> <span class="kt">SparkListenerStageSubmitted</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onTaskStart</span><span class="o">(</span><span class="n">taskStart</span><span class="k">:</span> <span class="kt">SparkListenerTaskStart</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onTaskGettingResult</span><span class="o">(</span><span class="n">taskGettingResult</span><span class="k">:</span> <span class="kt">SparkListenerTaskGettingResult</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onTaskEnd</span><span class="o">(</span><span class="n">taskEnd</span><span class="k">:</span> <span class="kt">SparkListenerTaskEnd</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onJobStart</span><span class="o">(</span><span class="n">jobStart</span><span class="k">:</span> <span class="kt">SparkListenerJobStart</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onJobEnd</span><span class="o">(</span><span class="n">jobEnd</span><span class="k">:</span> <span class="kt">SparkListenerJobEnd</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onEnvironmentUpdate</span><span class="o">(</span><span class="n">environmentUpdate</span><span class="k">:</span> <span class="kt">SparkListenerEnvironmentUpdate</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onBlockManagerAdded</span><span class="o">(</span><span class="n">blockManagerAdded</span><span class="k">:</span> <span class="kt">SparkListenerBlockManagerAdded</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onBlockManagerRemoved</span><span class="o">(</span>
</span><span class='line'>      <span class="n">blockManagerRemoved</span><span class="k">:</span> <span class="kt">SparkListenerBlockManagerRemoved</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onUnpersistRDD</span><span class="o">(</span><span class="n">unpersistRDD</span><span class="k">:</span> <span class="kt">SparkListenerUnpersistRDD</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onApplicationStart</span><span class="o">(</span><span class="n">applicationStart</span><span class="k">:</span> <span class="kt">SparkListenerApplicationStart</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onApplicationEnd</span><span class="o">(</span><span class="n">applicationEnd</span><span class="k">:</span> <span class="kt">SparkListenerApplicationEnd</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onExecutorMetricsUpdate</span><span class="o">(</span>
</span><span class='line'>      <span class="n">executorMetricsUpdate</span><span class="k">:</span> <span class="kt">SparkListenerExecutorMetricsUpdate</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onExecutorAdded</span><span class="o">(</span><span class="n">executorAdded</span><span class="k">:</span> <span class="kt">SparkListenerExecutorAdded</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onExecutorRemoved</span><span class="o">(</span><span class="n">executorRemoved</span><span class="k">:</span> <span class="kt">SparkListenerExecutorRemoved</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onBlockUpdated</span><span class="o">(</span><span class="n">blockUpdated</span><span class="k">:</span> <span class="kt">SparkListenerBlockUpdated</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">onOtherEvent</span><span class="o">(</span><span class="n">event</span><span class="k">:</span> <span class="kt">SparkListenerEvent</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span> <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Spark运行过程中会用到很多个SparkListener，每一种都有自己的用途。</p>

<p>例如EventLoggingListener用来将监听到的事件持久化到文件中，ExecutorAllocationListener用那个通知对应的ExecutorAllocationManager增加或移除executor等。</p>

<h2>SparkListenerBus</h2>

<p>SparkListener需要被注册到SparkListenerBus中才能起作用，SparkListenerBus负责分发监听到的Event给SparkListener。</p>

<p>SparkListenerBus继承自ListenerBus接口，并重载了doPostEvent方法。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">private</span><span class="o">[</span><span class="kt">spark</span><span class="o">]</span> <span class="k">trait</span> <span class="nc">SparkListenerBus</span>
</span><span class='line'>  <span class="k">extends</span> <span class="nc">ListenerBus</span><span class="o">[</span><span class="kt">SparkListenerInterface</span>, <span class="kt">SparkListenerEvent</span><span class="o">]</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">protected</span> <span class="k">override</span> <span class="k">def</span> <span class="n">doPostEvent</span><span class="o">(</span>
</span><span class='line'>      <span class="n">listener</span><span class="k">:</span> <span class="kt">SparkListenerInterface</span><span class="o">,</span>
</span><span class='line'>      <span class="n">event</span><span class="k">:</span> <span class="kt">SparkListenerEvent</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">event</span> <span class="k">match</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">case</span> <span class="n">stageSubmitted</span><span class="k">:</span> <span class="kt">SparkListenerStageSubmitted</span> <span class="o">=&gt;</span>
</span><span class='line'>        <span class="n">listener</span><span class="o">.</span><span class="n">onStageSubmitted</span><span class="o">(</span><span class="n">stageSubmitted</span><span class="o">)</span>
</span><span class='line'>      <span class="k">case</span> <span class="n">stageCompleted</span><span class="k">:</span> <span class="kt">SparkListenerStageCompleted</span> <span class="o">=&gt;</span>
</span><span class='line'>        <span class="n">listener</span><span class="o">.</span><span class="n">onStageCompleted</span><span class="o">(</span><span class="n">stageCompleted</span><span class="o">)</span>
</span><span class='line'>      <span class="k">case</span> <span class="n">jobStart</span><span class="k">:</span> <span class="kt">SparkListenerJobStart</span> <span class="o">=&gt;</span>
</span><span class='line'>        <span class="n">listener</span><span class="o">.</span><span class="n">onJobStart</span><span class="o">(</span><span class="n">jobStart</span><span class="o">)</span>
</span><span class='line'>      <span class="k">case</span> <span class="n">jobEnd</span><span class="k">:</span> <span class="kt">SparkListenerJobEnd</span> <span class="o">=&gt;</span>
</span><span class='line'>        <span class="n">listener</span><span class="o">.</span><span class="n">onJobEnd</span><span class="o">(</span><span class="n">jobEnd</span><span class="o">)</span>
</span><span class='line'>      <span class="o">...</span>
</span><span class='line'>      <span class="k">case</span> <span class="n">blockUpdated</span><span class="k">:</span> <span class="kt">SparkListenerBlockUpdated</span> <span class="o">=&gt;</span>
</span><span class='line'>        <span class="n">listener</span><span class="o">.</span><span class="n">onBlockUpdated</span><span class="o">(</span><span class="n">blockUpdated</span><span class="o">)</span>
</span><span class='line'>      <span class="k">case</span> <span class="n">logStart</span><span class="k">:</span> <span class="kt">SparkListenerLogStart</span> <span class="o">=&gt;</span> <span class="c1">// ignore event log metadata</span>
</span><span class='line'>      <span class="k">case</span> <span class="k">_</span> <span class="k">=&gt;</span> <span class="n">listener</span><span class="o">.</span><span class="n">onOtherEvent</span><span class="o">(</span><span class="n">event</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>该接口实现了消息的路由，根据事件类型调用相应的处理函数。</p>

<h3>ListenerBus</h3>

<p>ListenerBus接口的实现如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">private</span><span class="o">[</span><span class="kt">spark</span><span class="o">]</span> <span class="k">trait</span> <span class="nc">ListenerBus</span><span class="o">[</span><span class="kt">L</span> <span class="k">&lt;:</span> <span class="kt">AnyRef</span>, <span class="kt">E</span><span class="o">]</span> <span class="nc">extends</span> <span class="nc">Logging</span> <span class="o">{</span>
</span><span class='line'>
</span><span class='line'>  <span class="c1">// Marked `private[spark]` for access in tests.</span>
</span><span class='line'>  <span class="k">private</span><span class="o">[</span><span class="kt">spark</span><span class="o">]</span> <span class="k">val</span> <span class="n">listeners</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">CopyOnWriteArrayList</span><span class="o">[</span><span class="kt">L</span><span class="o">]</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">final</span> <span class="k">def</span> <span class="n">addListener</span><span class="o">(</span><span class="n">listener</span><span class="k">:</span> <span class="kt">L</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">listeners</span><span class="o">.</span><span class="n">add</span><span class="o">(</span><span class="n">listener</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">final</span> <span class="k">def</span> <span class="n">removeListener</span><span class="o">(</span><span class="n">listener</span><span class="k">:</span> <span class="kt">L</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">listeners</span><span class="o">.</span><span class="n">remove</span><span class="o">(</span><span class="n">listener</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/**</span>
</span><span class='line'><span class="cm">   * Post the event to all registered listeners. The `postToAll` caller should guarantee calling</span>
</span><span class='line'><span class="cm">   * `postToAll` in the same thread for all events.</span>
</span><span class='line'><span class="cm">   */</span>
</span><span class='line'>  <span class="k">final</span> <span class="k">def</span> <span class="n">postToAll</span><span class="o">(</span><span class="n">event</span><span class="k">:</span> <span class="kt">E</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// JavaConverters can create a JIterableWrapper if we use asScala.</span>
</span><span class='line'>    <span class="c1">// However, this method will be called frequently. To avoid the wrapper cost, here we use</span>
</span><span class='line'>    <span class="c1">// Java Iterator directly.</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">iter</span> <span class="k">=</span> <span class="n">listeners</span><span class="o">.</span><span class="n">iterator</span>
</span><span class='line'>    <span class="k">while</span> <span class="o">(</span><span class="n">iter</span><span class="o">.</span><span class="n">hasNext</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">listener</span> <span class="k">=</span> <span class="n">iter</span><span class="o">.</span><span class="n">next</span><span class="o">()</span>
</span><span class='line'>      <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">doPostEvent</span><span class="o">(</span><span class="n">listener</span><span class="o">,</span> <span class="n">event</span><span class="o">)</span>
</span><span class='line'>      <span class="o">}</span> <span class="k">catch</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">case</span> <span class="nc">NonFatal</span><span class="o">(</span><span class="n">e</span><span class="o">)</span> <span class="k">=&gt;</span>
</span><span class='line'>          <span class="n">logError</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;Listener ${Utils.getFormattedClassName(listener)} threw an exception&quot;</span><span class="o">,</span> <span class="n">e</span><span class="o">)</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="cm">/**</span>
</span><span class='line'><span class="cm">   * Post an event to the specified listener. `onPostEvent` is guaranteed to be called in the same</span>
</span><span class='line'><span class="cm">   * thread for all listeners.</span>
</span><span class='line'><span class="cm">   */</span>
</span><span class='line'>  <span class="k">protected</span> <span class="k">def</span> <span class="n">doPostEvent</span><span class="o">(</span><span class="n">listener</span><span class="k">:</span> <span class="kt">L</span><span class="o">,</span> <span class="n">event</span><span class="k">:</span> <span class="kt">E</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">private</span><span class="o">[</span><span class="kt">spark</span><span class="o">]</span> <span class="k">def</span> <span class="n">findListenersByClass</span><span class="o">[</span><span class="kt">T</span> <span class="k">&lt;:</span> <span class="kt">L</span> <span class="kt">:</span> <span class="kt">ClassTag</span><span class="o">]()</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">T</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">c</span> <span class="k">=</span> <span class="n">implicitly</span><span class="o">[</span><span class="kt">ClassTag</span><span class="o">[</span><span class="kt">T</span><span class="o">]].</span><span class="n">runtimeClass</span>
</span><span class='line'>    <span class="n">listeners</span><span class="o">.</span><span class="n">asScala</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getClass</span> <span class="o">==</span> <span class="n">c</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">T</span><span class="o">]).</span><span class="n">toSeq</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>本质上所有注册的Listener用一个数组记录下来，post操作就是根据事件找到对应的listener然后把event交给listener处理。</p>

<h3>LiveListenerBus</h3>

<p>SparkContext中会创建一个LiveListenerBus实例，LiveListenerBus是SparkListenerBus的一个具体实现，主要功能如下:</p>

<ul>
<li>保存有消息队列,负责消息的缓存</li>
<li>保存有注册过的listener,负责消息的分发</li>
</ul>


<p>消息队列用LinkBlockQueue实现：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// Cap the capacity of the event queue so we get an explicit error (rather than</span>
</span><span class='line'><span class="c1">// an OOM exception) if it&#39;s perpetually being added to more quickly than it&#39;s being drained.</span>
</span><span class='line'><span class="k">private</span> <span class="k">lazy</span> <span class="k">val</span> <span class="nc">EVENT_QUEUE_CAPACITY</span> <span class="k">=</span> <span class="n">validateAndGetQueueSize</span><span class="o">()</span>
</span><span class='line'><span class="k">private</span> <span class="k">lazy</span> <span class="k">val</span> <span class="n">eventQueue</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">LinkedBlockingQueue</span><span class="o">[</span><span class="kt">SparkListenerEvent</span><span class="o">](</span><span class="nc">EVENT_QUEUE_CAPACITY</span><span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>事件队列的长度EVENT_QUEUE_CAPACITY由spark.scheduler.listenerbus.eventqueue.size参数配置，默认为10000。</p>

<p>当缓存事件数量达到上限后,新来的事件会被丢弃。</p>

<p>消息的产生和分发按照 <b><font color=red>生产者-消费者模型</font></b> 实现。</p>

<p><b><font color=red>消息的分发(消费者)</font></b> 是通过一个listener线程异步处理的，代码如下。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">private</span> <span class="k">val</span> <span class="n">listenerThread</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Thread</span><span class="o">(</span><span class="n">name</span><span class="o">)</span> <span class="o">{</span>  <span class="c1">// &lt;-- 线程名为SparkListenerBus</span>
</span><span class='line'>  <span class="n">setDaemon</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
</span><span class='line'>  <span class="k">override</span> <span class="k">def</span> <span class="n">run</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">tryOrStopSparkContext</span><span class="o">(</span><span class="n">sparkContext</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="nc">LiveListenerBus</span><span class="o">.</span><span class="n">withinListenerThread</span><span class="o">.</span><span class="n">withValue</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="k">while</span> <span class="o">(</span><span class="kc">true</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>        <span class="n">eventLock</span><span class="o">.</span><span class="n">acquire</span><span class="o">()</span>
</span><span class='line'>        <span class="n">self</span><span class="o">.</span><span class="n">synchronized</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">processingEvent</span> <span class="k">=</span> <span class="kc">true</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>        <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>          <span class="k">val</span> <span class="n">event</span> <span class="k">=</span> <span class="n">eventQueue</span><span class="o">.</span><span class="n">poll</span>
</span><span class='line'>          <span class="k">if</span> <span class="o">(</span><span class="n">event</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>            <span class="c1">// Get out of the while loop and shutdown the daemon thread</span>
</span><span class='line'>            <span class="k">if</span> <span class="o">(!</span><span class="n">stopped</span><span class="o">.</span><span class="n">get</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>              <span class="k">throw</span> <span class="k">new</span> <span class="nc">IllegalStateException</span><span class="o">(</span><span class="s">&quot;Polling `null` from eventQueue means&quot;</span> <span class="o">+</span>
</span><span class='line'>                <span class="s">&quot; the listener bus has been stopped. So `stopped` must be true&quot;</span><span class="o">)</span>
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>            <span class="k">return</span>
</span><span class='line'>          <span class="o">}</span>
</span><span class='line'>          <span class="n">postToAll</span><span class="o">(</span><span class="n">event</span><span class="o">)</span>
</span><span class='line'>        <span class="o">}</span> <span class="k">finally</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">self</span><span class="o">.</span><span class="n">synchronized</span> <span class="o">{</span>
</span><span class='line'>            <span class="n">processingEvent</span> <span class="k">=</span> <span class="kc">false</span>
</span><span class='line'>          <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>为了保证生产者和消费者对消息队列的并发访问，在每次需要获取消息的时候,调用eventLock.acquire()来获取信号量, 信号量的值就是当前队列中所含有的事件数量。</p>

<p>如果正常获取到事件,就调用postToAll将事件分发给所有listener, 继续下一次循环。</p>

<p>如果获取到null值, 则有下面两种情况:</p>

<ul>
<li>整个application正常结束, 此时stopped值已经被设置为true。</li>
<li>系统发生了错误, 立即终止运行。</li>
</ul>


<p><font color=red><b>消息的产生(生产者)</font></b> 通过在Spark运行时调用LiveListenerBus的post方法来添加。实现如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">post</span><span class="o">(</span><span class="n">event</span><span class="k">:</span> <span class="kt">SparkListenerEvent</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">stopped</span><span class="o">.</span><span class="n">get</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Drop further events to make `listenerThread` exit ASAP</span>
</span><span class='line'>    <span class="n">logError</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$name has already stopped! Dropping event $event&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="k">return</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="k">val</span> <span class="n">eventAdded</span> <span class="k">=</span> <span class="n">eventQueue</span><span class="o">.</span><span class="n">offer</span><span class="o">(</span><span class="n">event</span><span class="o">)</span>  <span class="c1">// &lt;-- 这里将新来的事件添加到消息队列中</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">eventAdded</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">eventLock</span><span class="o">.</span><span class="n">release</span><span class="o">()</span>
</span><span class='line'>  <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">onDropEvent</span><span class="o">(</span><span class="n">event</span><span class="o">)</span>  <span class="c1">// &lt;-- 没有添加成功，则丢弃事件</span>
</span><span class='line'>    <span class="n">droppedEventsCounter</span><span class="o">.</span><span class="n">incrementAndGet</span><span class="o">()</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">val</span> <span class="n">droppedEvents</span> <span class="k">=</span> <span class="n">droppedEventsCounter</span><span class="o">.</span><span class="n">get</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">droppedEvents</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Don&#39;t log too frequently</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">(</span><span class="nc">System</span><span class="o">.</span><span class="n">currentTimeMillis</span><span class="o">()</span> <span class="o">-</span> <span class="n">lastReportTimestamp</span> <span class="o">&gt;=</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">1000</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>      <span class="c1">// There may be multiple threads trying to decrease droppedEventsCounter.</span>
</span><span class='line'>      <span class="c1">// Use &quot;compareAndSet&quot; to make sure only one thread can win.</span>
</span><span class='line'>      <span class="c1">// And if another thread is increasing droppedEventsCounter, &quot;compareAndSet&quot; will fail and</span>
</span><span class='line'>      <span class="c1">// then that thread will update it.</span>
</span><span class='line'>      <span class="k">if</span> <span class="o">(</span><span class="n">droppedEventsCounter</span><span class="o">.</span><span class="n">compareAndSet</span><span class="o">(</span><span class="n">droppedEvents</span><span class="o">,</span> <span class="mi">0</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">prevLastReportTimestamp</span> <span class="k">=</span> <span class="n">lastReportTimestamp</span>
</span><span class='line'>        <span class="n">lastReportTimestamp</span> <span class="k">=</span> <span class="nc">System</span><span class="o">.</span><span class="n">currentTimeMillis</span><span class="o">()</span>
</span><span class='line'>        <span class="n">logWarning</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;Dropped $droppedEvents SparkListenerEvents since &quot;</span> <span class="o">+</span>
</span><span class='line'>          <span class="k">new</span> <span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="nc">Date</span><span class="o">(</span><span class="n">prevLastReportTimestamp</span><span class="o">))</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>每成功放入一个事件,就调用eventLock.release()来增加信号量额值，以供消费者线程来进行消费. 如果队列满了,就调用onDropEvent来处理。</p>

<h2>消息队列建立/发送流程</h2>

<p>在SparkContext中创建了LiveListenerBus类类型的成员变量listenerBus。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="c1">// An asynchronous listener bus for Spark events</span>
</span><span class='line'><span class="k">private</span><span class="o">[</span><span class="kt">spark</span><span class="o">]</span> <span class="k">val</span> <span class="n">listenerBus</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">LiveListenerBus</span><span class="o">(</span><span class="k">this</span><span class="o">)</span>
</span><span class='line'><span class="n">随后创建各种listener</span><span class="err">，</span><span class="n">并注册到listenerBus中</span><span class="err">，</span><span class="n">通过调用listenerBus的start</span><span class="o">()</span><span class="n">方法启动消息分发流程</span><span class="err">。</span>
</span><span class='line'><span class="k">private</span> <span class="k">def</span> <span class="n">setupAndStartListenerBus</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="c1">// Use reflection to instantiate listeners specified via `spark.extraListeners`</span>
</span><span class='line'>  <span class="k">try</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">val</span> <span class="n">listenerClassNames</span><span class="k">:</span> <span class="kt">Seq</span><span class="o">[</span><span class="kt">String</span><span class="o">]</span> <span class="k">=</span>
</span><span class='line'>      <span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="o">(</span><span class="s">&quot;spark.extraListeners&quot;</span><span class="o">,</span> <span class="s">&quot;&quot;</span><span class="o">).</span><span class="n">split</span><span class="o">(</span><span class="sc">&#39;,&#39;</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">trim</span><span class="o">).</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span> <span class="o">!=</span> <span class="s">&quot;&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="k">for</span> <span class="o">(</span><span class="n">className</span> <span class="k">&lt;-</span> <span class="n">listenerClassNames</span><span class="o">)</span> <span class="o">{</span>  <span class="c1">// &lt;-- 如果指定了额外的SparkListenr类，可通过反射机制实例化并注册到listenerBus</span>
</span><span class='line'>      <span class="c1">// Use reflection to find the right constructor</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">constructors</span> <span class="k">=</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">val</span> <span class="n">listenerClass</span> <span class="k">=</span> <span class="nc">Utils</span><span class="o">.</span><span class="n">classForName</span><span class="o">(</span><span class="n">className</span><span class="o">)</span>
</span><span class='line'>        <span class="n">listenerClass</span>
</span><span class='line'>            <span class="o">.</span><span class="n">getConstructors</span>
</span><span class='line'>            <span class="o">.</span><span class="n">asInstanceOf</span><span class="o">[</span><span class="kt">Array</span><span class="o">[</span><span class="kt">Constructor</span><span class="o">[</span><span class="k">_</span> <span class="k">&lt;:</span> <span class="kt">SparkListenerInterface</span><span class="o">]]]</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">constructorTakingSparkConf</span> <span class="k">=</span> <span class="n">constructors</span><span class="o">.</span><span class="n">find</span> <span class="o">{</span> <span class="n">c</span> <span class="k">=&gt;</span>
</span><span class='line'>        <span class="n">c</span><span class="o">.</span><span class="n">getParameterTypes</span><span class="o">.</span><span class="n">sameElements</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="n">classOf</span><span class="o">[</span><span class="kt">SparkConf</span><span class="o">]))</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="k">lazy</span> <span class="k">val</span> <span class="n">zeroArgumentConstructor</span> <span class="k">=</span> <span class="n">constructors</span><span class="o">.</span><span class="n">find</span> <span class="o">{</span> <span class="n">c</span> <span class="k">=&gt;</span>
</span><span class='line'>        <span class="n">c</span><span class="o">.</span><span class="n">getParameterTypes</span><span class="o">.</span><span class="n">isEmpty</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="k">val</span> <span class="n">listener</span><span class="k">:</span> <span class="kt">SparkListenerInterface</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">(</span><span class="n">constructorTakingSparkConf</span><span class="o">.</span><span class="n">isDefined</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">constructorTakingSparkConf</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">newInstance</span><span class="o">(</span><span class="n">conf</span><span class="o">)</span>
</span><span class='line'>        <span class="o">}</span> <span class="k">else</span> <span class="k">if</span> <span class="o">(</span><span class="n">zeroArgumentConstructor</span><span class="o">.</span><span class="n">isDefined</span><span class="o">)</span> <span class="o">{</span>
</span><span class='line'>          <span class="n">zeroArgumentConstructor</span><span class="o">.</span><span class="n">get</span><span class="o">.</span><span class="n">newInstance</span><span class="o">()</span>
</span><span class='line'>        <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>          <span class="o">...</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>      <span class="o">}</span>
</span><span class='line'>      <span class="n">listenerBus</span><span class="o">.</span><span class="n">addListener</span><span class="o">(</span><span class="n">listener</span><span class="o">)</span>
</span><span class='line'>      <span class="n">logInfo</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;Registered listener $className&quot;</span><span class="o">)</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span> <span class="k">catch</span> <span class="o">{</span>
</span><span class='line'>    <span class="o">...</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">listenerBus</span><span class="o">.</span><span class="n">start</span><span class="o">()</span>
</span><span class='line'>  <span class="nc">_listenerBusStarted</span> <span class="k">=</span> <span class="kc">true</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>其中，listenerBus.start() 实现如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">start</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">started</span><span class="o">.</span><span class="n">compareAndSet</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>    <span class="n">listenerThread</span><span class="o">.</span><span class="n">start</span><span class="o">()</span>
</span><span class='line'>  <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nc">IllegalStateException</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;$name already started!&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>运行过程中产生的事件会post到listenerBus中。</p>

<p>当作业运行结束后会调用listenerBus.stop()来停止SparkListenerBus线程。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='scala'><span class='line'><span class="k">def</span> <span class="n">stop</span><span class="o">()</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="o">{</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(!</span><span class="n">started</span><span class="o">.</span><span class="n">get</span><span class="o">())</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">throw</span> <span class="k">new</span> <span class="nc">IllegalStateException</span><span class="o">(</span><span class="n">s</span><span class="s">&quot;Attempted to stop $name that has not yet started!&quot;</span><span class="o">)</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'>  <span class="k">if</span> <span class="o">(</span><span class="n">stopped</span><span class="o">.</span><span class="n">compareAndSet</span><span class="o">(</span><span class="kc">false</span><span class="o">,</span> <span class="kc">true</span><span class="o">))</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Call eventLock.release() so that listenerThread will poll `null` from `eventQueue` and know</span>
</span><span class='line'>    <span class="c1">// `stop` is called.</span>
</span><span class='line'>    <span class="n">eventLock</span><span class="o">.</span><span class="n">release</span><span class="o">()</span>
</span><span class='line'>    <span class="n">listenerThread</span><span class="o">.</span><span class="n">join</span><span class="o">()</span>
</span><span class='line'>  <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
</span><span class='line'>    <span class="c1">// Keep quiet</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>这里可以看到：</p>

<p><b><font color=red>在stop函数中调用了eventLock.release()来增加信号量的值. 然而并未向消息队列中加入新的消息。</p>

<p>这就导致在消费者线程listenerThread读取队列时会返回null值,进而达到结束listenerThread线程的目的。</font></b></p>

<p>以上就是Spark Core中消息队列机制的整体工作流程。</p>

<p><b>参考资料</b></p>

<ol>
<li>Spark 2.0 源码：<a href="https://github.com/apache/spark/tree/branch-2.0">https://github.com/apache/spark/tree/branch-2.0</a></li>
<li>Spark消息队列机制源码学习Blog：<a href="http://blog.csdn.net/sivolin/article/details/47316099">http://blog.csdn.net/sivolin/article/details/47316099</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark日志配置方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa/"/>
    <updated>2016-10-19T19:13:38+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa</id>
    <content type="html"><![CDATA[<p>本文介绍Spark日志级别控制方法。</p>

<p>Apache Spark 默认使用 log4j 作为日志工具。
Baidu Spark 根据不同的发布版本，使用 log4j 或者 logback 作为日志工具。</p>

<!--more-->


<h2>Driver日志级别设置</h2>

<p>日志配置默认在Spark客户端conf目录下，log4j配置log4j.properties文件，logback配置logback.xml文件。</p>

<h3>log4j配置示例</h3>

<p>示例配置文件log4j.properties如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># Set everything to be logged to the console</span>
</span><span class='line'>log4j.rootCategory<span class="o">=</span>INFO, DRFA
</span><span class='line'>log4j.appender.console<span class="o">=</span>org.apache.log4j.ConsoleAppender
</span><span class='line'>log4j.appender.console.target<span class="o">=</span>System.err
</span><span class='line'>log4j.appender.console.layout<span class="o">=</span>org.apache.log4j.PatternLayout
</span><span class='line'>log4j.appender.console.layout.ConversionPattern<span class="o">=</span>%d<span class="o">{</span>yy/MM/dd HH:mm:ss<span class="o">}</span> %p %c<span class="o">{</span>1<span class="o">}</span>: %m%n
</span><span class='line'>log4j.appender.DRFA<span class="o">=</span>org.apache.log4j.DailyRollingFileAppender
</span><span class='line'>log4j.appender.DRFA.File<span class="o">=</span>log/spark.log
</span><span class='line'>log4j.appender.DRFA.layout<span class="o">=</span>org.apache.log4j.PatternLayout
</span><span class='line'>log4j.appender.DRFA.layout.ConversionPattern<span class="o">=</span>%d<span class="o">{</span>yy/MM/dd HH:mm:ss<span class="o">}</span> <span class="o">[</span>%t<span class="o">]</span> %p %c<span class="o">{</span>1<span class="o">}</span>: %m%n
</span><span class='line'><span class="c"># Settings to quiet third party logs that are too verbose</span>
</span><span class='line'>log4j.logger.org.eclipse.jetty<span class="o">=</span>WARN
</span><span class='line'>log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle<span class="o">=</span>ERROR
</span><span class='line'>log4j.logger.org.apache.spark.repl.SparkIMain<span class="nv">$exprTyper</span><span class="o">=</span>INFO
</span><span class='line'>log4j.logger.org.apache.spark.repl.SparkILoop<span class="nv">$SparkILoopInterpreter</span><span class="o">=</span>INFO
</span><span class='line'>log4j.logger.org.apache.hadoop.fs.DfsInputStream<span class="o">=</span>INFO
</span><span class='line'>log4j.logger.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl<span class="o">=</span>INFO
</span><span class='line'>log4j.logger.org.apache.hadoop.hive.ql.io.*<span class="o">=</span>DEBUG
</span></code></pre></td></tr></table></div></figure>


<p>其中：</p>

<ul>
<li>log4j.rootCategory配置总体的默认日志级别。</li>
<li>log4j.appender.DRFA.File用来配置日志重定向的目标文件。</li>
<li>log4j.appender.DRFA.layout.ConversionPattern用来配置日志的输出格式。</li>
<li>log4j.logger.xxx.xxx.xxx用来配置指定类的日志级别，xxx.xxx.xxx代表类路径。</li>
</ul>


<h3>logback配置示例</h3>

<p>示例配置文件logback.xml如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="cp">&lt;?xml version=&quot;1.0&quot;?&gt;</span>
</span><span class='line'><span class="cp">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span>
</span><span class='line'><span class="nt">&lt;configuration&gt;</span>
</span><span class='line'>  <span class="nt">&lt;appender</span> <span class="na">name=</span><span class="s">&quot;RFILE&quot;</span> <span class="na">class=</span><span class="s">&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;file&gt;</span>log/spark.log<span class="nt">&lt;/file&gt;</span>
</span><span class='line'>    <span class="nt">&lt;rollingPolicy</span> <span class="na">class=</span><span class="s">&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>      <span class="nt">&lt;fileNamePattern&gt;</span>log/spark.%d{yyyy-MM-dd}.log<span class="nt">&lt;/fileNamePattern&gt;</span>
</span><span class='line'>      <span class="nt">&lt;maxHistory&gt;</span>30<span class="nt">&lt;/maxHistory&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/rollingPolicy&gt;</span>
</span><span class='line'>    <span class="nt">&lt;encoder&gt;</span>
</span><span class='line'>      <span class="nt">&lt;pattern&gt;</span> %date %level [%logger] - %msg%n<span class="nt">&lt;/pattern&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/encoder&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/appender&gt;</span>
</span><span class='line'>  <span class="nt">&lt;logger</span> <span class="na">name=</span><span class="s">&quot;org.apache.hadoop.hive.ql.io.*&quot;</span>  <span class="na">level=</span><span class="s">&quot;INFO&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;root</span> <span class="na">level=</span><span class="s">&quot;info&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;appender-ref</span> <span class="na">ref=</span><span class="s">&quot;RFILE&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/root&gt;</span>
</span><span class='line'><span class="nt">&lt;/configuration&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>其中：</p>

<ul>
<li><root>用来配置总体的默认日志级别。</li>
<li><file>用来配置日志重定向的目标文件。</li>
<li><pattern>用来配置日志的输出格式。</li>
<li><logger>用来配置指定类的日志级别。</li>
</ul>


<h2>Executor日志级别设置</h2>

<p>executor日志级别配置同driver端类似，但需要将executor的日志配置文件上传，并通过executor的Java参数指定使用的配置文件名。</p>

<h3>log4j配置示例</h3>

<p>配置好executor的log4j配置文件，命名随意，假设为executor-log4j.properties，放在conf目录下。</p>

<p>配置spark-conf.defaults文件，在 spark.executor.extraJavaOptions 中增加:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>spark.executor.extraJavaOptions -Dlog4j.configuration<span class="o">=</span>executor-log4j.properties
</span></code></pre></td></tr></table></div></figure>


<p>启动Spark作业时增加–files参数。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$spark</span>-submit --files /path/to/conf/executor-log4j.properties xxx
</span><span class='line'>则作业运行时的日志级别将由executor-log4j.properties文件控制。
</span></code></pre></td></tr></table></div></figure>


<h3>logback配置示例</h3>

<p>配置好executor的logback配置文件，命名随意，假设为executor-logback.xml，放在conf目录下。</p>

<p>配置spark-conf.defaults文件，在 spark.executor.extraJavaOptions 中增加:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>spark.executor.extraJavaOptions -Dlogback.configurationFile<span class="o">=</span>executor-logback.xml
</span></code></pre></td></tr></table></div></figure>


<p>启动Spark作业时增加–files参数。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$spark</span>-submit --files /path/to/conf/executor-logback.xml xxx
</span></code></pre></td></tr></table></div></figure>


<p>则作业运行时的日志级别将由executor-logback.xml文件控制。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[JavaScript实现博文目录自动生成]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/09/13/javascriptshi-xian-bo-wen-mu-lu-zi-dong-sheng-cheng/"/>
    <updated>2016-09-13T17:16:48+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/09/13/javascriptshi-xian-bo-wen-mu-lu-zi-dong-sheng-cheng</id>
    <content type="html"><![CDATA[<p>自建的博客站点，常常需要博文目录自动生成的功能，本文介绍一种JavaScript实现的解决方案。</p>

<p>实现参考了 <a href="http://www.iyanlei.com/markdown_catelog.html%E3%80%82">http://www.iyanlei.com/markdown_catelog.html%E3%80%82</a></p>

<!--more-->


<h2>目录自动生成</h2>

<p>以本站博客的生成代码为例。我们需要借助JQuery实现，因此首先加载JQuery库。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'><span class="nt">&lt;script </span><span class="na">src=</span><span class="s">&quot;//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js&quot;</span><span class="nt">&gt;&lt;/script&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h3>相关HTML</h3>

<p>在博文顶部留出一块区域用作目录：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;blogAnchor&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nt">&lt;p</span> <span class="na">id=</span><span class="s">&quot;anchorContentToggle&quot;</span> <span class="na">title=</span><span class="s">&quot;收起&quot;</span><span class="nt">&gt;</span>导航[-]<span class="nt">&lt;/p&gt;</span>
</span><span class='line'>  <span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">&quot;anchorContent&quot;</span> <span class="na">id=</span><span class="s">&quot;anchorContent&quot;</span><span class="nt">&gt;</span> <span class="nt">&lt;/div&gt;</span>
</span><span class='line'><span class="nt">&lt;/div&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>在博文末尾添加一段JavaScript代码，扫描当前页面的h2~h6元素。</p>

<h3>使用JQuery生成目录</h3>

<p>为每个标题生成一个超链接a元素，添加到anchorContent区域中。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='html'><span class='line'><span class="nt">&lt;script </span><span class="na">src=</span><span class="s">&quot;/javascripts/libs/blogDirectory.js&quot;</span> <span class="na">type=</span><span class="s">&quot;text/javascript&quot;</span><span class="nt">&gt;&lt;/script&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>blogDirectory.js具体实现如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">$</span><span class="p">(</span><span class="s2">&quot;.entry-content&quot;</span><span class="p">).</span><span class="nx">find</span><span class="p">(</span><span class="s2">&quot;h2,h3,h4,h5,h6&quot;</span><span class="p">).</span><span class="nx">each</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span> <span class="nx">item</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">tag</span> <span class="o">=</span> <span class="nx">$</span><span class="p">(</span><span class="nx">item</span><span class="p">).</span><span class="nx">get</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nx">localName</span><span class="p">;</span>
</span><span class='line'>  <span class="nx">$</span><span class="p">(</span><span class="nx">item</span><span class="p">).</span><span class="nx">attr</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;wow&quot;</span> <span class="o">+</span> <span class="nx">i</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">$</span><span class="p">(</span><span class="s2">&quot;#anchorContent&quot;</span><span class="p">).</span><span class="nx">append</span><span class="p">(</span>
</span><span class='line'>    <span class="s1">&#39;&lt;p&gt;&lt;a class = &quot;title-&#39;</span> <span class="o">+</span> <span class="nx">tag</span> <span class="o">+</span>
</span><span class='line'>    <span class="s1">&#39; anchor-link&quot; onclick = &quot;return false;&quot; href = &quot;#&quot; link = &quot;#wow&#39;</span> <span class="o">+</span> <span class="nx">i</span> <span class="o">+</span>
</span><span class='line'>    <span class="s1">&#39;&quot;&gt;&#39;</span> <span class="o">+</span> <span class="nx">$</span><span class="p">(</span><span class="k">this</span><span class="p">).</span><span class="nx">text</span><span class="p">()</span> <span class="o">+</span> <span class="s1">&#39;&lt;/a&gt;&lt;/p&gt;&#39;</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">$</span><span class="p">(</span><span class="s2">&quot;.title-h2&quot;</span><span class="p">).</span><span class="nx">css</span><span class="p">(</span><span class="s2">&quot;margin-left&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">$</span><span class="p">(</span><span class="s2">&quot;.title-h3&quot;</span><span class="p">).</span><span class="nx">css</span><span class="p">(</span><span class="s2">&quot;margin-left&quot;</span><span class="p">,</span> <span class="mi">20</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">$</span><span class="p">(</span><span class="s2">&quot;.title-h4&quot;</span><span class="p">).</span><span class="nx">css</span><span class="p">(</span><span class="s2">&quot;margin-left&quot;</span><span class="p">,</span> <span class="mi">40</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">$</span><span class="p">(</span><span class="s2">&quot;.title-h5&quot;</span><span class="p">).</span><span class="nx">css</span><span class="p">(</span><span class="s2">&quot;margin-left&quot;</span><span class="p">,</span> <span class="mi">60</span><span class="p">);</span>
</span><span class='line'>  <span class="nx">$</span><span class="p">(</span><span class="s2">&quot;.title-h6&quot;</span><span class="p">).</span><span class="nx">css</span><span class="p">(</span><span class="s2">&quot;margin-left&quot;</span><span class="p">,</span> <span class="mi">80</span><span class="p">);</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>代码根据标题的级别控制CSS样式表单，对不同的级别的标题使用了不同的缩进。</p>

<h2>点击收放功能</h2>

<p>向blogDirectory.js中添加一段代码，即可实现点击收放功能。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">$</span><span class="p">(</span><span class="s2">&quot;#anchorContentToggle&quot;</span><span class="p">).</span><span class="nx">click</span><span class="p">(</span><span class="kd">function</span><span class="p">(){</span>
</span><span class='line'>    <span class="kd">var</span> <span class="nx">text</span> <span class="o">=</span> <span class="nx">$</span><span class="p">(</span><span class="k">this</span><span class="p">).</span><span class="nx">html</span><span class="p">();</span>
</span><span class='line'>    <span class="k">if</span><span class="p">(</span><span class="nx">text</span><span class="o">==</span><span class="s2">&quot;导航[-]&quot;</span><span class="p">){</span>
</span><span class='line'>        <span class="nx">$</span><span class="p">(</span><span class="k">this</span><span class="p">).</span><span class="nx">html</span><span class="p">(</span><span class="s2">&quot;导航[+]&quot;</span><span class="p">);</span>
</span><span class='line'>        <span class="nx">$</span><span class="p">(</span><span class="k">this</span><span class="p">).</span><span class="nx">attr</span><span class="p">({</span><span class="s2">&quot;title&quot;</span><span class="o">:</span><span class="s2">&quot;展开&quot;</span><span class="p">});</span>
</span><span class='line'>    <span class="p">}</span><span class="k">else</span><span class="p">{</span>
</span><span class='line'>        <span class="nx">$</span><span class="p">(</span><span class="k">this</span><span class="p">).</span><span class="nx">html</span><span class="p">(</span><span class="s2">&quot;导航[-]&quot;</span><span class="p">);</span>
</span><span class='line'>        <span class="nx">$</span><span class="p">(</span><span class="k">this</span><span class="p">).</span><span class="nx">attr</span><span class="p">({</span><span class="s2">&quot;title&quot;</span><span class="o">:</span><span class="s2">&quot;收起&quot;</span><span class="p">});</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="nx">$</span><span class="p">(</span><span class="s2">&quot;#anchorContent&quot;</span><span class="p">).</span><span class="nx">toggle</span><span class="p">();</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>实现的方法就是注册anchorContentToggle元素的点击响应函数。</p>

<p>通过调用anchorContent元素的toggle方法来调整anchorContent的可见性，达到收放的效果。</p>

<h2>锚点自动跳转</h2>

<p>我们希望点击目录中的某个条目，能够跳转到博文中相应的位置，这可以借助锚点实现。</p>

<p>向blogDirectory.js中继续添加代码：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">$</span><span class="p">(</span><span class="s2">&quot;.anchor-link&quot;</span><span class="p">).</span><span class="nx">click</span><span class="p">(</span><span class="kd">function</span><span class="p">(){</span>
</span><span class='line'>    <span class="nx">$</span><span class="p">(</span><span class="s2">&quot;html,body&quot;</span><span class="p">).</span><span class="nx">animate</span><span class="p">({</span><span class="nx">scrollTop</span><span class="o">:</span> <span class="nx">$</span><span class="p">(</span><span class="nx">$</span><span class="p">(</span><span class="k">this</span><span class="p">).</span><span class="nx">attr</span><span class="p">(</span><span class="s2">&quot;link&quot;</span><span class="p">)).</span><span class="nx">offset</span><span class="p">().</span><span class="nx">top</span><span class="p">},</span> <span class="mi">400</span><span class="p">);</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>实现了每个条目的点击响应函数，即跳转到相应的锚点位置，这里使用了JQuery animate方法来实现平滑过渡。</p>

<h2>样式调节美化</h2>

<p>到这里，目录的基本功能已经实现了。我们需要调整一下样式，美化一下目录。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class='css'><span class='line'><span class="nc">.anchorContent</span> <span class="nt">a</span> <span class="p">{</span>
</span><span class='line'>  <span class="k">color</span><span class="o">:</span> <span class="m">#999</span><span class="p">;</span>
</span><span class='line'>  <span class="k">font-size</span><span class="o">:</span> <span class="m">14px</span><span class="p">;</span>
</span><span class='line'>  <span class="k">text-decoration</span><span class="o">:</span> <span class="k">none</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="nc">.anchorContent</span> <span class="nt">a</span><span class="nd">:hover</span> <span class="p">{</span>
</span><span class='line'>  <span class="k">color</span><span class="o">:</span> <span class="err">$</span><span class="n">theme</span><span class="o">-</span><span class="n">color1</span><span class="p">;</span>
</span><span class='line'>  <span class="k">font-size</span><span class="o">:</span> <span class="m">16px</span><span class="p">;</span>
</span><span class='line'>  <span class="k">font-weight</span><span class="o">:</span> <span class="k">bold</span><span class="p">;</span>
</span><span class='line'>  <span class="k">padding-left</span><span class="o">:</span> <span class="m">5px</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'>
</span><span class='line'><span class="nf">#anchorContentToggle</span> <span class="p">{</span>
</span><span class='line'>  <span class="k">color</span><span class="o">:</span> <span class="m">#a61c00</span><span class="p">;</span>
</span><span class='line'>  <span class="k">cursor</span><span class="o">:</span> <span class="k">pointer</span><span class="p">;</span>
</span><span class='line'>  <span class="k">font-size</span><span class="o">:</span> <span class="m">16px</span><span class="p">;</span>
</span><span class='line'>  <span class="k">font-weight</span><span class="o">:</span> <span class="k">bold</span><span class="p">;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>这样鼠标扫过相关条目会有字号颜色的改变，起到突出显示的效果。</p>

<p>最终效果如下：</p>

<p><img src="http://lionheartwang.github.io/images/blog/03-blog_directory.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark HistoryServer 配置和使用方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/08/24/spark-historyserver-configuration/"/>
    <updated>2016-08-24T17:17:58+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/08/24/spark-historyserver-configuration</id>
    <content type="html"><![CDATA[<p>本文介绍Spark History Server的配置和使用方法。</p>

<!--More-->


<h2>History Server 配置</h2>

<p>Spark提供了History Server服务可以保存历史Application的运行记录。</p>

<h3>客户端配置</h3>

<p>对于提交应用程序的客户端需要在conf/spark-defaults.conf中配置以下参数：</p>

<table>
<thead>
<tr>
<th> 参数        </th>
<th style="text-align:left;"> 功能           </th>
</tr>
</thead>
<tbody>
<tr>
<td> spark.eventLog.enabled      </td>
<td style="text-align:left;"> 是否记录Spark事件，用于应用程序在完成后重构webUI。 </td>
</tr>
<tr>
<td> spark.eventLog.dir      </td>
<td style="text-align:left;"> spark.eventLog.enabled为 true，该属性为记录spark事件的根目录。在此根目录中，Spark为每个应用程序创建分目录，并将应用程序的事件记录到此目录中。<br>可以将此属性设置为HDFS目录，以便history server读取历史记录文件。</br>      </td>
</tr>
<tr>
<td> spark.yarn.historyServer.address </td>
<td style="text-align:left;"> Spark history server的地址。 这个地址会在Spark应用程序完成后提交给YARN RM，然后RM将信息从RM UI写到history server UI上。<br><strong><font color=red>注意：hostname:port，前面不加http：//，末尾也不要加反斜杠。</font></strong> </br>     </td>
</tr>
</tbody>
</table>


<h3>服务端配置</h3>

<p>服务端主要需要在conf/spark-defaults.conf中配置如下属性：</p>

<table>
<thead>
<tr>
<th> 参数        </th>
<th style="text-align:left;"> 功能           </th>
<th style="text-align:left;"> 默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td> spark.history.ui.port     </td>
<td style="text-align:left;"> History Server的默认访问端口。<strong><font color=red>建议配置在8000~9000之间，以确保再内网浏览器能够正常显示。</strong><font color=red> </td>
<td style="text-align:left;"> 18080 </td>
</tr>
<tr>
<td> spark.history.fs.logDirectory    </td>
<td style="text-align:left;">   用于指定HistoryServer读取的eventlog存放的hdfs路径。  </td>
<td style="text-align:left;">无</td>
</tr>
<tr>
<td> spark.history.updateInterval </td>
<td style="text-align:left;">  History Server显示信息的刷新时间间隔，以秒为单位。每次更新都会检查持久层事件日志的任何变化。  </td>
<td style="text-align:left;"> 10 </td>
</tr>
<tr>
<td>spark.history.retainedApplications</td>
<td style="text-align:left;">在History Server上显示的最大应用程序数量，如果超过这个值，旧的应用程序信息将被删除。</td>
<td style="text-align:left;">250</td>
</tr>
</tbody>
</table>


<p>如果使用Kerberos认证可以配置如下参数：</p>

<table>
<thead>
<tr>
<th> 参数        </th>
<th style="text-align:left;"> 功能           </th>
<th style="text-align:left;"> 默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td> spark.history.kerberos.enabled    </td>
<td style="text-align:left;"> 是否使用kerberos方式登录访问history server。</td>
<td style="text-align:left;">false</td>
</tr>
<tr>
<td>spark.history.kerberos.principal</td>
<td style="text-align:left;">spark.history.kerberos.enabled为true时使用，用于指定History Server的kerberos主体名称</td>
<td style="text-align:left;">空</td>
</tr>
<tr>
<td>spark.history.kerberos.keytab</td>
<td style="text-align:left;">spark.history.kerberos.enabled为true时使用，用于指定History Server的kerberos keytab文件位置</td>
<td style="text-align:left;">空</td>
</tr>
<tr>
<td>spark.history.ui.acls.enable</td>
<td style="text-align:left;">授权用户查看应用程序信息的时候是否检查acl。如果启用，无论应用程序的spark.ui.acls.enable怎么设置，都要进行授权检查。<br>只有应用程序所有者和spark.ui.view.acls指定的用户可以查看应用程序信息; 如果禁用，不做任何检查。</br></td>
<td style="text-align:left;">false</td>
</tr>
</tbody>
</table>


<p>另外，服务端可以配置以下环境变量：</p>

<table>
<thead>
<tr>
<th> 参数        </th>
<th style="text-align:left;"> 功能           </th>
</tr>
</thead>
<tbody>
<tr>
<td>SPARK_DAEMON_JAVA_OPTS</td>
<td style="text-align:left;">History Server的JVM参数，默认为空</td>
</tr>
<tr>
<td>SPARK_DAEMON_MEMORY</td>
<td style="text-align:left;">分配给History Server的内存大小，默认512M</td>
</tr>
<tr>
<td>SPARK_HISTORY_OPTS</td>
<td style="text-align:left;">History Server的属性设置，默认为空。</td>
</tr>
<tr>
<td>SPARK_PUBLIC_DNS</td>
<td style="text-align:left;">History Server的公网地址，如果不设置，可以用内网地址来访问，默认为空。</td>
</tr>
</tbody>
</table>


<h2>History Server 使用</h2>

<h3>启动</h3>

<p>一般将客户端运行生成的eventlog统一存放在一个HDFS路径下便于查询历史记录，然后History Server端 spark.history.fs.logDirectory的值设为该路径即可。</p>

<p>启动History Server命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$sh sbin/start-history-server.sh</span></code></pre></td></tr></table></div></figure>


<h3>停止</h3>

<p>停止History Server命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$sh sbin/stop-history-server.sh</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[免密码远程登录/拷贝方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/08/18/no-pass-world-ssh-method/"/>
    <updated>2016-08-18T15:41:28+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/08/18/no-pass-world-ssh-method</id>
    <content type="html"><![CDATA[<p>生产环境中尝尝需要在机器间建立信任关系，远程登录或拷贝大量数据，本文介绍非如何快速免密码登录机器或拷贝数据。</p>

<!--more-->


<h2>生成密钥</h2>

<p>设有源机器 A，目标机器 B。现需要从A免密码登录B，或从B拷贝数据到A。</p>

<p>首先需要生成密钥。登录A机器，执行</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ssh-keygen -t rsa</span></code></pre></td></tr></table></div></figure>


<p>一路回车到底。这时在A主机~/.ssh目录下生成了私钥id_rsa以及陪对的id_rsa.pub公钥文件。</p>

<h2>复制公钥</h2>

<p>接下来把公钥复制到远程主机B。</p>

<p>即把公钥id_rsa.pub追加写入到机器B的 ~/.ssh/authorized_keys文件中即可。</p>

<p>执行如下命令：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$cat ~/.ssh/id_rsa.pub | ssh user@host "mkdir ~/.ssh; cat &gt;&gt; ~/.ssh/authorized_keys"</span></code></pre></td></tr></table></div></figure>


<p>如有多台远程主机多次复制即可。</p>

<p>此时已经可以免密码从B拷贝数据到A或从A ssh登录到B。</p>

<p>如果机器A和B用户一致的话，就可以直接从A ssh hostname 直接登陆B。</p>

<p>若不一致则可通过ssh username@hostname登录。</p>

<h2>配置ssh</h2>

<p>对于本地机器用户和远程登录用户名不同的情况可通过修改本地登陆用户的 ~/.ssh/config 文件解决。</p>

<p>向 ~/.ssh/config 中添加内容如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Host 机器B的hostname
</span><span class='line'>user 登录用户名
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>这样，本地机器A和远程机器B用户名不一致也可 ssh hostname 免密码直接登陆。</p>
]]></content>
  </entry>
  
</feed>
