<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[机器学习 | Workspace of LionHeart]]></title>
  <link href="http://lionheartwang.github.io/blog/categories/机器学习/atom.xml" rel="self"/>
  <link href="http://lionheartwang.github.io/"/>
  <updated>2018-03-11T00:38:56+08:00</updated>
  <id>http://lionheartwang.github.io/</id>
  <author>
    <name><![CDATA[Wang Yiguang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tensorflow模型保存与加载方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa/"/>
    <updated>2017-12-10T16:57:13+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa</id>
    <content type="html"><![CDATA[<p>本文档介绍如何保存和读取Tensorflow变量和模型。</p>

<p>官方文档可参考：</p>

<ul>
<li><a href="https://www.tensorflow.org/programmers_guide/saved_model">https://www.tensorflow.org/programmers_guide/saved_model</a></li>
</ul>


<!--More-->


<h2>保存/读取变量</h2>

<p>本节介绍如何存取Tensorflow变量。注意Estimators会自动在model_dir中存取变量。</p>

<p><code>tf.train.Saver</code> 提供了存取模型的接口，其构造函数会在图中增加存取模型的op。</p>

<p>示例化的Saver对象提供方法来运行这些op并，设置checkpoint文件用于保存和恢复模型信息。</p>

<p>Saver关于将模型中定义的所有变量保存，如果你不了解加载模型的graph结构，可以参考后面的保存恢复模型一节。</p>

<p>TensorFlow将变量以二进制形式存储在文件中，保存的信息主要是变量名以及对应的Tensor的值。</p>

<h3>保存变量</h3>

<p>调用tf.train.Saver()来管理模型变量，示例代码：</p>

<pre><code class="python"># Create some variables.
v1 = tf.get_variable("v1", shape=[3], initializer = tf.zeros_initializer)
v2 = tf.get_variable("v2", shape=[5], initializer = tf.zeros_initializer)

inc_v1 = v1.assign(v1+1)
dec_v2 = v2.assign(v2-1)

# Add an op to initialize the variables.
init_op = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables, do some work, and save the
# variables to disk.
with tf.Session() as sess:
  sess.run(init_op)
  # Do some work with the model.
  inc_v1.op.run()
  dec_v2.op.run()
  # Save the variables to disk.
  save_path = saver.save(sess, "/tmp/model.ckpt")
  print("Model saved in file: %s" % save_path)
</code></pre>

<h3>恢复变量</h3>

<p>同样使用tf.train.Saver来从checkpoint中恢复变量，示例代码：</p>

<pre><code class="python">tf.reset_default_graph()

# Create some variables.
v1 = tf.get_variable("v1", shape=[3])
v2 = tf.get_variable("v2", shape=[5])

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, use the saver to restore variables from disk, and
# do some work with the model.
with tf.Session() as sess:
  # Restore variables from disk.
  saver.restore(sess, "/tmp/model.ckpt")
  print("Model restored.")
  # Check the values of the variables
  print("v1 : %s" % v1.eval())
  print("v2 : %s" % v2.eval())
</code></pre>

<h3>存取指定的变量</h3>

<p>有时可能只需要存取模型graph中的部分变量，可以list或者dict两种形式传给tf.train.Saver()来指定需要存取的变量。</p>

<ul>
<li>list形式，模型的变量列表。</li>
<li>dict形式，name为新的保存后的变量名，value为模型中的变量名。</li>
</ul>


<p>示例代码：</p>

<pre><code class="python">tf.reset_default_graph()
# Create some variables.
v1 = tf.get_variable("v1", [3], initializer = tf.zeros_initializer)
v2 = tf.get_variable("v2", [5], initializer = tf.zeros_initializer)

# Add ops to save and restore only `v2` using the name "v2"
saver = tf.train.Saver({"v2": v2})

# Use the saver object normally after that.
with tf.Session() as sess:
  # Initialize v1 since the saver will not.
  v1.initializer.run()
  saver.restore(sess, "/tmp/model.ckpt")

  print("v1 : %s" % v1.eval())
  print("v2 : %s" % v2.eval())
</code></pre>

<p>注意：</p>

<ul>
<li>可以创建多个模型Saver对象来保存模型变量，同样的变量可被不同的Saver保存多次，restore操作用于从checkpoint中恢复变量的值。</li>
<li>如果checkpoint中只保存了部分变量，那么恢复后，graph中其他的变量仍需要被初始化。</li>
<li>查看checkpoint中的变量可以使用inspect_checkpoint库，特别是print_tensors_in_checkpoint_file函数。</li>
<li>Saver默认使用tf.Variable.name属性作为每个变量的变量名，然而你可以在Saver对象中未变量指定存储在checkpoint中的新名字。</li>
</ul>


<h2>保存/读取模型概述</h2>

<p>当你想要保存整个模型(变量、模型graph以及graph的meta信息)时，我们推荐使用<code>SavedModel</code>。</p>

<p>SavedModel是一种面向多语言的，可恢复的高度序列化封装的格式。</p>

<p>SavedModel运行上层系统或工具来生产、消费或者转换Tensorflow模型。</p>

<p>Tensorflow提供了多种机制来同SavedModel进行交互，包括tf.saved_model API, Estimator API 以及 CLI方式。</p>

<h2>操作SavedModel API</h2>

<p>本节聚焦在使用底层Tensorflow API时需要用到的保存或加载SavedModel的API。</p>

<h3>构建SavedModel</h3>

<p>我们提供了SavedModel builder的python实现。SavedModelBuilder提供保存MetaGraphDef结构的功能。</p>

<ul>
<li>MetaGraphDef是MetaGraph的proto buffer表达形式。</li>
<li>MetaGraph是一个数据流图，以及相关的变量、资源和signatures。</li>
<li>signature是一个graph的输入与输出的集合。</li>
</ul>


<p>每个加入到 SavedModel中的MetaGraphDef需要以用户指定的tag标注。tag提供了区分特定MetaGraphDef的方法。通常这些tag会标注MetaGraphDef的功能以及一些可选的硬件相关的信息。</p>

<p>SavedModelBuilder的使用示例代码如下：</p>

<pre><code class="python">export_dir = ...
...
builder = tf.saved_model_builder.SavedModelBuilder(export_dir)
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph_and_variables(sess,
                                       [tag_constants.TRAINING],
                                       signature_def_map=foo_signatures,
                                       assets_collection=foo_assets)
...
# Add a second MetaGraphDef for inference.
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph([tag_constants.SERVING])
...
builder.save()
</code></pre>

<h3>加载SaveModel</h3>

<p>调用python版本的SaveModel loader需要提供一下信息：</p>

<ul>
<li>保存graph定义和变量的session。</li>
<li>用来标识MetaGraphDef的tag。</li>
<li>SavedModel对应的目录位置。</li>
</ul>


<p>示例代码如下：</p>

<pre><code class="python">export_dir = ...
...
with tf.Session(graph=tf.Graph()) as sess:
  tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)
  ...
</code></pre>

<p>C++版本的SavedModel loader也提供从指定目录恢复模型的API，并支持指定SessionOptions和RunOptions参数。</p>

<p>同样需要指定taq参数，被加载的SavedModel被SavedModelBundle引用，包含了MetaGraphDef以及加载它的session信息。</p>

<p>示例代码：</p>

<pre><code class="c++">const string export_dir = ...
SavedModelBundle bundle;
...
LoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain},
               &amp;bundle);
</code></pre>

<p>另外Tensorflow提供了一组MetaGraphDef和SignatureDef相关的常量供用户使用。</p>

<p>MetaGraphDef常量：</p>

<ul>
<li>python：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/tag_constants.py">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/tag_constants.py</a></li>
<li>c++：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/tag_constants.h">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/tag_constants.h</a></li>
</ul>


<p>SignatureDef常量</p>

<ul>
<li>python：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/signature_constants.py">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/signature_constants.py</a></li>
<li>c++：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/signature_constants.h">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/signature_constants.h</a></li>
</ul>


<h2>在Estimators中使用SaveModel</h2>

<p>当训练好Estimator模型后，你可能需要部署预测服务。你可以选择在本机启动一个本地服务，或在云端进行扩展。</p>

<p>部署Estimator训练出的模型需要先把模型导出为SaveModel格式，本节介绍：</p>

<ul>
<li>如何制定输出节点以及相关的API</li>
<li>使用SavedModel导出模型</li>
<li>发请求给本地模型预测服务</li>
</ul>


<h3>准备Serving输入</h3>

<p>训练过程中 <code>input_fn()</code>用于提供数据输入，类似地，预测阶段输入数据由 <code>serving_input_receiver_fn</code>提供。</p>

<p>serving_input_receiver_fn有如下两个功能：</p>

<ul>
<li>将预测需要输入数据的placeholder添加到graph中。</li>
<li>添加额外的用于将输入数据格式转换为feature Tensors格式的op。</li>
</ul>


<p>函数返回 <code>tf.estimator.export.ServingInputReceiver</code>对象，封装了placeholders以及feature Tensor。</p>

<p>当编写 <code>serving_input_receiver_fn</code>时，需要提供一个 tf.parse_example的特定parser描述来说明数据解析的方式。</p>

<p>Parser说明是一个dict的形式，包含：</p>

<ul>
<li>tf.FixedLenFeature</li>
<li>tf.VarLenFeature</li>
<li>tf.SparseFeature</li>
</ul>


<p>示例代码：</p>

<pre><code class="python">feature_spec = {'foo': tf.FixedLenFeature(...),
                'bar': tf.VarLenFeature(...)}

def serving_input_receiver_fn():
  """An input receiver that expects a serialized tf.Example."""
  serialized_tf_example = tf.placeholder(dtype=tf.string,
                                         shape=[default_batch_size],
                                         name='input_example_tensor')
  receiver_tensors = {'examples': serialized_tf_example}
  features = tf.parse_example(serialized_tf_example, feature_spec)
  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)
</code></pre>

<p>tf.estimator.export.build_parsing_serving_input_receiver_fn 工具函数给出了一个通用实现。</p>

<h3>导出模型</h3>

<p>调用 <code>tf.estimator.Estimator.export_savedmodel</code> ，提供导出路径以及serving_input_receiver_fn进行模型导出：</p>

<pre><code class="python">estimator.export_savedmodel(export_dir_base, serving_input_receiver_fn)
</code></pre>

<p>该方法创建一个新的graph并调用serving_input_receiver_fn来获取输入tensor，随后调用Estimator的<code>model_fn()</code> 来产生模型的graph。</p>

<p>最终会创建一个带时间戳的目录(export_dir_base/<timestamp>)并将模型导出为SavedModel。</p>

<h3>指定模型输出</h3>

<p>通过export_outputs指定，其类型为 <code>tf.estimator.EstimatorSpec</code>，是一个形如 {name: output} 的dict，用于描述预测阶段的输出。</p>

<p>预测输出的值类型必须为 <code>ExportOutput</code> 的某个实现，例如：</p>

<ul>
<li>tf.estimator.export.ClassificationOutput,</li>
<li>tf.estimator.export.RegressionOutput</li>
<li>tf.estimator.export.PredictOutput.</li>
</ul>


<h3>部署本地预测服务</h3>

<p>本地部署预测服务需要使用TensorFlow Serving。</p>

<p>TensorFlow Serving是一个独立的开源项目，功能是加载SavedModel模型并对外提供gRPC服务。</p>

<p>首先安装Tensorflow Serving。</p>

<p>部署服务命令如下，将 <code>$export_dir_base</code> 替换为SavedModel导出的目录。</p>

<pre><code class="bash">bazel build //tensorflow_serving/model_servers:tensorflow_model_server
bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_base_path=$export_dir_base
</code></pre>

<p>执行后在9000端口会启动一个gRPC预测服务。</p>

<h3>向本地Server发送请求</h3>

<p>发送预测请求需要通过PredictionService gRPC API。相关API依赖：</p>

<pre><code class="bash">  deps = [
    "//tensorflow_serving/apis:classification_proto_py_pb2",
    "//tensorflow_serving/apis:regression_proto_py_pb2",
    "//tensorflow_serving/apis:predict_proto_py_pb2",
    "//tensorflow_serving/apis:prediction_service_proto_py_pb2"
  ]
</code></pre>

<p>python代码中可以像如下示例使用：</p>

<pre><code class="python">from tensorflow_serving.apis import classification_pb2
from tensorflow_serving.apis import regression_pb2
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2
</code></pre>

<p>请求的数据会以proto buffer的形式发送，发送请求的示例代码：</p>

<pre><code class="python">from grpc.beta import implementations

channel = implementations.insecure_channel(host, int(port))
stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)

request = classification_pb2.ClassificationRequest()
example = request.input.example_list.examples.add()
example.features.feature['x'].float_list.value.extend(image[0].astype(float))

result = stub.Classify(request, 10.0)  # 10 secs timeout
</code></pre>

<p>本例中的返回值是一个 <code>ClassificationResponse</code>格式的PB数据。</p>

<h2>SavedModel目录结构</h2>

<p>Tensorflow为每个SavedModel组织目录结构形式如下：</p>

<pre><code class="bash">assets/
assets.extra/
variables/
    variables.data-?????-of-?????
    variables.index
saved_model.pb|saved_model.pbtxt
</code></pre>

<p>说明如下：</p>

<ul>
<li>assets：是一个子目录，包含了一些外部文件，例如词表等，这些资源文件会被特定的MetaGraphDef读取使用。</li>
<li>assets.extra：是一个子目录，用于上层应用或者用户添加一些自己的资源文件，但不会被模型的graph加载。该目录不由SavedModel管理。</li>
<li>variables：是一个子目录，用于存储tf.train.Saver的输出。</li>
<li>saved_model.pb/saved_model.pbtxt SavedModel的Proto Buffer描述。包含了MetaGraphDef的proto buffer形式的定义。</li>
</ul>


<p>一个单独的SavedModel可以表达多个graph，SavedModel的多个graph共享一组checkpoint(变量和资源文件)。</p>

<p>组织形式如下图所示：</p>

<p><img src="/images/blog/10-savemodel.png"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[论文阅读: On-line Random Forest]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/11/30/on-line-random-forest-paper/"/>
    <updated>2017-11-30T21:25:38+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/11/30/on-line-random-forest-paper</id>
    <content type="html"><![CDATA[<p>2009年的一篇论文, 提出了一种在线随机森林算法（ORF）。</p>

<p>原文链接：<a href="http://ieeexplore.ieee.org/abstract/document/5457447/?reload=true">http://ieeexplore.ieee.org/abstract/document/5457447/?reload=true</a></p>

<!-- More -->


<h2>本文解决的问题</h2>

<p>经典的随机森林算法是离线训练，每次要基于全局数据生成一系列决策树，但在线环境下难以获得全局数据，因而无法直接使用。</p>

<p>本文提出的算法设计参考了online-bagging的思路，效果接近离线版本的算法。</p>

<h2>核心内容</h2>

<p>本文提出的在线online random forest算法中，每棵树可以在线分裂。样本采样时每棵树采样次数基于泊松分布，每个叶子分裂的条件是预测的数量要达到一定的值和每个叶子节点信息。</p>

<p>每个树的生长主要通过在线接收的实时样本数据，每棵树的叶子节点分裂主要根据该节点的熵或Gini系数。</p>

<p>算法流程如下图所示：</p>

<p><img src="/images/blog/07-online_random_forest.png"></p>

<p>说明如下：</p>

<ul>
<li>步骤3. 用个possion分布确定从采样的次数，其原理见online boosting： <a href="http://www.cnblogs.com/liqizhou/archive/2012/05/10/2494145.html">http://www.cnblogs.com/liqizhou/archive/2012/05/10/2494145.html</a></li>
<li>步骤6. u代表分类的类别。</li>
<li>步骤7. j代表第t棵树上叶子节点。</li>
<li>步骤8. 统计第j个叶子节点的数目和计算Gini。</li>
<li>步骤9. 判断条件是否分裂的二个条件。</li>
<li>步骤10. 在符合条件的叶子节点中，选择一个Gini最大的叶子节点作为分类节点。</li>
</ul>


<p>有时在线训练过程中希望丢弃较旧的信息，需要适当丢弃随机森林中的某些树。针对这种需求，原文给出了随机森林中树的选择性丢弃方案。</p>

<p>算法流程如下图所示：</p>

<p><img src="/images/blog/08-orf_temporal_knowledge_weighting.png"></p>

<p>附C++源码实现：</p>

<ul>
<li><a href="https://github.com/amirsaffari/online-random-forests">https://github.com/amirsaffari/online-random-forests</a></li>
</ul>


<h2>本文解决方案效果</h2>

<p>随着样本数量增加，算法效果逼近离线随机森林算法。</p>
]]></content>
  </entry>
  
</feed>
