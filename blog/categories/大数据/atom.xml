<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[大数据 | Workspace of LionHeart]]></title>
  <link href="http://lionheartwang.github.io/blog/categories/大数据/atom.xml" rel="self"/>
  <link href="http://lionheartwang.github.io/"/>
  <updated>2018-03-05T01:11:31+08:00</updated>
  <id>http://lionheartwang.github.io/</id>
  <author>
    <name><![CDATA[Wang Yiguang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Flink架构及工作原理介绍]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/03/05/flink-framwork-introduction/"/>
    <updated>2018-03-05T00:54:42+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/03/05/flink-framwork-introduction</id>
    <content type="html"><![CDATA[<p>本文整体介绍Apache Flink流计算框架的特性、概念、组件栈、架构及原理分析。</p>

<p>主要内容参考如下博客整理：</p>

<ul>
<li><a href="http://shiyanjun.cn/archives/1508.html">http://shiyanjun.cn/archives/1508.html</a></li>
</ul>


<!--More-->


<h2>Flink框架整体介绍</h2>

<p>Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台。</p>

<p>它能够基于同一个Flink运行时（Flink Runtime），提供支持流处理和批处理两种类型应用的功能。</p>

<p>现有的开源计算方案，会把流处理和批处理作为两种不同的应用类型，因为他们它们所提供的SLA是完全不相同的：</p>

<ul>
<li>流处理一般需要支持低延迟、Exactly-once保证</li>
<li>批处理需要支持高吞吐、高效处理</li>
</ul>


<p>所以在实现的时候通常是分别给出两套实现方法，或者通过一个独立的开源框架来实现其中每一种处理方案。</p>

<p>例如，实现批处理的开源方案有MapReduce、Tez、Crunch、Spark，实现流处理的开源方案有Samza、Storm。</p>

<p>Flink在实现流处理和批处理时，与传统的一些方案完全不同，它从另一个视角看待流处理和批处理，将二者统一起来：</p>

<ul>
<li>Flink是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；</li>
<li>批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。</li>
<li>基于同一个Flink运行时（Flink Runtime），分别提供了流处理和批处理API，而这两种API也是实现上层面向流处理、批处理类型应用框架的基础。</li>
</ul>


<h3>基本特性</h3>

<p>关于Flink所支持的特性，我这里只是通过分类的方式简单做一下梳理，涉及到具体的一些概念及其原理会在后面的部分做详细说明。</p>

<p>流处理特性</p>

<ul>
<li>支持高吞吐、低延迟、高性能的流处理</li>
<li>支持带有事件时间的窗口（Window）操作</li>
<li>支持有状态计算的Exactly-once语义</li>
<li>支持高度灵活的窗口（Window）操作，支持基于time、count、session，以及data-driven的窗口操作</li>
<li>支持具有Backpressure功能的持续流模型</li>
<li>支持基于轻量级分布式快照（Snapshot）实现的容错</li>
<li>一个运行时同时支持Batch on Streaming处理和Streaming处理</li>
<li>Flink在JVM内部实现了自己的内存管理</li>
<li>支持迭代计算</li>
<li>支持程序自动优化：避免特定情况下Shuffle、排序等昂贵操作，中间结果有必要进行缓存</li>
</ul>


<h3>API支持</h3>

<p>对Streaming数据类应用，提供DataStream API</p>

<p>对批处理类应用，提供DataSet API（支持Java/Scala）</p>

<h3>Libraries支持</h3>

<p>相关上层Library支持情况如下：</p>

<ul>
<li>支持机器学习（FlinkML）</li>
<li>支持图分析（Gelly）</li>
<li>支持关系数据处理（Table）</li>
<li>支持复杂事件处理（CEP）</li>
</ul>


<p>与其他外部系统对接支持如下：</p>

<ul>
<li>支持Flink on YARN</li>
<li>支持HDFS</li>
<li>支持来自Kafka的输入数据</li>
<li>支持Apache HBase</li>
<li>支持Hadoop程序</li>
<li>支持Tachyon</li>
<li>支持ElasticSearch</li>
<li>支持RabbitMQ</li>
<li>支持Apache Storm</li>
<li>支持S3</li>
<li>支持XtreemFS</li>
</ul>


<h2>基本概念</h2>

<h3>Stream &amp; Transformation &amp; Operator</h3>

<p>用户实现的Flink程序是由Stream和Transformation这两个基本构建块组成，其中Stream是一个中间结果数据，而Transformation是一个操作，它对一个或多个输入Stream进行计算处理，输出一个或多个结果Stream。</p>

<p>当一个Flink程序被执行的时候，它会被映射为Streaming Dataflow。</p>

<p>一个Streaming Dataflow是由一组Stream和Transformation Operator组成，它类似于一个DAG图，在启动的时候从一个或多个Source Operator开始，结束于一个或多个Sink Operator。</p>

<p>下面是一个由Flink程序映射为Streaming Dataflow的示意图，如下所示：</p>

<p><img src="/images/blog/11-flink-streaming-dataflow-example.png"></p>

<p>上图中，FlinkKafkaConsumer是一个Source Operator，map、keyBy、timeWindow、apply是Transformation Operator，RollingSink是一个Sink Operator。</p>

<h3>Parallel Dataflow</h3>

<p>在Flink中，程序天生是并行和分布式的：</p>

<ul>
<li>一个Stream可以被分成多个Stream分区（Stream Partitions），一个Operator可以被分成多个Operator Subtask，每一个Operator Subtask是在不同的线程中独立执行的。</li>
<li>一个Operator的并行度，等于Operator Subtask的个数，一个Stream的并行度总是等于生成它的Operator的并行度。</li>
</ul>


<p>有关Parallel Dataflow的实例，如下图所示：</p>

<p><img src="/images/blog/13-flink-parallel-dataflow.png"></p>

<p>上图Streaming Dataflow的并行视图中，展现了在两个Operator之间的Stream的两种模式：</p>

<h4>One-to-one模式</h4>

<p>比如从Source[1]到map()[1]，它保持了Source的分区特性（Partitioning）和分区内元素处理的有序性。</p>

<p>也就是说map()[1]的Subtask看到数据流中记录的顺序，与Source[1]中看到的记录顺序是一致的。</p>

<h4>Redistribution模式</h4>

<p>这种模式改变了输入数据流的分区。</p>

<p>比如从map()[1]、map()[2]到keyBy()/window()/apply()[1]、keyBy()/window()/apply()[2]，上游的Subtask向下游的多个不同的Subtask发送数据，改变了数据流的分区，这与实际应用所选择的Operator有关系。</p>

<p>另外，Source Operator对应2个Subtask，所以并行度为2，而Sink Operator的Subtask只有1个，故而并行度为1。</p>

<h3>Task &amp; Operator Chain</h3>

<p>在Flink分布式执行环境中，会将多个Operator Subtask串起来组成一个Operator Chain，实际上就是一个执行链。</p>

<p>每个执行链会在TaskManager上一个独立的线程中执行，如下图所示：</p>

<p><img src="/images/blog/14-flink-tasks-chains.png"></p>

<p>上图中上半部分表示的是一个Operator Chain，多个Operator通过Stream连接，而每个Operator在运行时对应一个Task。</p>

<p>图中下半部分是上半部分的一个并行版本，也就是对每一个Task都并行化为多个Subtask。</p>

<h3>Time &amp; Window</h3>

<p>Flink支持基于时间窗口操作，也支持基于数据的窗口操作，如下图所示：</p>

<p><img src="/images/blog/15-flink-window.png"></p>

<p>上图中，基于时间的窗口操作，在每个相同的时间间隔对Stream中的记录进行处理，通常各个时间间隔内的窗口操作处理的记录数不固定。</p>

<p>而基于数据驱动的窗口操作，可以在Stream中选择固定数量的记录作为一个窗口，对该窗口中的记录进行处理。</p>

<p>有关窗口操作的不同类型，可以分为如下几种：</p>

<ul>
<li>倾斜窗口（Tumbling Windows，记录没有重叠）</li>
<li>滑动窗口（Slide Windows，记录有重叠）</li>
<li>会话窗口（Session Windows）</li>
</ul>


<p>具体可以查阅相关资料。</p>

<p>在处理Stream中的记录时，记录中通常会包含各种典型的时间字段，Flink支持多种时间的处理，如下图所示：</p>

<p><img src="/images/blog/16-flink-event-ingestion-processing-time.png"></p>

<p>上图描述了在基于Flink的流处理系统中，各种不同的时间所处的位置和含义。</p>

<p>其中：</p>

<ul>
<li>Event Time表示事件创建时间</li>
<li>Ingestion Time表示事件进入到Flink Dataflow的时间</li>
<li>Processing Time表示某个Operator对事件进行处理事的本地系统时间（是在TaskManager节点上）。</li>
</ul>


<p>这里，谈一下基于Event Time进行处理的问题。</p>

<p>通常根据Event Time会给整个Streaming应用带来一定的延迟性，因为在一个基于事件的处理系统中，进入系统的事件可能会基于Event Time而发生乱序现象。</p>

<p>比如事件来源于外部的多个系统，为了增强事件处理吞吐量会将输入的多个Stream进行自然分区，每个Stream分区内部有序，但是要保证全局有序必须同时兼顾多个Stream分区的处理，设置一定的时间窗口进行暂存数据，当多个Stream分区基于Event Time排列对齐后才能进行延迟处理。</p>

<p>所以，设置的暂存数据记录的时间窗口越长，处理性能越差，甚至严重影响Stream处理的实时性。</p>

<p>有关基于时间的Streaming处理，可以参考官方文档，在Flink中借鉴了Google使用的WaterMark实现方式，可以查阅相关资料。</p>

<h2>基本架构</h2>

<p>Flink系统的架构与Spark类似，是一个基于Master-Slave风格的架构，如下图所示：</p>

<p><img src="/images/blog/17-flink-system-architecture.png"></p>

<p>Flink集群启动时，会启动一个JobManager进程、至少一个TaskManager进程。</p>

<p>在Local模式下，会在同一个JVM内部启动一个JobManager进程和TaskManager进程。</p>

<p>当Flink程序提交后，会创建一个Client来进行预处理，并转换为一个并行数据流，这是对应着一个Flink Job，从而可以被JobManager和TaskManager执行。</p>

<p>在实现上，Flink基于Actor实现了JobManager和TaskManager，所以JobManager与TaskManager之间的信息交换，都是通过事件的方式来进行处理。</p>

<p>如上图所示，Flink系统主要包含如下3个主要的进程：</p>

<h3>JobManager</h3>

<p>JobManager是Flink系统的协调者，它负责接收Flink Job，调度组成Job的多个Task的执行。</p>

<p>同时，JobManager还负责收集Job的状态信息，并管理Flink集群中从节点TaskManager。</p>

<p>JobManager所负责的各项管理功能，它接收到并处理的事件主要包括：</p>

<ul>
<li>RegisterTaskManager：在Flink集群启动的时候，TaskManager会向JobManager注册，如果注册成功，则JobManager会向TaskManager回复消息AcknowledgeRegistration。</li>
<li>SubmitJob：Flink程序内部通过Client向JobManager提交Flink Job，其中在消息SubmitJob中以JobGraph形式描述了Job的基本信息。</li>
<li>CancelJob：请求取消一个Flink Job的执行，CancelJob消息中包含了Job的ID，如果成功则返回消息CancellationSuccess，失败则返回消息CancellationFailure。</li>
<li>UpdateTaskExecutionState：TaskManager会向JobManager请求更新ExecutionGraph中的ExecutionVertex的状态信息，更新成功则返回true。</li>
<li>RequestNextInputSplit：运行在TaskManager上面的Task，请求获取下一个要处理的输入Split，成功则返回NextInputSplit。</li>
<li>JobStatusChanged：ExecutionGraph向JobManager发送该消息，用来表示Flink Job的状态发生的变化，例如：RUNNING、CANCELING、FINISHED等。</li>
</ul>


<h3>TaskManager</h3>

<p>TaskManager也是一个Actor，它是实际负责执行计算的Worker，在其上执行Flink Job的一组Task。</p>

<p>每个TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报。</p>

<p>TaskManager端可以分成两个阶段：</p>

<ul>
<li>注册阶段：TaskManager会向JobManager注册，发送RegisterTaskManager消息，等待JobManager返回AcknowledgeRegistration，然后TaskManager就可以进行初始化过程。</li>
<li>可操作阶段：该阶段TaskManager可以接收并处理与Task有关的消息，如SubmitTask、CancelTask、FailTask。</li>
</ul>


<p>如果TaskManager无法连接到JobManager，这是TaskManager就失去了与JobManager的联系，会自动进入“注册阶段”，只有完成注册才能继续处理Task相关的消息。</p>

<h3>Client</h3>

<p>当用户提交一个Flink程序时，会首先创建一个Client。</p>

<p>该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群中处理，所以Client需要从用户提交的Flink程序配置中获取JobManager的地址，并建立到JobManager的连接，将Flink Job提交给JobManager。</p>

<p>Client会将用户提交的Flink程序组装一个JobGraph， 并且是以JobGraph的形式提交的。</p>

<p>一个JobGraph是一个Flink Dataflow，它由多个JobVertex组成的DAG。</p>

<p>其中，一个JobGraph包含了一个Flink程序的如下信息：JobID、Job名称、配置信息、一组JobVertex等。</p>

<h2>组件栈</h2>

<p>Flink是一个分层架构的系统，每一层所包含的组件都提供了特定的抽象，用来服务于上层组件。Flink分层的组件栈如下图所示：</p>

<p><img src="/images/blog/18-flink-component-stack.png"></p>

<p>下面，我们自下而上，分别针对每一层进行解释说明。</p>

<h3>Deployment层</h3>

<p>该层主要涉及了Flink的部署模式，Flink支持多种部署模式：</p>

<ul>
<li>本地、集群（Standalone/YARN）</li>
<li>云（GCE/EC2）</li>
<li>Standalone部署模式与Spark类似。</li>
</ul>


<p>这里，我们看一下Flink on YARN的部署模式，如下图所示：</p>

<p><img src="/images/blog/19-flink-on-yarn.png"></p>

<p>了解YARN的话，对上图的原理非常熟悉，实际Flink也实现了满足在YARN集群上运行的各个组件：</p>

<ul>
<li>Flink YARN Client负责与YARN RM通信协商资源请求</li>
<li>Flink JobManager和Flink TaskManager分别申请到Container去运行各自的进程。</li>
</ul>


<p>通过上图可以看到，YARN AM与Flink JobManager在同一个Container中，这样AM可以知道Flink JobManager的地址，从而AM可以申请Container去启动Flink TaskManager。</p>

<p>待Flink成功运行在YARN集群上，Flink YARN Client就可以提交Flink Job到Flink JobManager，并进行后续的映射、调度和计算处理。</p>

<h3>Runtime层</h3>

<p>Runtime层提供了支持Flink计算的全部核心实现，比如：</p>

<ul>
<li>支持分布式Stream处理</li>
<li>JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务。</li>
</ul>


<h3>API层</h3>

<p>API层主要实现了面向无界Stream的流处理和面向Batch的批处理API。</p>

<p>其中面向流处理对应DataStream API，面向批处理对应DataSet API。</p>

<h3>Libraries层</h3>

<p>该层也可以称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。</p>

<ul>
<li>面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；</li>
<li>面向批处理支持：FlinkML（机器学习库）、Gelly（图处理）。</li>
</ul>


<h2>内部原理</h2>

<h3>容错机制</h3>

<p>Flink基于Checkpoint机制实现容错，它的原理是不断地生成分布式Streaming数据流Snapshot。</p>

<p>在流处理失败时，通过这些Snapshot可以恢复数据流处理。</p>

<h4>Barrier</h4>

<p>理解Flink的容错机制，首先需要了解一下Barrier这个概念：</p>

<ul>
<li>Stream Barrier是Flink分布式Snapshotting中的核心元素，它会作为数据流的记录被同等看待，被插入到数据流中，将数据流中记录的进行分组，并沿着数据流的方向向前推进。</li>
<li>每个Barrier会携带一个Snapshot ID，属于该Snapshot的记录会被推向该Barrier的前方。因为Barrier非常轻量，所以并不会中断数据流。带有Barrier的数据流。</li>
</ul>


<p>如下图所示：</p>

<p><img src="/images/blog/20-flink-stream-barriers.png"></p>

<p>基于上图，我们通过如下要点来说明：</p>

<ul>
<li>出现一个Barrier，在该Barrier之前出现的记录都属于该Barrier对应的Snapshot，在该Barrier之后出现的记录属于下一个Snapshot。</li>
<li>来自不同Snapshot多个Barrier可能同时出现在数据流中，也就是说同一个时刻可能并发生成多个Snapshot。</li>
<li>当一个中间（Intermediate）Operator接收到一个Barrier后，它会发送Barrier到属于该Barrier的Snapshot的数据流中，等到Sink Operator接收到该Barrier后会向Checkpoint Coordinator确认该Snapshot。</li>
<li>直到所有的Sink Operator都确认了该Snapshot，才被认为完成了该Snapshot。</li>
</ul>


<p>这里还需要强调的是，Snapshot并不仅仅是对数据流做了一个状态的Checkpoint，它也包含了一个Operator内部所持有的状态，这样才能够在保证在流处理系统失败时能够正确地恢复数据流处理。</p>

<p>也就是说，如果一个Operator包含任何形式的状态，这种状态必须是Snapshot的一部分。</p>

<h4>Operator State</h4>

<p>Operator的状态包含两种：</p>

<ul>
<li>一种是系统状态，一个Operator进行计算处理的时候需要对数据进行缓冲，所以数据缓冲区的状态是与Operator相关联的，以窗口操作的缓冲区为例，Flink系统会收集或聚合记录数据并放到缓冲区中，直到该缓冲区中的数据被处理完成；</li>
<li>另一种是用户自定义状态（状态可以通过转换函数进行创建和修改），它可以是函数中的Java对象这样的简单变量，也可以是与函数相关的Key/Value状态。</li>
</ul>


<p>对于具有轻微状态的Streaming应用，会生成非常轻量的Snapshot而且非常频繁，但并不会影响数据流处理性能。</p>

<p>Streaming应用的状态会被存储到一个可配置的存储系统中，例如HDFS。</p>

<p>在一个Checkpoint执行过程中，存储的状态信息及其交互过程，如下图所示：</p>

<p><img src="/images/blog/21-flink-checkpointing.png"></p>

<h4>Stream Aligning</h4>

<p>在Checkpoint过程中，还有一个比较重要的操作——Stream Aligning。</p>

<p>当Operator接收到多个输入的数据流时，需要在Snapshot Barrier中对数据流进行排列对齐，如下图所示：</p>

<p><img src="/images/blog/22-flink-stream-aligning.png"></p>

<p>具体排列过程如下：</p>

<ul>
<li>Operator从一个incoming Stream接收到Snapshot Barrier n，然后暂停处理，直到其它的incoming Stream的Barrier n（否则属于2个Snapshot的记录就混在一起了）到达该Operator。</li>
<li>接收到Barrier n的Stream被临时搁置，来自这些Stream的记录不会被处理，而是被放在一个Buffer中</li>
<li>一旦最后一个Stream接收到Barrier n，Operator会emit所有暂存在Buffer中的记录，然后向Checkpoint Coordinator发送Snapshot n</li>
<li>继续处理来自多个Stream的记录</li>
</ul>


<p>基于Stream Aligning操作能够实现Exactly Once语义，但是也会给流处理应用带来延迟，因为为了排列对齐Barrier，会暂时缓存一部分Stream的记录到Buffer中。</p>

<p>尤其是在数据流并行度很高的场景下可能更加明显，通常以最迟对齐Barrier的一个Stream为处理Buffer中缓存记录的时刻点。</p>

<p>在Flink中，提供了一个开关，选择是否使用Stream Aligning，如果关掉则Exactly Once会变成At least once。</p>

<h3>调度机制</h3>

<p>在JobManager端，会接收到Client提交的JobGraph形式的Flink Job。</p>

<p>JobManager会将一个JobGraph转换映射为一个ExecutionGraph，如下图所示：</p>

<p><img src="/images/blog/23-flink-job-and-execution-graph.png"></p>

<p>通过上图可以看出：</p>

<ul>
<li>JobGraph是一个Job的用户逻辑视图表示，将一个用户要对数据流进行的处理表示为单个DAG图（对应于JobGraph）</li>
<li>DAG图由顶点（JobVertex）和中间结果集（IntermediateDataSet）组成，</li>
<li>其中JobVertex表示了对数据流进行的转换操作，比如map、flatMap、filter、keyBy等操作，而IntermediateDataSet是由上游的JobVertex所生成，同时作为下游的JobVertex的输入。</li>
</ul>


<p>而ExecutionGraph是JobGraph的并行表示，也就是实际JobManager调度一个Job在TaskManager上运行的逻辑视图。</p>

<p>它也是一个DAG图，是由ExecutionJobVertex、IntermediateResult（或IntermediateResultPartition）组成</p>

<p>ExecutionJobVertex实际对应于JobGraph图中的JobVertex，只不过在ExecutionJobVertex内部是一种并行表示，由多个并行的ExecutionVertex所组成。</p>

<p>另外，这里还有一个重要的概念，就是Execution，它是一个ExecutionVertex的一次运行Attempt。</p>

<p>也就是说，一个ExecutionVertex可能对应多个运行状态的Execution。</p>

<p>比如，一个ExecutionVertex运行产生了一个失败的Execution，然后还会创建一个新的Execution来运行，这时就对应这个2次运行Attempt。</p>

<p>每个Execution通过ExecutionAttemptID来唯一标识，在TaskManager和JobManager之间进行Task状态的交换都是通过ExecutionAttemptID来实现的。</p>

<p>下面看一下，在物理上进行调度，基于资源的分配与使用的一个例子，来自官网，如下图所示：</p>

<p><img src="/images/blog/24-flink-scheduled-task-slots.png"></p>

<p>说明如下：</p>

<ul>
<li>左上子图：有2个TaskManager，每个TaskManager有3个Task Slot</li>
<li>左下子图：一个Flink Job，逻辑上包含了1个data source、1个MapFunction、1个ReduceFunction，对应一个JobGraph</li>
<li>左下子图：用户提交的Flink Job对各个Operator进行的配置——data source的并行度设置为4，MapFunction的并行度也为4，ReduceFunction的并行度为3，在JobManager端对应于ExecutionGraph</li>
<li>右上子图：TaskManager 1上，有2个并行的ExecutionVertex组成的DAG图，它们各占用一个Task Slot</li>
<li>右下子图：TaskManager 2上，也有2个并行的ExecutionVertex组成的DAG图，它们也各占用一个Task Slot</li>
</ul>


<p>在2个TaskManager上运行的4个Execution是并行执行的</p>

<h3>迭代机制</h3>

<p>机器学习和图计算应用，都会使用到迭代计算。</p>

<p>Flink通过在迭代Operator中定义Step函数来实现迭代算法，这种迭代算法包括Iterate和Delta Iterate两种类型，在实现上它们反复地在当前迭代状态上调用Step函数，直到满足给定的条件才会停止迭代。</p>

<p>下面，对Iterate和Delta Iterate两种类型的迭代算法原理进行说明：</p>

<h4>Iterate</h4>

<p>Iterate Operator是一种简单的迭代形式：</p>

<ul>
<li>每一轮迭代，Step函数的输入或者是输入的整个数据集，或者是上一轮迭代的结果，通过该轮迭代计算出下一轮计算所需要的输入（也称为Next Partial Solution）</li>
<li>满足迭代的终止条件后，会输出最终迭代结果，具体执行流程如下图所示：</li>
</ul>


<p><img src="/images/blog/25-flink-iterations-iterate-operator.png"></p>

<p>Step函数在每一轮迭代中都会被执行，它可以是由map、reduce、join等Operator组成的数据流。</p>

<p>下面通过官网给出的一个例子来说明Iterate Operator，非常简单直观，如下图所示：</p>

<p><img src="/images/blog/26-flink-iterations-iterate-operator-example.png"></p>

<p>上面迭代过程中，输入数据为1到5的数字，Step函数就是一个简单的map函数，会对每个输入的数字进行加1处理，而Next Partial Solution对应于经过map函数处理后的结果。</p>

<p>比如第一轮迭代，对输入的数字1加1后结果为2，对输入的数字2加1后结果为3，直到对输入数字5加1后结果为变为6，这些新生成结果数字2~6会作为第二轮迭代的输入。</p>

<p>迭代终止条件为进行10轮迭代，则最终的结果为11~15。</p>

<h4>Delta Iterate</h4>

<p>Delta Iterate Operator实现了增量迭代，它的实现原理如下图所示：</p>

<p><img src="/images/blog/27-flink-iterations-delta-iterate-operator.png"></p>

<p>基于Delta Iterate Operator实现增量迭代，它有2个输入：</p>

<ul>
<li>其中一个是初始Workset，表示输入待处理的增量Stream数据</li>
<li>另一个是初始Solution Set，它是经过Stream方向上Operator处理过的结果。</li>
</ul>


<p>第一轮迭代会将Step函数作用在初始Workset上，得到的计算结果Workset作为下一轮迭代的输入，同时还要增量更新初始Solution Set。</p>

<p>如果反复迭代知道满足迭代终止条件，最后会根据Solution Set的结果，输出最终迭代结果。</p>

<p>比如，我们现在已知一个Solution集合中保存的是，已有的商品分类大类中购买量最多的商品。</p>

<p>而Workset输入的是来自线上实时交易中最新达成购买的商品的人数，经过计算会生成新的商品分类大类中商品购买量最多的结果。</p>

<p>如果某些大类中商品购买量突然增长，它需要更新Solution Set中的结果（原来购买量最多的商品，经过增量迭代计算，可能已经不是最多），最后会输出最终商品分类大类中购买量最多的商品结果集合。</p>

<p>更详细的例子，可以参考官网给出的“Propagate Minimum in Graph”，这里不再累述。</p>

<h3>Backpressure监控机制</h3>

<p>Backpressure在流式计算系统中会比较受到关注。</p>

<p>因为在一个Stream上进行处理的多个Operator之间，它们处理速度和方式可能非常不同，所以就存在上游Operator如果处理速度过快，下游Operator处可能机会堆积Stream记录，严重会造成处理延迟或下游Operator负载过重而崩溃（有些系统可能会丢失数据）。</p>

<p>因此，对下游Operator处理速度跟不上的情况，如果下游Operator能够将自己处理状态传播给上游Operator，使得上游Operator处理速度慢下来就会缓解上述问题，比如通过告警的方式通知现有流处理系统存在的问题。</p>

<p>Flink Web界面上提供了对运行Job的Backpressure行为的监控，它通过使用Sampling线程对正在运行的Task进行堆栈跟踪采样来实现，具体实现方式如下图所示：</p>

<p><img src="/images/blog/28-flink-back-pressure-sampling.png"></p>

<p>JobManager会反复调用一个Job的Task运行所在线程的Thread.getStackTrace()。</p>

<p>默认情况下，JobManager会每间隔50ms触发对一个Job的每个Task依次进行100次堆栈跟踪调用，根据调用调用结果来确定Backpressure，Flink是通过计算得到一个比值（Radio）来确定当前运行Job的Backpressure状态。</p>

<p>在Web界面上可以看到这个Radio值，它表示在一个内部方法调用中阻塞（Stuck）的堆栈跟踪次数，例如，radio=0.01，表示100次中仅有1次方法调用阻塞。</p>

<p>Flink目前定义了如下Backpressure状态：</p>

<ul>
<li>OK: 0 &lt;= Ratio &lt;= 0.10</li>
<li>LOW: 0.10 &lt; Ratio &lt;= 0.5</li>
<li>HIGH: 0.5 &lt; Ratio &lt;= 1</li>
</ul>


<p>另外，Flink还提供了3个参数来配置Backpressure监控行为：</p>

<table>
<thead>
<tr>
<th> 参数名称 </th>
<th style="text-align:left;"> 默认值 </th>
<th style="text-align:left;"> 说明 </th>
</tr>
</thead>
<tbody>
<tr>
<td> jobmanager.web.backpressure.refresh-interval </td>
<td style="text-align:left;"> 60000 </td>
<td style="text-align:left;"> 默认1分钟，表示采样统计结果刷新时间间隔 </td>
</tr>
<tr>
<td> jobmanager.web.backpressure.num-samples </td>
<td style="text-align:left;"> 100 </td>
<td style="text-align:left;"> 评估Backpressure状态，所使用的堆栈跟踪调用次数 </td>
</tr>
<tr>
<td> jobmanager.web.backpressure.delay-between-samples </td>
<td style="text-align:left;"> 50 </td>
<td style="text-align:left;"> 默认50毫秒，表示对一个Job的每个Task依次调用的时间间隔 </td>
</tr>
</tbody>
</table>


<p>通过上面个定义的Backpressure状态，以及调整相应的参数，可以确定当前运行的Job的状态是否正常，并且保证不影响JobManager提供服务。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[搭建Ray集群步骤]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/02/08/da-jian-rayji-qun-bu-zou/"/>
    <updated>2018-02-08T10:29:16+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/02/08/da-jian-rayji-qun-bu-zou</id>
    <content type="html"><![CDATA[<p>本文介绍如何搭建Ray 0.3集群环境。</p>

<p>可参考官方文档：</p>

<ul>
<li><a href="https://ray.readthedocs.io/en/latest/using-ray-on-a-cluster.html">https://ray.readthedocs.io/en/latest/using-ray-on-a-cluster.html</a></li>
</ul>


<!-- More -->


<h2>安装Ray</h2>

<p>首先在每台机器上安装如下组件。</p>

<h3>安装Anaconda</h3>

<p>首先安装Anaconda，下载：</p>

<ul>
<li>Anaconda2-4.3.0-Linux-x86_64.sh</li>
</ul>


<p>按提示执行安装即可。</p>

<h3>安装Ray依赖</h3>

<p>ray依赖如下库：</p>

<ul>
<li>six (>=1.0.0)</li>
<li>redis</li>
<li>pytest</li>
<li>psutil</li>
<li>numpy</li>
<li>funcsigs</li>
<li>flatbuffers</li>
<li>colorama</li>
<li>cloudpickle (==0.5.2)</li>
<li>click</li>
</ul>


<p>注意：</p>

<ul>
<li>如果机器环境通pip源则直接pip install即可。</li>
<li>如果不通可以在 <a href="https://pypi.python.org/pypi/ray/0.3.0">https://pypi.python.org/pypi/ray/0.3.0</a> 下载.whl包后上传到机器pip本地安装。</li>
</ul>


<h3>安装Ray 0.3</h3>

<p>如果环境通pip源</p>

<pre><code class="bash">$ pip install ray
</code></pre>

<p>如果不通则在 <a href="https://pypi.python.org/pypi/ray/0.3.0">https://pypi.python.org/pypi/ray/0.3.0</a> 下载 ：</p>

<ul>
<li>ray-0.3.0-cp27-cp27mu-manylinux1_x86_64.whl</li>
</ul>


<p>然后执行：</p>

<pre><code class="bash">$ pip install ray-0.3.0-cp27-cp27mu-manylinux1_x86_64.whl
</code></pre>

<h2>搭建集群</h2>

<p>假设集群IP如下：</p>

<pre><code>192.168.0.1
192.168.0.2
192.168.0.3
192.168.0.4
192.168.0.5
192.168.0.6
192.168.0.7
192.168.0.8
192.168.0.9
192.168.0.10
</code></pre>

<p>搭建集群环境如下：</p>

<h3>启动Head节点</h3>

<p>选一个节点作为Head节点，例如IP为：</p>

<pre><code>192.168.0.1
</code></pre>

<p>在head节点执行：</p>

<pre><code class="bash">ray start --head --node-ip-address 192.168.0.1 --redis-port=6379
</code></pre>

<p>执行后会启动Head节点相关的服务。</p>

<h3>启动Worker节点</h3>

<p>Worker节点IP为：</p>

<pre><code class="bash">192.168.0.2
192.168.0.3
192.168.0.4
192.168.0.5
192.168.0.6
192.168.0.7
192.168.0.8
192.168.0.9
192.168.0.10
</code></pre>

<p>在每台Worker节点上执行：</p>

<pre><code class="bash">ray start --redis-address :6379 192.168.0.x --num-cpus 10
</code></pre>

<p>执行后会启动Worker节点相关服务，其中：</p>

<ul>
<li>192.168.0.x 为对应节点IP</li>
<li>num-cpu选项可以用于设置每台节点可用的cpu数，默认为机器总的cpu数。</li>
</ul>


<h3>停止集群</h3>

<p>Head节点与Worker节点服务的停止命令相同，执行：</p>

<pre><code>ray stop
</code></pre>

<h2>连接集群</h2>

<p>使用如下方法建立连接：</p>

<pre><code class="python">import ray

ray.init(redis_address="192.168.0.1:6379")
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Fuse挂载HDFS到本地目录方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/11/14/gua-zai-hdfsdao-ben-di-mu-lu-fang-fa/"/>
    <updated>2017-11-14T00:10:27+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/11/14/gua-zai-hdfsdao-ben-di-mu-lu-fang-fa</id>
    <content type="html"><![CDATA[<p>网上关于挂载HDFS到本地的介绍大多基于较早版本的Hadoop。
本文以Hadoop-2.8.0为例，介绍通过Fuse挂载HDFS到本地的方法。</p>

<!--more-->


<h2>安装Fuse</h2>

<p>对每台节点，执行如下命令一键安装</p>

<pre><code class="bash">sudo yum -y install fuse fuse-libs
</code></pre>

<h2>编译fuse-dfs工具</h2>

<p>下载hadoop-2.8.0源码，解压编译</p>

<pre><code class="bash">tar zxvf hadoop-2.8.0.tar.gz
cd hadoop-2.8.0
mvn package -Drequire.fuse=true -DskipTests -Dmaven.javadoc.skip=true -Dtar
</code></pre>

<p>编译后会生成fuse_dfs的可执行文件，位于</p>

<blockquote><p>./hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs/fuse_dfs</p></blockquote>

<p>另外会生成一个对该可执行程序的封装脚本，位于</p>

<blockquote><p>./hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh</p></blockquote>

<h2>配置环境变量</h2>

<p>可以为fuse_dfs_wrapper.sh建立软链接到当前目录方便后续使用。</p>

<pre><code class="bash">ln -s /&lt;Hadoop源码路径&gt;/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_dfs_wrapper.sh .
</code></pre>

<p>编辑fuse_dfs_wrapper.sh内容，有一些需要根据具体情况修改：</p>

<pre><code class="bash">HADOOP_HOME=/path/to/your/hadoop
HADOOP_PREFIX=/path/to/your/hadoop/src/

export FUSEDFS_PATH="$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs-native-client/target/main/native/fuse-dfs"
export LIBHDFS_PATH="$HADOOP_PREFIX/hadoop-hdfs-project/hadoop-hdfs-native-client/target/native/target/usr/local/lib"
HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

if [ "$OS_ARCH" = "" ]; then
  export OS_ARCH=amd64
fi

# 这里需要替换为JDK的安装路径
JAVA_HOME=/home/yiguang.wyg/tools/jdk1.8.0_121

if [ "$LD_LIBRARY_PATH" = "" ]; then
  export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:/usr/local/lib
fi

JARS=`find "$HADOOP_PREFIX/hadoop-hdfs-project" -name "*.jar" | xargs`
for jar in $JARS; do
  CLASSPATH=$jar:$CLASSPATH
done

JARS=`find "$HADOOP_PREFIX/hadoop-client" -name "*.jar" | xargs`
for jar in $JARS; do
  CLASSPATH=$jar:$CLASSPATH
done

export CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATH
export PATH=$FUSEDFS_PATH:$PATH
export LD_LIBRARY_PATH=$LIBHDFS_PATH:$JAVA_HOME/jre/lib/$OS_ARCH/server:$LD_LIBRARY_PATH

fuse_dfs "$@"
</code></pre>

<p>重点需要配置好HADOOP_HOME和HADOOP_PREFIX，分别为hadoop安装路径和hadoop源码路径。</p>

<h2>挂载HDFS</h2>

<p>挂载HDFS之前需要确保HDFS已经启动。</p>

<p>创建挂载目录</p>

<pre><code class="bash">sudo mkdir /mnt/hdfs
</code></pre>

<p>执行：</p>

<pre><code class="bash">sudo sh fuse_dfs_wrapper.sh hdfs://&lt;hdfs路径&gt; /mnt/hdfs
</code></pre>

<p>输出：</p>

<blockquote><p>INFO /&hellip;/hadoop-2.8.0-src/hadoop-hdfs-project/hadoop-hdfs-native-client/src/main/native/fuse-dfs/fuse_options.c:164 Adding FUSE arg /mnt/hdfs</p></blockquote>

<p>进入挂载目录，如果能访问到HDFS中的内容，说明挂载成功。
<code>
cd /mnt/hdfs
ls
</code></p>

<p>挂载成功后，就可以将HDFS当做本地路径使用了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ZooKeeper安装使用指南]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/01/20/zookeeperan-zhuang-shi-yong-zhi-nan/"/>
    <updated>2017-01-20T15:56:22+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/01/20/zookeeperan-zhuang-shi-yong-zhi-nan</id>
    <content type="html"><![CDATA[<p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务。</p>

<p>作为分布式应用提供一致性服务的软件，ZooKeeper 封装了易错的关键服务，提供简单高效、功能稳定接口给用户</p>

<p>本文介绍 ZooKeeper 的配置方法和客户端使用方法。</p>

<!--more-->


<h2>ZooKeeper 安装</h2>

<p>以ZooKeeper 3.4.8为例，下载 <a href="http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.4.8/">ZooKeeper 3.4.8</a>
下载解压后配置conf/zoo.cfg，配置clientPort，dataDir等。
示例配置：</p>

<pre><code class="bash"># The number of milliseconds of each tick
tickTime=2000
# The number of ticks that the initial synchronization phase can take
initLimit=10
# The number of ticks that can pass between sending a request and getting an acknowledgement
syncLimit=5
# the directory where the snapshot is stored. do not use /tmp for storage, /tmp here is just example sakes.
dataDir=/tmp/zookeeper
# the port at which the clients will connect
clientPort=2181
</code></pre>

<h2>ZooKeeper 使用</h2>

<p>配置好Zk后需要先启动ZkServer，然后可以用Zk Client直接以命令行的方式操作Zk。</p>

<h3>Server端</h3>

<p>配置好后启动zk：</p>

<pre><code class="bash">$sh bin/zkServer.sh start &gt; zookeeper.out
</code></pre>

<h3>Client端</h3>

<p>ZooKeeper客户端的使用非常简单，启动：</p>

<pre><code class="bash"># ip和端口根据启动情况修改
$sh bin/zkCli.sh -server 127.0.0.1:2181
</code></pre>

<p>之后可以用ls、delete、get等命令查询或修改各ZK节点的值。命令帮助如下：</p>

<pre><code>ZooKeeper -server host:port cmd args
    connect host:port
    get path [watch]
    ls path [watch]
    set path data [version]
    rmr path
    delquota [-n|-b] path
    quit
    printwatches on|off
    create [-s] [-e] path data acl
    stat path [watch]
    close
    ls2 path [watch]
    history
    listquota path
    setAcl path acl
    getAcl path
    sync path
    redo cmdno
    addauth scheme auth
    delete path [version]
    setquota -n|-b val path
</code></pre>

<h2>ZooKeeper API</h2>

<p>除了通过客户端操作ZooKeeper，还可以调用ZooKeeper提供的API操作ZooKeeper的节点。</p>

<p>这里以ZooKeeper 3.4.5为例，介绍常用的几个Java API。</p>

<h3>建立连接</h3>

<p>在应用程序中使用Zk需要先创建ZooKeeper对象，后续的操作都是基于该对象进行的。</p>

<pre><code class="java">public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher) throws IOException  
</code></pre>

<p>参数说明：</p>

<ul>
<li>connectString： zookeeper server列表, 以逗号隔开。ZooKeeper对象初始化后, 将从列表中选择一个server, 并尝试建立连接。如果失败,则会从剩余项中选择并再次尝试建立连接。</li>
<li>sessionTimeout：指定连接的超时时间.</li>
<li>watcher： 事件回调接口。</li>
</ul>


<h3>创建/删除znode</h3>

<p>ZooKeeper对象的create/delete方法用于创建/删除 znode。如果该node存在, 则返回该node的状态信息, 否则返回null。</p>

<pre><code class="java">public String create(String path, byte[] data, List acl, CreateMode createMode); 
public void delete(final String path, int version);  
</code></pre>

<p>参数说明：</p>

<ul>
<li>path： znode的路径。</li>
<li>data：与znode关联的数据。</li>
<li>acl：指定权限信息</li>
<li>createMode：指定znode类型，按持久化节点与临时节点，以及自动编号节点与非自动编号节点两个维度划分，共4类。</li>
<li>version：指定要更新的数据的版本, 如果version和真实的版本不同, 更新操作将失败.。指定version为-1则忽略版本检查。</li>
</ul>


<h3>获取子znode列表</h3>

<p>ZooKeeper对象的getChildren方法用于获取子node列表。</p>

<pre><code class="java">public List getChildren(String path, boolean watch); 
</code></pre>

<p>参数说明：</p>

<ul>
<li>path： znode的路径。</li>
<li>watch参数用于指定是否监听path node的创建, 删除事件, 以及数据更新事件。</li>
</ul>


<h3>判断znode是否存在</h3>

<p>ZooKeeper对象的exists方法用于判断指定znode是否存在。如果该node存在, 则返回该node的状态信息, 否则返回null。</p>

<pre><code class="java">public Stat exists(String path, boolean watch);  
</code></pre>

<p>参数说明：</p>

<ul>
<li>path： znode的路径。</li>
<li>watch：用于指定是否监听path node的创建, 删除事件, 以及数据更新事件。</li>
</ul>


<h3>获取/更新znode数据</h3>

<p>ZooKeeper对象的getData/setData方法用于获取/更新 znode关联的数据。</p>

<pre><code class="java">public byte[] getData(String path, boolean watch, Stat stat);  
public Stat setData(final String path, byte data[], int version); 
</code></pre>

<p>参数说明：</p>

<ul>
<li>path： znode的路径。</li>
<li>stat：传出参数, getData方法会将path node的状态信息设置到该参数中。</li>
<li>data：与znode关联的数据。</li>
<li>watch：用于指定是否监听path node的创建, 删除事件, 以及数据更新事件。</li>
<li>version：指定要更新的数据的版本, 如果version和真实的版本不同, 更新操作将失败.。指定version为-1则忽略版本检查。</li>
</ul>


<p>更全的API介绍参考 ZooKeeper 3.4.5 API</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Core 消息队列机制]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/11/28/spark-core-message-queue-mechanism/"/>
    <updated>2016-11-28T14:31:03+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/11/28/spark-core-message-queue-mechanism</id>
    <content type="html"><![CDATA[<p>本文介绍Spark中的消息队列机制，首先SparkListenerEvent，SparkListener和SparkListenerBus等基本数据结构实现。</p>

<p>重点介绍了异步消息总线LiveListenerBus的实现。随后介绍了Spark消息队列的整体工作流程。</p>

<!--more-->


<h2>SparkListenerEvent</h2>

<p>Spark中的消息由SparkListenerEvent表示。其本身定义只是一个接口：</p>

<pre><code class="scala">trait SparkListenerEvent {
  /* Whether output this event to the event log */
  protected[spark] def logEvent: Boolean = true
}
</code></pre>

<p>SparkListenerEvent有多个具体的实现，每种实现代表了Spark运行过程中的一种事件。</p>

<ul>
<li>SparkListenerStageSubmitted</li>
<li>SparkListenerStageCompleted</li>
<li>SparkListenerTaskStart</li>
<li>SparkListenerTaskGettingResult</li>
<li>SparkListenerTaskEnd</li>
<li>SparkListenerJobStart</li>
<li>SparkListenerJobEnd</li>
<li>SparkListenerEnvironmentUpdate</li>
<li>SparkListenerBlockManagerAdded</li>
<li>SparkListenerBlockManagerRemoved</li>
<li>SparkListenerUnpersistRDD</li>
<li>SparkListenerExecutorAdded</li>
<li>SparkListenerExecutorRemoved</li>
<li>SparkListenerBlockUpdated</li>
<li>SparkListenerExecutorMetricsUpdate</li>
<li>SparkListenerApplicationStart</li>
<li>SparkListenerApplicationEnd</li>
<li>SparkListenerLogStart</li>
</ul>


<p>根据名称可以知道每一种事件代表的含义。</p>

<h2>SparkListener</h2>

<p>SparkListeners负责监听SparkListenerEvents。</p>

<p>所有Spark消息SparkListenerEvents 被异步的发送给已经注册过的SparkListeners。</p>

<p>SparkListenerInterface定义了SparkListener的接口：</p>

<ul>
<li>onStageCompleted</li>
<li>onStageSubmitted</li>
<li>onTaskStart</li>
<li>onTaskGettingResult</li>
<li>onTaskEnd</li>
<li>onJobStart</li>
<li>onJobEnd</li>
<li>onEnvironmentUpdate</li>
<li>onBlockManagerAdded</li>
<li>onBlockManagerRemoved</li>
<li>onUnpersistRDD</li>
<li>onApplicationStart</li>
<li>onApplicationEnd</li>
<li>onExecutorMetricsUpdate</li>
<li>onExecutorAdded</li>
<li>onExecutorRemoved</li>
<li>onBlockUpdated</li>
<li>onOtherEvent</li>
</ul>


<p>根据名称可以知道每一个方法是对应事件消息的响应函数。SparkListener的实现：</p>

<pre><code class="scala">abstract class SparkListener extends SparkListenerInterface {
  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = { }
  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = { }
  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = { }
  override def onTaskGettingResult(taskGettingResult: SparkListenerTaskGettingResult): Unit = { }
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = { }
  override def onJobStart(jobStart: SparkListenerJobStart): Unit = { }
  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = { }
  override def onEnvironmentUpdate(environmentUpdate: SparkListenerEnvironmentUpdate): Unit = { }
  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit = { }
  override def onBlockManagerRemoved(
      blockManagerRemoved: SparkListenerBlockManagerRemoved): Unit = { }
  override def onUnpersistRDD(unpersistRDD: SparkListenerUnpersistRDD): Unit = { }
  override def onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit = { }
  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = { }
  override def onExecutorMetricsUpdate(
      executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = { }
  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = { }
  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = { }
  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = { }
  override def onOtherEvent(event: SparkListenerEvent): Unit = { }
}
</code></pre>

<p>Spark运行过程中会用到很多个SparkListener，每一种都有自己的用途。</p>

<p>例如EventLoggingListener用来将监听到的事件持久化到文件中，ExecutorAllocationListener用那个通知对应的ExecutorAllocationManager增加或移除executor等。</p>

<h2>SparkListenerBus</h2>

<p>SparkListener需要被注册到SparkListenerBus中才能起作用，SparkListenerBus负责分发监听到的Event给SparkListener。</p>

<p>SparkListenerBus继承自ListenerBus接口，并重载了doPostEvent方法。</p>

<pre><code class="scala">private[spark] trait SparkListenerBus
  extends ListenerBus[SparkListenerInterface, SparkListenerEvent] {

  protected override def doPostEvent(
      listener: SparkListenerInterface,
      event: SparkListenerEvent): Unit = {
    event match {
      case stageSubmitted: SparkListenerStageSubmitted =&gt;
        listener.onStageSubmitted(stageSubmitted)
      case stageCompleted: SparkListenerStageCompleted =&gt;
        listener.onStageCompleted(stageCompleted)
      case jobStart: SparkListenerJobStart =&gt;
        listener.onJobStart(jobStart)
      case jobEnd: SparkListenerJobEnd =&gt;
        listener.onJobEnd(jobEnd)
      ...
      case blockUpdated: SparkListenerBlockUpdated =&gt;
        listener.onBlockUpdated(blockUpdated)
      case logStart: SparkListenerLogStart =&gt; // ignore event log metadata
      case _ =&gt; listener.onOtherEvent(event)
    }
  }

}
</code></pre>

<p>该接口实现了消息的路由，根据事件类型调用相应的处理函数。</p>

<h3>ListenerBus</h3>

<p>ListenerBus接口的实现如下：</p>

<pre><code>private[spark] trait ListenerBus[L &lt;: AnyRef, E] extends Logging {

  // Marked `private[spark]` for access in tests.
  private[spark] val listeners = new CopyOnWriteArrayList[L]

  final def addListener(listener: L): Unit = {
    listeners.add(listener)
  }

  final def removeListener(listener: L): Unit = {
    listeners.remove(listener)
  }

  /**
   * Post the event to all registered listeners. The `postToAll` caller should guarantee calling
   * `postToAll` in the same thread for all events.
   */
  final def postToAll(event: E): Unit = {
    // JavaConverters can create a JIterableWrapper if we use asScala.
    // However, this method will be called frequently. To avoid the wrapper cost, here we use
    // Java Iterator directly.
    val iter = listeners.iterator
    while (iter.hasNext) {
      val listener = iter.next()
      try {
        doPostEvent(listener, event)
      } catch {
        case NonFatal(e) =&gt;
          logError(s"Listener ${Utils.getFormattedClassName(listener)} threw an exception", e)
      }
    }
  }

  /**
   * Post an event to the specified listener. `onPostEvent` is guaranteed to be called in the same
   * thread for all listeners.
   */
  protected def doPostEvent(listener: L, event: E): Unit

  private[spark] def findListenersByClass[T &lt;: L : ClassTag](): Seq[T] = {
    val c = implicitly[ClassTag[T]].runtimeClass
    listeners.asScala.filter(_.getClass == c).map(_.asInstanceOf[T]).toSeq
  }

}
</code></pre>

<p>本质上所有注册的Listener用一个数组记录下来，post操作就是根据事件找到对应的listener然后把event交给listener处理。</p>

<h3>LiveListenerBus</h3>

<p>SparkContext中会创建一个LiveListenerBus实例，LiveListenerBus是SparkListenerBus的一个具体实现，主要功能如下:</p>

<ul>
<li>保存有消息队列,负责消息的缓存</li>
<li>保存有注册过的listener,负责消息的分发</li>
</ul>


<p>消息队列用LinkBlockQueue实现：</p>

<pre><code class="scala">// Cap the capacity of the event queue so we get an explicit error (rather than
// an OOM exception) if it's perpetually being added to more quickly than it's being drained.
private lazy val EVENT_QUEUE_CAPACITY = validateAndGetQueueSize()
private lazy val eventQueue = new LinkedBlockingQueue[SparkListenerEvent](EVENT_QUEUE_CAPACITY)
</code></pre>

<p>事件队列的长度EVENT_QUEUE_CAPACITY由spark.scheduler.listenerbus.eventqueue.size参数配置，默认为10000。</p>

<p>当缓存事件数量达到上限后,新来的事件会被丢弃。</p>

<p>消息的产生和分发按照 <b><font color=red>生产者-消费者模型</font></b> 实现。</p>

<p><b><font color=red>消息的分发(消费者)</font></b> 是通过一个listener线程异步处理的，代码如下。</p>

<pre><code class="scala">private val listenerThread = new Thread(name) {  // &lt;-- 线程名为SparkListenerBus
  setDaemon(true)
  override def run(): Unit = Utils.tryOrStopSparkContext(sparkContext) {
    LiveListenerBus.withinListenerThread.withValue(true) {
      while (true) {
        eventLock.acquire()
        self.synchronized {
          processingEvent = true
        }
        try {
          val event = eventQueue.poll
          if (event == null) {
            // Get out of the while loop and shutdown the daemon thread
            if (!stopped.get) {
              throw new IllegalStateException("Polling `null` from eventQueue means" +
                " the listener bus has been stopped. So `stopped` must be true")
            }
            return
          }
          postToAll(event)
        } finally {
          self.synchronized {
            processingEvent = false
          }
        }
      }
    }
  }
}
</code></pre>

<p>为了保证生产者和消费者对消息队列的并发访问，在每次需要获取消息的时候,调用eventLock.acquire()来获取信号量, 信号量的值就是当前队列中所含有的事件数量。</p>

<p>如果正常获取到事件,就调用postToAll将事件分发给所有listener, 继续下一次循环。</p>

<p>如果获取到null值, 则有下面两种情况:</p>

<ul>
<li>整个application正常结束, 此时stopped值已经被设置为true。</li>
<li>系统发生了错误, 立即终止运行。</li>
</ul>


<p><font color=red><b>消息的产生(生产者)</font></b> 通过在Spark运行时调用LiveListenerBus的post方法来添加。实现如下：</p>

<pre><code class="scala">def post(event: SparkListenerEvent): Unit = {
  if (stopped.get) {
    // Drop further events to make `listenerThread` exit ASAP
    logError(s"$name has already stopped! Dropping event $event")
    return
  }
  val eventAdded = eventQueue.offer(event)  // &lt;-- 这里将新来的事件添加到消息队列中
  if (eventAdded) {
    eventLock.release()
  } else {
    onDropEvent(event)  // &lt;-- 没有添加成功，则丢弃事件
    droppedEventsCounter.incrementAndGet()
  }

  val droppedEvents = droppedEventsCounter.get
  if (droppedEvents &gt; 0) {
    // Don't log too frequently
    if (System.currentTimeMillis() - lastReportTimestamp &gt;= 60 * 1000) {
      // There may be multiple threads trying to decrease droppedEventsCounter.
      // Use "compareAndSet" to make sure only one thread can win.
      // And if another thread is increasing droppedEventsCounter, "compareAndSet" will fail and
      // then that thread will update it.
      if (droppedEventsCounter.compareAndSet(droppedEvents, 0)) {
        val prevLastReportTimestamp = lastReportTimestamp
        lastReportTimestamp = System.currentTimeMillis()
        logWarning(s"Dropped $droppedEvents SparkListenerEvents since " +
          new java.util.Date(prevLastReportTimestamp))
      }
    }
  }
}
</code></pre>

<p>每成功放入一个事件,就调用eventLock.release()来增加信号量额值，以供消费者线程来进行消费. 如果队列满了,就调用onDropEvent来处理。</p>

<h2>消息队列建立/发送流程</h2>

<p>在SparkContext中创建了LiveListenerBus类类型的成员变量listenerBus。</p>

<pre><code class="scala">// An asynchronous listener bus for Spark events
private[spark] val listenerBus = new LiveListenerBus(this)
随后创建各种listener，并注册到listenerBus中，通过调用listenerBus的start()方法启动消息分发流程。
private def setupAndStartListenerBus(): Unit = {
  // Use reflection to instantiate listeners specified via `spark.extraListeners`
  try {
    val listenerClassNames: Seq[String] =
      conf.get("spark.extraListeners", "").split(',').map(_.trim).filter(_ != "")  
    for (className &lt;- listenerClassNames) {  // &lt;-- 如果指定了额外的SparkListenr类，可通过反射机制实例化并注册到listenerBus
      // Use reflection to find the right constructor
      val constructors = {
        val listenerClass = Utils.classForName(className)
        listenerClass
            .getConstructors
            .asInstanceOf[Array[Constructor[_ &lt;: SparkListenerInterface]]]
      }
      val constructorTakingSparkConf = constructors.find { c =&gt;
        c.getParameterTypes.sameElements(Array(classOf[SparkConf]))
      }
      lazy val zeroArgumentConstructor = constructors.find { c =&gt;
        c.getParameterTypes.isEmpty
      }
      val listener: SparkListenerInterface = {
        if (constructorTakingSparkConf.isDefined) {
          constructorTakingSparkConf.get.newInstance(conf)
        } else if (zeroArgumentConstructor.isDefined) {
          zeroArgumentConstructor.get.newInstance()
        } else {
          ...
        }
      }
      listenerBus.addListener(listener)
      logInfo(s"Registered listener $className")
    }
  } catch {
    ...
  }

  listenerBus.start()
  _listenerBusStarted = true
}
</code></pre>

<p>其中，listenerBus.start() 实现如下：</p>

<pre><code class="scala">def start(): Unit = {
  if (started.compareAndSet(false, true)) {
    listenerThread.start()
  } else {
    throw new IllegalStateException(s"$name already started!")
  }
}
</code></pre>

<p>运行过程中产生的事件会post到listenerBus中。</p>

<p>当作业运行结束后会调用listenerBus.stop()来停止SparkListenerBus线程。</p>

<pre><code class="scala">def stop(): Unit = {
  if (!started.get()) {
    throw new IllegalStateException(s"Attempted to stop $name that has not yet started!")
  }
  if (stopped.compareAndSet(false, true)) {
    // Call eventLock.release() so that listenerThread will poll `null` from `eventQueue` and know
    // `stop` is called.
    eventLock.release()
    listenerThread.join()
  } else {
    // Keep quiet
  }
}
</code></pre>

<p>这里可以看到：</p>

<p><b><font color=red>在stop函数中调用了eventLock.release()来增加信号量的值. 然而并未向消息队列中加入新的消息。</p>

<p>这就导致在消费者线程listenerThread读取队列时会返回null值,进而达到结束listenerThread线程的目的。</font></b></p>

<p>以上就是Spark Core中消息队列机制的整体工作流程。</p>

<p><b>参考资料</b></p>

<ol>
<li>Spark 2.0 源码：<a href="https://github.com/apache/spark/tree/branch-2.0">https://github.com/apache/spark/tree/branch-2.0</a></li>
<li>Spark消息队列机制源码学习Blog：<a href="http://blog.csdn.net/sivolin/article/details/47316099">http://blog.csdn.net/sivolin/article/details/47316099</a></li>
</ol>

]]></content>
  </entry>
  
</feed>
