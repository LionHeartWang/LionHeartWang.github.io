<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Spark | Workspace of LionHeart]]></title>
  <link href="http://lionheartwang.github.io/blog/categories/spark/atom.xml" rel="self"/>
  <link href="http://lionheartwang.github.io/"/>
  <updated>2018-03-11T00:38:56+08:00</updated>
  <id>http://lionheartwang.github.io/</id>
  <author>
    <name><![CDATA[Wang Yiguang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Spark Shuffle Implementation]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/03/11/spark-shuffle-implementation/"/>
    <updated>2018-03-11T00:34:09+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/03/11/spark-shuffle-implementation</id>
    <content type="html"><![CDATA[<p>Spark中，数据通过从一个state流向下一个 stage 是通过shuffle过程完成的。</p>

<p>本文介绍Spark中的shuffle过程设计和工作原理。</p>

<!--More-->


<h2>MapReduce 和 Spark Shuffle 过程对比</h2>

<p>Hadoop MapReduce 中的 shuffle 过程，和Spark的shuffle过程有一些区别和联系。</p>

<p>从 high-level 的角度来看，两者都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer。（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。</p>

<p>Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。</p>

<p>从 low-level 的角度来看，两者差别不小。</p>

<p>Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。</p>

<p>这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。</p>

<p>1.2之前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。</p>

<p>如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。</p>

<p>从Spark 1.2起，sort作为默认的Shuffle实现。</p>

<p>从实现角度来看，两者也有不少差别。</p>

<p>Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。</p>

<p>每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。</p>

<p>在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。</p>

<p>如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。</p>

<p>那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read 的处理逻辑？以及两个处理逻辑应该怎么高效实现？</p>

<h2>Shuffle Write</h2>

<p>由于不要求数据有序，shuffle write 的任务很简单：</p>

<ul>
<li>将数据 partition 好，并持久化。</li>
<li>之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。</li>
</ul>


<p>shuffle write 的任务很简单，那么实现也很简单：</p>

<ul>
<li>将 shuffle write 的处理逻辑加入到 ShuffleMapStage（ShuffleMapTask 所在的 stage） 的最后，该 stage 的 final RDD 每输出一个 record 就将其 partition 并持久化。</li>
</ul>


<p>图示如下：</p>

<p><img src="/images/blog/29-shuffle-write-no-consolidation" title=".png" ></p>

<p>上图有 4 个 ShuffleMapTask 要在同一个 worker node 上运行，CPU core 数为 2，可以同时运行两个 task。</p>

<p>每个 task 的执行结果（该 stage 的 finalRDD 中某个 partition 包含的 records）被逐一写到本地磁盘上。</p>

<p>每个 task 包含 R 个缓冲区，R = reducer 个数（也就是下一个 stage 中 task 的个数）。</p>

<p>缓冲区被称为 bucket，其大小为spark.shuffle.file.buffer.kb ，默认是 32KB（Spark 1.1 版本以前是 100KB）。</p>

<p>其实 bucket 是一个广义的概念，代表 ShuffleMapTask 输出结果经过 partition 后要存放的地方，这里为了细化数据存放位置和数据名称，仅仅用 bucket 表示缓冲区。</p>

<p>ShuffleMapTask 的执行过程很简单：</p>

<ul>
<li>先利用 pipeline 计算得到 finalRDD 中对应 partition 的 records。每得到一个 record 就将其送到对应的 bucket 里，具体是哪个 bucket 由partitioner.partition(record.getKey()))决定。</li>
<li>每个 bucket 里面的数据会不断被写到本地磁盘上，形成一个 ShuffleBlockFile，或者简称 FileSegment。</li>
<li>之后的 reducer 会去 fetch 属于自己的 FileSegment，进入 shuffle read 阶段。</li>
</ul>


<p>这样的实现很简单，但有几个问题：</p>

<ul>
<li>产生的 FileSegment 过多。每个 ShuffleMapTask 产生 R（reducer 个数）个 FileSegment，M 个 ShuffleMapTask 就会产生 M * R 个文件。一般 Spark job 的 M 和 R 都很大，因此磁盘上会存在大量的数据文件。</li>
<li>缓冲区占用内存空间大。每个 ShuffleMapTask 需要开 R 个 bucket，M 个 ShuffleMapTask 就会产生 M R 个 bucket。虽然一个 ShuffleMapTask 结束后，对应的缓冲区可以被回收，但一个 worker node 上同时存在的 bucket 个数可以达到 cores R 个（一般 worker 同时可以运行 cores 个 ShuffleMapTask），占用的内存空间也就达到了cores * R * 32 KB。对于 8 核 1000 个 reducer 来说，占用内存就是 256MB。</li>
</ul>


<p>目前来看，第二个问题还没有好的方法解决，因为写磁盘终究是要开缓冲区的，缓冲区太小会影响 IO 速度。</p>

<p>但第一个问题有一些方法去解决，下面介绍已经在 Spark 里面实现的 FileConsolidation 方法。先上图：</p>

<p><img src="/images/blog/30-shuffle-write-consolidation.png"></p>

<p>可以明显看出，在一个 core 上连续执行的 ShuffleMapTasks 可以共用一个输出文件 ShuffleFile。</p>

<p>先执行完的 ShuffleMapTask 形成 ShuffleBlock i，后执行的 ShuffleMapTask 可以将输出数据直接追加到 ShuffleBlock i 后面，形成 ShuffleBlock i'，每个 ShuffleBlock 被称为 FileSegment。下一个 stage 的 reducer 只需要 fetch 整个 ShuffleFile 就行了。</p>

<p>这样，每个 worker 持有的文件数降为 cores * R。FileConsolidation 功能可以通过spark.shuffle.consolidateFiles=true来开启。</p>

<h2>Shuffle Read</h2>

<p>先看一张包含 ShuffleDependency 的物理执行图，来自 reduceByKey：</p>

<p><img src="/images/blog/31-reduceByKeyStage.png"></p>

<p>很自然地，要计算 ShuffleRDD 中的数据，必须先把 MapPartitionsRDD 中的数据 fetch 过来。那么问题就来了：</p>

<ul>
<li>在什么时候 fetch，parent stage 中的一个 ShuffleMapTask 执行完还是等全部 ShuffleMapTasks 执行完？</li>
<li>边 fetch 边处理还是一次性 fetch 完再处理？</li>
<li>fetch 来的数据存放到哪里？</li>
<li>怎么获得要 fetch 的数据的存放位置？</li>
</ul>


<p>依次解答如下。</p>

<h3>在什么时候 fetch？</h3>

<p>当 parent stage 的所有 ShuffleMapTasks 结束后再 fetch。</p>

<p>理论上讲一个 ShuffleMapTask 结束后就可以 fetch，但是为了迎合 stage 的概念（即一个 stage 如果其 parent stages 没有执行完，自己是不能被提交执行的），还是选择全部 ShuffleMapTasks 执行完再去 fetch。</p>

<p>因为 fetch 来的 FileSegments 要先在内存做缓冲，所以一次 fetch 的 FileSegments 总大小不能太大。Spark 规定这个缓冲界限不能超过 spark.reducer.maxMbInFlight，这里用 softBuffer 表示，默认大小为 48MB。</p>

<p>一个 softBuffer 里面一般包含多个 FileSegment，但如果某个 FileSegment 特别大的话，这一个就可以填满甚至超过 softBuffer 的界限。</p>

<h3>边 fetch 边处理还是一次性 fetch 完再处理？</h3>

<p>边 fetch 边处理。</p>

<p>本质上，MapReduce shuffle 阶段就是边 fetch 边使用 combine() 进行处理，只是 combine() 处理的是部分数据。</p>

<p>MapReduce 为了让进入 reduce() 的 records 有序，必须等到全部数据都 shuffle-sort 后再开始 reduce()。</p>

<p>因为 Spark 不要求 shuffle 后的数据全局有序，因此没必要等到全部数据 shuffle 完成后再处理。那么如何实现边 shuffle 边处理，而且流入的 records 是无序的？</p>

<p>答案是使用可以 aggregate 的数据结构，比如 HashMap。</p>

<p>每 shuffle 得到（从缓冲的 FileSegment 中 deserialize 出来）一个 record，直接将其放进 HashMap 里面。</p>

<p>如果该 HashMap 已经存在相应的 Key，那么直接进行 aggregate 也就是 func(hashMap.get(Key), Value)。</p>

<p>比如上面 WordCount 例子中的 func 就是 hashMap.get(Key) ＋ Value，并将 func 的结果重新 put(key) 到 HashMap 中去。</p>

<p>这个 func 功能上相当于 reduce()，但实际处理数据的方式与 MapReduce reduce() 有差别，差别相当于下面两段程序的差别。</p>

<pre><code class="java">  // MapReduce
  reduce(K key, Iterable&lt;V&gt; values) { 
      result = process(key, values)
      return result    
  }

  // Spark
  reduce(K key, Iterable&lt;V&gt; values) {
      result = null 
      for (V value : values) 
          result  = func(result, value)
      return result
  }
</code></pre>

<p>MapReduce 可以在 process 函数里面可以定义任何数据结构，也可以将部分或全部的 values 都 cache 后再进行处理，非常灵活。</p>

<p>而 Spark 中的 func 的输入参数是固定的，一个是上一个 record 的处理结果，另一个是当前读入的 record，它们经过 func 处理后的结果被下一个 record 处理时使用。</p>

<p>因此一些算法比如求平均数，在 process 里面很好实现，直接sum(values)/values.length，而在 Spark 中 func 可以实现sum(values)，但不好实现/values.length。更多的 func 将会在下面的章节细致分析。</p>

<h3>fetch 来的数据存放到哪里？</h3>

<p>刚 fetch 来的 FileSegment 存放在 softBuffer 缓冲区，经过处理后的数据放在内存 + 磁盘上。</p>

<p>这里我们主要讨论处理后的数据，可以灵活设置这些数据是“只用内存”还是“内存＋磁盘”。</p>

<p>如果spark.shuffle.spill = false就只用内存。</p>

<p>内存使用的是AppendOnlyMap ，类似 Java 的HashMap，内存＋磁盘使用的是ExternalAppendOnlyMap，如果内存空间不足时，ExternalAppendOnlyMap可以将 records 进行 sort 后 spill 到磁盘上，等到需要它们的时候再进行归并，后面会详解。</p>

<p>使用“内存＋磁盘”的一个主要问题就是如何在两者之间取得平衡？</p>

<p>在 Hadoop MapReduce 中，默认将 reducer 的 70% 的内存空间用于存放 shuffle 来的数据，等到这个空间利用率达到 66% 的时候就开始 merge-combine()-spill。</p>

<p>在 Spark 中，也适用同样的策略，一旦 ExternalAppendOnlyMap 达到一个阈值就开始 spill，具体细节下面会讨论。</p>

<h3>怎么获得要 fetch 的数据的存放位置？</h3>

<p>在上一章讨论物理执行图中的 stage 划分的时候，我们强调 “一个 ShuffleMapStage 形成后，会将该 stage 最后一个 final RDD 注册到 MapOutputTrackerMaster.registerShuffle(shuffleId, rdd.partitions.size)，这一步很重要，因为 shuffle 过程需要 MapOutputTrackerMaster 来指示 ShuffleMapTask 输出数据的位置”。</p>

<p>因此，reducer 在 shuffle 的时候是要去 driver 里面的 MapOutputTrackerMaster 询问 ShuffleMapTask 输出的数据位置的。</p>

<p>每个 ShuffleMapTask 完成时会将 FileSegment 的存储位置信息汇报给 MapOutputTrackerMaster。</p>

<p>至此，我们已经讨论了 shuffle write 和 shuffle read 设计的核心思想、算法及某些实现。接下来，我们深入一些细节来讨论。</p>

<h2>典型算子实现</h2>

<h3>reduceByKey(func)</h3>

<p>上面初步介绍了 reduceByKey() 是如何实现边 fetch 边 reduce() 的。需要注意的是虽然 Example(WordCount) 中给出了各个 RDD 的内容，但一个 partition 里面的 records 并不是同时存在的。比如在 ShuffledRDD 中，每 fetch 来一个 record 就立即进入了 func 进行处理。MapPartitionsRDD 中的数据是 func 在全部 records 上的处理结果。从 record 粒度上来看，reduce() 可以表示如下：</p>

<p><img src="/images/blog/32-reduceByKeyRecord.png"></p>

<p>可以看到，fetch 来的 records 被逐个 aggreagte 到 HashMap 中，等到所有 records 都进入 HashMap，就得到最后的处理结果。唯一要求是 func 必须是 commulative 的（参见上面的 Spark 的 reduce() 的代码）。</p>

<p>ShuffledRDD 到 MapPartitionsRDD 使用的是 mapPartitionsWithContext 操作。</p>

<p>为了减少数据传输量，MapReduce 可以在 map 端先进行 combine()，其实在 Spark 也可以实现，只需要将上图 ShuffledRDD => MapPartitionsRDD 的 mapPartitionsWithContext 在 ShuffleMapStage 中也进行一次即可，比如 reduceByKey 例子中 ParallelCollectionRDD => MapPartitionsRDD 完成的就是 map 端的 combine()。</p>

<p>对比 MapReduce 的 map()-reduce() 和 Spark 中的 reduceByKey()：</p>

<p>map 端的区别：map() 没有区别。对于 combine()，MapReduce 先 sort 再 combine()，Spark 直接在 HashMap 上进行 combine()。
reduce 端区别：MapReduce 的 shuffle 阶段先 fetch 数据，数据量到达一定规模后 combine()，再将剩余数据 merge-sort 后 reduce()，reduce() 非常灵活。Spark 边 fetch 边 reduce()（在 HashMap 上执行 func），因此要求 func 符合 commulative 的特性。
从内存利用上来对比：</p>

<p>map 端区别：MapReduce 需要开一个大型环形缓冲区来暂存和排序 map() 的部分输出结果，但 combine() 不需要额外空间（除非用户自己定义）。 Spark 需要 HashMap 内存数据结构来进行 combine()，同时输出 records 到磁盘上时也需要一个小的 buffer（bucket）。
reduce 端区别：MapReduce 需要一部分内存空间来存储 shuffle 过来的数据，combine() 和 reduce() 不需要额外空间，因为它们的输入数据分段有序，只需归并一下就可以得到。在 Spark 中，fetch 时需要 softBuffer，处理数据时如果只使用内存，那么需要 HashMap 来持有处理后的结果。如果使用内存＋磁盘，那么在 HashMap 存放一部分处理后的数据。</p>

<h3>groupByKey(numPartitions)</h3>

<p>ShuffleGroupByKey</p>

<p>与 reduceByKey() 流程一样，只是 func 变成 result = result ++ record.value，功能是将每个 key 对应的所有 values 链接在一起。result 来自 hashMap.get(record.key)，计算后的 result 会再次被 put 到 hashMap 中。与 reduceByKey() 的区别就是 groupByKey() 没有 map 端的 combine()。对于 groupByKey() 来说 map 端的 combine() 只是减少了重复 Key 占用的空间，如果 key 重复率不高，没必要 combine()，否则，最好能够 combine()。</p>

<p><img src="/images/blog/37-ShuffleGroupByKey.png"></p>

<h3>distinct(numPartitions)</h3>

<p>ShuffleDistinct</p>

<p>与 reduceByKey() 流程一样，只是 func 变成 result = result == null? record.value : result，如果 HashMap 中没有该 record 就将其放入，否则舍弃。与 reduceByKey() 相同，在map 端存在 combine()。</p>

<p><img src="/images/blog/33-ShuffleDistinct.png"></p>

<h3>cogroup(otherRDD, numPartitions)</h3>

<p>ShuffleCoGroup</p>

<p>CoGroupedRDD 可能有 0 个、1 个或者多个 ShuffleDependency。但并不是要为每一个 ShuffleDependency 建立一个 HashMap，而是所有的 Dependency 共用一个 HashMap。与 reduceByKey() 不同的是，HashMap 在 CoGroupedRDD 的 compute() 中建立，而不是在 mapPartitionsWithContext() 中建立。</p>

<p>粗线表示的 task 首先 new 出一个 Array[ArrayBuffer(), ArrayBuffer()]，ArrayBuffer() 的个数与参与 cogroup 的 RDD 个数相同。func 的逻辑是这样的：每当从 RDD a 中 shuffle 过来一个 \ record 就将其添加到 hashmap.get(Key) 对应的 Array 中的第一个 ArrayBuffer() 中，每当从 RDD b 中 shuffle 过来一个 record，就将其添加到对应的 Array 中的第二个 ArrayBuffer()。</p>

<p><img src="/images/blog/34-ShuffleCoGroup.png"></p>

<p>CoGroupedRDD => MappedValuesRDD 对应 mapValues() 操作，就是将 [ArrayBuffer(), ArrayBuffer()] 变成 [Iterable[V], Iterable[W]]。</p>

<h3>intersection(otherRDD) 和 join(otherRDD, numPartitions)</h3>

<p><img src="/images/blog/35-ShuffleIntersection.png"></p>

<p>join 这两个操作中均使用了 cogroup，所以 shuffle 的处理方式与 cogroup 一样。</p>

<p><img src="/images/blog/36-ShuffleJoin.png"></p>

<h3>sortByKey(ascending, numPartitions)</h3>

<p><img src="/images/blog/38-ShuffleSortByKey.png"></p>

<p>sortByKey() 中 ShuffledRDD => MapPartitionsRDD 的处理逻辑与 reduceByKey() 不太一样，没有使用 HashMap 和 func 来处理 fetch 过来的 records。</p>

<p>sortByKey() 中 ShuffledRDD => MapPartitionsRDD 的处理逻辑是：将 shuffle 过来的一个个 record 存放到一个 Array 里，然后按照 Key 来对 Array 中的 records 进行 sort。</p>

<h3>coalesce(numPartitions, shuffle = true)</h3>

<p><img src="/images/blog/41-ShuffleCoalesce.png"></p>

<p>coalesce() 虽然有 ShuffleDependency，但不需要对 shuffle 过来的 records 进行 aggregate，所以没有建立 HashMap。每 shuffle 一个 record，就直接流向 CoalescedRDD，进而流向 MappedRDD 中。</p>

<h2>基础数据结构</h2>

<p>HashMap 是 Spark shuffle read 过程中频繁使用的、用于 aggregate 的数据结构。Spark 设计了两种：
- 一种是全内存的 AppendOnlyMap
- 另一种是内存＋磁盘的 ExternalAppendOnlyMap。</p>

<p>下面我们来分析一下两者特性及内存使用情况。</p>

<h3>AppendOnlyMap</h3>

<p>AppendOnlyMap 的官方介绍是 A simple open hash table optimized for the append-only use case, where keys are never removed, but the value for each key may be changed。意思是类似 HashMap，但没有remove(key)方法。其实现原理很简单，开一个大 Object 数组，蓝色部分存储 Key，白色部分存储 Value。如下图：</p>

<p><img src="/images/blog/40-appendonlymap.png"></p>

<p>当要 put(K, V) 时，先 hash(K) 找存放位置，如果存放位置已经被占用，就使用 Quadratic probing 探测方法来找下一个空闲位置。对于图中的 K6 来说，第三次查找找到 K4 后面的空闲位置，放进去即可。get(K6) 的时候类似，找三次找到 K6，取出紧挨着的 V6，与先来的 value 做 func，结果重新放到 V6 的位置。</p>

<p>迭代 AppendOnlyMap 中的元素的时候，从前到后扫描输出。</p>

<p>如果 Array 的利用率达到 70%，那么就扩张一倍，并对所有 key 进行 rehash 后，重新排列每个 key 的位置。</p>

<p>AppendOnlyMap 还有一个 destructiveSortedIterator(): Iterator[(K, V)] 方法，可以返回 Array 中排序后的 (K, V) pairs。实现方法很简单：先将所有 (K, V) pairs compact 到 Array 的前端，并使得每个 (K, V) 占一个位置（原来占两个），之后直接调用 Array.sort() 排序，不过这样做会破坏数组（key 的位置变化了）。</p>

<h3>ExternalAppendOnlyMap</h3>

<p>相比 AppendOnlyMap，ExternalAppendOnlyMap 的实现略复杂，但逻辑其实很简单，类似 Hadoop MapReduce 中的 shuffle-merge-combine-sort 过程：</p>

<p>ExternalAppendOnlyMap 持有一个 AppendOnlyMap，shuffle 来的一个个 (K, V) record 先 insert 到 AppendOnlyMap 中，insert 过程与原始的 AppendOnlyMap 一模一样。</p>

<p>如果 AppendOnlyMap 快被装满时检查一下内存剩余空间是否可以够扩展，够就直接在内存中扩展，不够就 sort 一下 AppendOnlyMap，将其内部所有 records 都 spill 到磁盘上。</p>

<p><img src="/images/blog/39-ExternalAppendOnlyMap.png"></p>

<p>图中 spill 了 4 次，每次 spill 完在磁盘上生成一个 spilledMap 文件，然后重新 new 出来一个 AppendOnlyMap。</p>

<p>最后一个 (K, V) record insert 到 AppendOnlyMap 后，表示所有 shuffle 来的 records 都被放到了 ExternalAppendOnlyMap 中，但不表示 records 已经被处理完，因为每次 insert 的时候，新来的 record 只与 AppendOnlyMap 中的 records 进行 aggregate，并不是与所有的 records 进行 aggregate（一些 records 已经被 spill 到磁盘上了）。</p>

<p>因此当需要 aggregate 的最终结果时，需要对 AppendOnlyMap 和所有的 spilledMaps 进行全局 merge-aggregate。</p>

<p>全局 merge-aggregate 的流程也很简单：</p>

<p>先将 AppendOnlyMap 中的 records 进行 sort，形成 sortedMap。</p>

<p>然后利用 DestructiveSortedIterator 和 DiskMapIterator 分别从 sortedMap 和各个 spilledMap 读出一部分数据（StreamBuffer）放到 mergeHeap 里面。StreamBuffer 里面包含的 records 需要具有相同的 hash(key)，所以图中第一个 spilledMap 只读出前三个 records 进入 StreamBuffer。</p>

<p>mergeHeap 顾名思义就是使用堆排序不断提取出 hash(firstRecord.Key) 相同的 StreamBuffer，并将其一个个放入 mergeBuffers 中，放入的时候与已经存在于 mergeBuffers 中的 StreamBuffer 进行 merge-combine，第一个被放入 mergeBuffers 的 StreamBuffer 被称为 minBuffer，那么 minKey 就是 minBuffer 中第一个 record 的 key。</p>

<p>当 merge-combine 的时候，与 minKey 相同的 records 被 aggregate 一起，然后输出。整个 merge-combine 在 mergeBuffers 中结束后，StreamBuffer 剩余的 records 随着 StreamBuffer 重新进入 mergeHeap。</p>

<p>一旦某个 StreamBuffer 在 merge-combine 后变为空（里面的 records 都被输出了），那么会使用 DestructiveSortedIterator 或 DiskMapIterator 重新装填 hash(key) 相同的 records，然后再重新进入 mergeHeap。</p>

<p>整个 insert-merge-aggregate 的过程有三点需要进一步探讨一下：</p>

<p>内存剩余空间检测</p>

<p>与 Hadoop MapReduce 规定 reducer 中 70% 的空间可用于 shuffle-sort 类似，Spark 也规定 executor 中 spark.shuffle.memoryFraction * spark.shuffle.safetyFraction 的空间（默认是0.3 * 0.8）可用于 ExternalOnlyAppendMap。</p>

<p>Spark 略保守是不是？更保守的是这 24％ 的空间不是完全用于一个 ExternalOnlyAppendMap 的，而是由在 executor 上同时运行的所有 reducer 共享的。</p>

<p>为此，exectuor 专门持有一个 ShuffleMemroyMap: HashMap[threadId, occupiedMemory] 来监控每个 reducer 中 ExternalOnlyAppendMap 占用的内存量。</p>

<p>每当 AppendOnlyMap 要扩展时，都会计算 ShuffleMemroyMap 持有的所有 reducer 中的 AppendOnlyMap 已占用的内存 ＋ 扩展后的内存 是会否会大于内存限制，大于就会将 AppendOnlyMap spill 到磁盘。</p>

<p>有一点需要注意的是前 1000 个 records 进入 AppendOnlyMap 的时候不会启动是否要 spill 的检查，需要扩展时就直接在内存中扩展。</p>

<p>AppendOnlyMap 大小估计</p>

<p>为了获知 AppendOnlyMap 占用的内存空间，可以在每次扩展时都将 AppendOnlyMap reference 的所有 objects 大小都算一遍，然后加和，但这样做非常耗时。</p>

<p>所以 Spark 设计了粗略的估算算法，算法时间复杂度是 O(1)，核心思想是利用 AppendOnlyMap 中每次 insert-aggregate record 后 result 的大小变化及一共 insert 的 records 的个数来估算大小，具体见 SizeTrackingAppendOnlyMap 和 SizeEstimator。</p>

<p>Spill 过程</p>

<p>与 shuffle write 一样，在 spill records 到磁盘上的时候，会建立一个 buffer 缓冲区，大小仍为 spark.shuffle.file.buffer.kb ，默认是 32KB。</p>

<p>另外，由于 serializer 也会分配缓冲区用于序列化和反序列化，所以如果一次 serialize 的 records 过多的话缓冲区会变得很大。Spark 限制每次 serialize 的 records 个数为 spark.shuffle.spill.batchSize，默认是 10000。</p>

<h2>参考资料</h2>

<p>通过本章的介绍可以发现，相比 MapReduce 固定的 shuffle-combine-merge-reduce 策略，Spark 更加灵活，会根据不同的 transformation() 的语义去设计不同的 shuffle-aggregate 策略，再加上不同的内存数据结构来混搭出合理的执行流程。</p>

<p>参考资料：</p>

<ul>
<li>Spark Shuffle过程：<a href="https://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html">https://spark-internals.books.yourtion.com/markdown/4-shuffleDetails.html</a></li>
<li>Spark Shuffle进化史：<a href="http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/">http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark Core 消息队列机制]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/11/28/spark-core-message-queue-mechanism/"/>
    <updated>2016-11-28T14:31:03+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/11/28/spark-core-message-queue-mechanism</id>
    <content type="html"><![CDATA[<p>本文介绍Spark中的消息队列机制，首先SparkListenerEvent，SparkListener和SparkListenerBus等基本数据结构实现。</p>

<p>重点介绍了异步消息总线LiveListenerBus的实现。随后介绍了Spark消息队列的整体工作流程。</p>

<!--more-->


<h2>SparkListenerEvent</h2>

<p>Spark中的消息由SparkListenerEvent表示。其本身定义只是一个接口：</p>

<pre><code class="scala">trait SparkListenerEvent {
  /* Whether output this event to the event log */
  protected[spark] def logEvent: Boolean = true
}
</code></pre>

<p>SparkListenerEvent有多个具体的实现，每种实现代表了Spark运行过程中的一种事件。</p>

<ul>
<li>SparkListenerStageSubmitted</li>
<li>SparkListenerStageCompleted</li>
<li>SparkListenerTaskStart</li>
<li>SparkListenerTaskGettingResult</li>
<li>SparkListenerTaskEnd</li>
<li>SparkListenerJobStart</li>
<li>SparkListenerJobEnd</li>
<li>SparkListenerEnvironmentUpdate</li>
<li>SparkListenerBlockManagerAdded</li>
<li>SparkListenerBlockManagerRemoved</li>
<li>SparkListenerUnpersistRDD</li>
<li>SparkListenerExecutorAdded</li>
<li>SparkListenerExecutorRemoved</li>
<li>SparkListenerBlockUpdated</li>
<li>SparkListenerExecutorMetricsUpdate</li>
<li>SparkListenerApplicationStart</li>
<li>SparkListenerApplicationEnd</li>
<li>SparkListenerLogStart</li>
</ul>


<p>根据名称可以知道每一种事件代表的含义。</p>

<h2>SparkListener</h2>

<p>SparkListeners负责监听SparkListenerEvents。</p>

<p>所有Spark消息SparkListenerEvents 被异步的发送给已经注册过的SparkListeners。</p>

<p>SparkListenerInterface定义了SparkListener的接口：</p>

<ul>
<li>onStageCompleted</li>
<li>onStageSubmitted</li>
<li>onTaskStart</li>
<li>onTaskGettingResult</li>
<li>onTaskEnd</li>
<li>onJobStart</li>
<li>onJobEnd</li>
<li>onEnvironmentUpdate</li>
<li>onBlockManagerAdded</li>
<li>onBlockManagerRemoved</li>
<li>onUnpersistRDD</li>
<li>onApplicationStart</li>
<li>onApplicationEnd</li>
<li>onExecutorMetricsUpdate</li>
<li>onExecutorAdded</li>
<li>onExecutorRemoved</li>
<li>onBlockUpdated</li>
<li>onOtherEvent</li>
</ul>


<p>根据名称可以知道每一个方法是对应事件消息的响应函数。SparkListener的实现：</p>

<pre><code class="scala">abstract class SparkListener extends SparkListenerInterface {
  override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = { }
  override def onStageSubmitted(stageSubmitted: SparkListenerStageSubmitted): Unit = { }
  override def onTaskStart(taskStart: SparkListenerTaskStart): Unit = { }
  override def onTaskGettingResult(taskGettingResult: SparkListenerTaskGettingResult): Unit = { }
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = { }
  override def onJobStart(jobStart: SparkListenerJobStart): Unit = { }
  override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = { }
  override def onEnvironmentUpdate(environmentUpdate: SparkListenerEnvironmentUpdate): Unit = { }
  override def onBlockManagerAdded(blockManagerAdded: SparkListenerBlockManagerAdded): Unit = { }
  override def onBlockManagerRemoved(
      blockManagerRemoved: SparkListenerBlockManagerRemoved): Unit = { }
  override def onUnpersistRDD(unpersistRDD: SparkListenerUnpersistRDD): Unit = { }
  override def onApplicationStart(applicationStart: SparkListenerApplicationStart): Unit = { }
  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = { }
  override def onExecutorMetricsUpdate(
      executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = { }
  override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = { }
  override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = { }
  override def onBlockUpdated(blockUpdated: SparkListenerBlockUpdated): Unit = { }
  override def onOtherEvent(event: SparkListenerEvent): Unit = { }
}
</code></pre>

<p>Spark运行过程中会用到很多个SparkListener，每一种都有自己的用途。</p>

<p>例如EventLoggingListener用来将监听到的事件持久化到文件中，ExecutorAllocationListener用那个通知对应的ExecutorAllocationManager增加或移除executor等。</p>

<h2>SparkListenerBus</h2>

<p>SparkListener需要被注册到SparkListenerBus中才能起作用，SparkListenerBus负责分发监听到的Event给SparkListener。</p>

<p>SparkListenerBus继承自ListenerBus接口，并重载了doPostEvent方法。</p>

<pre><code class="scala">private[spark] trait SparkListenerBus
  extends ListenerBus[SparkListenerInterface, SparkListenerEvent] {

  protected override def doPostEvent(
      listener: SparkListenerInterface,
      event: SparkListenerEvent): Unit = {
    event match {
      case stageSubmitted: SparkListenerStageSubmitted =&gt;
        listener.onStageSubmitted(stageSubmitted)
      case stageCompleted: SparkListenerStageCompleted =&gt;
        listener.onStageCompleted(stageCompleted)
      case jobStart: SparkListenerJobStart =&gt;
        listener.onJobStart(jobStart)
      case jobEnd: SparkListenerJobEnd =&gt;
        listener.onJobEnd(jobEnd)
      ...
      case blockUpdated: SparkListenerBlockUpdated =&gt;
        listener.onBlockUpdated(blockUpdated)
      case logStart: SparkListenerLogStart =&gt; // ignore event log metadata
      case _ =&gt; listener.onOtherEvent(event)
    }
  }

}
</code></pre>

<p>该接口实现了消息的路由，根据事件类型调用相应的处理函数。</p>

<h3>ListenerBus</h3>

<p>ListenerBus接口的实现如下：</p>

<pre><code>private[spark] trait ListenerBus[L &lt;: AnyRef, E] extends Logging {

  // Marked `private[spark]` for access in tests.
  private[spark] val listeners = new CopyOnWriteArrayList[L]

  final def addListener(listener: L): Unit = {
    listeners.add(listener)
  }

  final def removeListener(listener: L): Unit = {
    listeners.remove(listener)
  }

  /**
   * Post the event to all registered listeners. The `postToAll` caller should guarantee calling
   * `postToAll` in the same thread for all events.
   */
  final def postToAll(event: E): Unit = {
    // JavaConverters can create a JIterableWrapper if we use asScala.
    // However, this method will be called frequently. To avoid the wrapper cost, here we use
    // Java Iterator directly.
    val iter = listeners.iterator
    while (iter.hasNext) {
      val listener = iter.next()
      try {
        doPostEvent(listener, event)
      } catch {
        case NonFatal(e) =&gt;
          logError(s"Listener ${Utils.getFormattedClassName(listener)} threw an exception", e)
      }
    }
  }

  /**
   * Post an event to the specified listener. `onPostEvent` is guaranteed to be called in the same
   * thread for all listeners.
   */
  protected def doPostEvent(listener: L, event: E): Unit

  private[spark] def findListenersByClass[T &lt;: L : ClassTag](): Seq[T] = {
    val c = implicitly[ClassTag[T]].runtimeClass
    listeners.asScala.filter(_.getClass == c).map(_.asInstanceOf[T]).toSeq
  }

}
</code></pre>

<p>本质上所有注册的Listener用一个数组记录下来，post操作就是根据事件找到对应的listener然后把event交给listener处理。</p>

<h3>LiveListenerBus</h3>

<p>SparkContext中会创建一个LiveListenerBus实例，LiveListenerBus是SparkListenerBus的一个具体实现，主要功能如下:</p>

<ul>
<li>保存有消息队列,负责消息的缓存</li>
<li>保存有注册过的listener,负责消息的分发</li>
</ul>


<p>消息队列用LinkBlockQueue实现：</p>

<pre><code class="scala">// Cap the capacity of the event queue so we get an explicit error (rather than
// an OOM exception) if it's perpetually being added to more quickly than it's being drained.
private lazy val EVENT_QUEUE_CAPACITY = validateAndGetQueueSize()
private lazy val eventQueue = new LinkedBlockingQueue[SparkListenerEvent](EVENT_QUEUE_CAPACITY)
</code></pre>

<p>事件队列的长度EVENT_QUEUE_CAPACITY由spark.scheduler.listenerbus.eventqueue.size参数配置，默认为10000。</p>

<p>当缓存事件数量达到上限后,新来的事件会被丢弃。</p>

<p>消息的产生和分发按照 <b><font color=red>生产者-消费者模型</font></b> 实现。</p>

<p><b><font color=red>消息的分发(消费者)</font></b> 是通过一个listener线程异步处理的，代码如下。</p>

<pre><code class="scala">private val listenerThread = new Thread(name) {  // &lt;-- 线程名为SparkListenerBus
  setDaemon(true)
  override def run(): Unit = Utils.tryOrStopSparkContext(sparkContext) {
    LiveListenerBus.withinListenerThread.withValue(true) {
      while (true) {
        eventLock.acquire()
        self.synchronized {
          processingEvent = true
        }
        try {
          val event = eventQueue.poll
          if (event == null) {
            // Get out of the while loop and shutdown the daemon thread
            if (!stopped.get) {
              throw new IllegalStateException("Polling `null` from eventQueue means" +
                " the listener bus has been stopped. So `stopped` must be true")
            }
            return
          }
          postToAll(event)
        } finally {
          self.synchronized {
            processingEvent = false
          }
        }
      }
    }
  }
}
</code></pre>

<p>为了保证生产者和消费者对消息队列的并发访问，在每次需要获取消息的时候,调用eventLock.acquire()来获取信号量, 信号量的值就是当前队列中所含有的事件数量。</p>

<p>如果正常获取到事件,就调用postToAll将事件分发给所有listener, 继续下一次循环。</p>

<p>如果获取到null值, 则有下面两种情况:</p>

<ul>
<li>整个application正常结束, 此时stopped值已经被设置为true。</li>
<li>系统发生了错误, 立即终止运行。</li>
</ul>


<p><font color=red><b>消息的产生(生产者)</font></b> 通过在Spark运行时调用LiveListenerBus的post方法来添加。实现如下：</p>

<pre><code class="scala">def post(event: SparkListenerEvent): Unit = {
  if (stopped.get) {
    // Drop further events to make `listenerThread` exit ASAP
    logError(s"$name has already stopped! Dropping event $event")
    return
  }
  val eventAdded = eventQueue.offer(event)  // &lt;-- 这里将新来的事件添加到消息队列中
  if (eventAdded) {
    eventLock.release()
  } else {
    onDropEvent(event)  // &lt;-- 没有添加成功，则丢弃事件
    droppedEventsCounter.incrementAndGet()
  }

  val droppedEvents = droppedEventsCounter.get
  if (droppedEvents &gt; 0) {
    // Don't log too frequently
    if (System.currentTimeMillis() - lastReportTimestamp &gt;= 60 * 1000) {
      // There may be multiple threads trying to decrease droppedEventsCounter.
      // Use "compareAndSet" to make sure only one thread can win.
      // And if another thread is increasing droppedEventsCounter, "compareAndSet" will fail and
      // then that thread will update it.
      if (droppedEventsCounter.compareAndSet(droppedEvents, 0)) {
        val prevLastReportTimestamp = lastReportTimestamp
        lastReportTimestamp = System.currentTimeMillis()
        logWarning(s"Dropped $droppedEvents SparkListenerEvents since " +
          new java.util.Date(prevLastReportTimestamp))
      }
    }
  }
}
</code></pre>

<p>每成功放入一个事件,就调用eventLock.release()来增加信号量额值，以供消费者线程来进行消费. 如果队列满了,就调用onDropEvent来处理。</p>

<h2>消息队列建立/发送流程</h2>

<p>在SparkContext中创建了LiveListenerBus类类型的成员变量listenerBus。</p>

<pre><code class="scala">// An asynchronous listener bus for Spark events
private[spark] val listenerBus = new LiveListenerBus(this)
随后创建各种listener，并注册到listenerBus中，通过调用listenerBus的start()方法启动消息分发流程。
private def setupAndStartListenerBus(): Unit = {
  // Use reflection to instantiate listeners specified via `spark.extraListeners`
  try {
    val listenerClassNames: Seq[String] =
      conf.get("spark.extraListeners", "").split(',').map(_.trim).filter(_ != "")  
    for (className &lt;- listenerClassNames) {  // &lt;-- 如果指定了额外的SparkListenr类，可通过反射机制实例化并注册到listenerBus
      // Use reflection to find the right constructor
      val constructors = {
        val listenerClass = Utils.classForName(className)
        listenerClass
            .getConstructors
            .asInstanceOf[Array[Constructor[_ &lt;: SparkListenerInterface]]]
      }
      val constructorTakingSparkConf = constructors.find { c =&gt;
        c.getParameterTypes.sameElements(Array(classOf[SparkConf]))
      }
      lazy val zeroArgumentConstructor = constructors.find { c =&gt;
        c.getParameterTypes.isEmpty
      }
      val listener: SparkListenerInterface = {
        if (constructorTakingSparkConf.isDefined) {
          constructorTakingSparkConf.get.newInstance(conf)
        } else if (zeroArgumentConstructor.isDefined) {
          zeroArgumentConstructor.get.newInstance()
        } else {
          ...
        }
      }
      listenerBus.addListener(listener)
      logInfo(s"Registered listener $className")
    }
  } catch {
    ...
  }

  listenerBus.start()
  _listenerBusStarted = true
}
</code></pre>

<p>其中，listenerBus.start() 实现如下：</p>

<pre><code class="scala">def start(): Unit = {
  if (started.compareAndSet(false, true)) {
    listenerThread.start()
  } else {
    throw new IllegalStateException(s"$name already started!")
  }
}
</code></pre>

<p>运行过程中产生的事件会post到listenerBus中。</p>

<p>当作业运行结束后会调用listenerBus.stop()来停止SparkListenerBus线程。</p>

<pre><code class="scala">def stop(): Unit = {
  if (!started.get()) {
    throw new IllegalStateException(s"Attempted to stop $name that has not yet started!")
  }
  if (stopped.compareAndSet(false, true)) {
    // Call eventLock.release() so that listenerThread will poll `null` from `eventQueue` and know
    // `stop` is called.
    eventLock.release()
    listenerThread.join()
  } else {
    // Keep quiet
  }
}
</code></pre>

<p>这里可以看到：</p>

<p><b><font color=red>在stop函数中调用了eventLock.release()来增加信号量的值. 然而并未向消息队列中加入新的消息。</p>

<p>这就导致在消费者线程listenerThread读取队列时会返回null值,进而达到结束listenerThread线程的目的。</font></b></p>

<p>以上就是Spark Core中消息队列机制的整体工作流程。</p>

<p><b>参考资料</b></p>

<ol>
<li>Spark 2.0 源码：<a href="https://github.com/apache/spark/tree/branch-2.0">https://github.com/apache/spark/tree/branch-2.0</a></li>
<li>Spark消息队列机制源码学习Blog：<a href="http://blog.csdn.net/sivolin/article/details/47316099">http://blog.csdn.net/sivolin/article/details/47316099</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark日志配置方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa/"/>
    <updated>2016-10-19T19:13:38+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa</id>
    <content type="html"><![CDATA[<p>本文介绍Spark日志级别控制方法。</p>

<p>Apache Spark 默认使用 log4j 作为日志工具。
Baidu Spark 根据不同的发布版本，使用 log4j 或者 logback 作为日志工具。</p>

<!--more-->


<h2>Driver日志级别设置</h2>

<p>日志配置默认在Spark客户端conf目录下，log4j配置log4j.properties文件，logback配置logback.xml文件。</p>

<h3>log4j配置示例</h3>

<p>示例配置文件log4j.properties如下：</p>

<pre><code class="bash"># Set everything to be logged to the console
log4j.rootCategory=INFO, DRFA
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
log4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.DRFA.File=log/spark.log
log4j.appender.DRFA.layout=org.apache.log4j.PatternLayout
log4j.appender.DRFA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} [%t] %p %c{1}: %m%n
# Settings to quiet third party logs that are too verbose
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
log4j.logger.org.apache.hadoop.fs.DfsInputStream=INFO
log4j.logger.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl=INFO
log4j.logger.org.apache.hadoop.hive.ql.io.*=DEBUG
</code></pre>

<p>其中：</p>

<ul>
<li>log4j.rootCategory配置总体的默认日志级别。</li>
<li>log4j.appender.DRFA.File用来配置日志重定向的目标文件。</li>
<li>log4j.appender.DRFA.layout.ConversionPattern用来配置日志的输出格式。</li>
<li>log4j.logger.xxx.xxx.xxx用来配置指定类的日志级别，xxx.xxx.xxx代表类路径。</li>
</ul>


<h3>logback配置示例</h3>

<p>示例配置文件logback.xml如下：</p>

<pre><code class="xml">&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
  &lt;appender name="RFILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
    &lt;file&gt;log/spark.log&lt;/file&gt;
    &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt;
      &lt;fileNamePattern&gt;log/spark.%d{yyyy-MM-dd}.log&lt;/fileNamePattern&gt;
      &lt;maxHistory&gt;30&lt;/maxHistory&gt;
    &lt;/rollingPolicy&gt;
    &lt;encoder&gt;
      &lt;pattern&gt; %date %level [%logger] - %msg%n&lt;/pattern&gt;
    &lt;/encoder&gt;
  &lt;/appender&gt;
  &lt;logger name="org.apache.hadoop.hive.ql.io.*"  level="INFO" /&gt;
  &lt;root level="info"&gt;
    &lt;appender-ref ref="RFILE" /&gt;
  &lt;/root&gt;
&lt;/configuration&gt;
</code></pre>

<p>其中：</p>

<ul>
<li><root>用来配置总体的默认日志级别。</li>
<li><file>用来配置日志重定向的目标文件。</li>
<li><pattern>用来配置日志的输出格式。</li>
<li><logger>用来配置指定类的日志级别。</li>
</ul>


<h2>Executor日志级别设置</h2>

<p>executor日志级别配置同driver端类似，但需要将executor的日志配置文件上传，并通过executor的Java参数指定使用的配置文件名。</p>

<h3>log4j配置示例</h3>

<p>配置好executor的log4j配置文件，命名随意，假设为executor-log4j.properties，放在conf目录下。</p>

<p>配置spark-conf.defaults文件，在 spark.executor.extraJavaOptions 中增加:</p>

<pre><code class="bash">spark.executor.extraJavaOptions -Dlog4j.configuration=executor-log4j.properties
</code></pre>

<p>启动Spark作业时增加–files参数。</p>

<pre><code class="bash">$spark-submit --files /path/to/conf/executor-log4j.properties xxx
则作业运行时的日志级别将由executor-log4j.properties文件控制。
</code></pre>

<h3>logback配置示例</h3>

<p>配置好executor的logback配置文件，命名随意，假设为executor-logback.xml，放在conf目录下。</p>

<p>配置spark-conf.defaults文件，在 spark.executor.extraJavaOptions 中增加:</p>

<pre><code class="bash">spark.executor.extraJavaOptions -Dlogback.configurationFile=executor-logback.xml
</code></pre>

<p>启动Spark作业时增加–files参数。</p>

<pre><code class="bash">$spark-submit --files /path/to/conf/executor-logback.xml xxx
</code></pre>

<p>则作业运行时的日志级别将由executor-logback.xml文件控制。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spark HistoryServer 配置和使用方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2016/08/24/spark-historyserver-configuration/"/>
    <updated>2016-08-24T17:17:58+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2016/08/24/spark-historyserver-configuration</id>
    <content type="html"><![CDATA[<p>本文介绍Spark History Server的配置和使用方法。</p>

<!--More-->


<h2>History Server 配置</h2>

<p>Spark提供了History Server服务可以保存历史Application的运行记录。</p>

<h3>客户端配置</h3>

<p>对于提交应用程序的客户端需要在conf/spark-defaults.conf中配置以下参数：</p>

<table>
<thead>
<tr>
<th> 参数        </th>
<th style="text-align:left;"> 功能           </th>
</tr>
</thead>
<tbody>
<tr>
<td> spark.eventLog.enabled      </td>
<td style="text-align:left;"> 是否记录Spark事件，用于应用程序在完成后重构webUI。 </td>
</tr>
<tr>
<td> spark.eventLog.dir      </td>
<td style="text-align:left;"> spark.eventLog.enabled为 true，该属性为记录spark事件的根目录。在此根目录中，Spark为每个应用程序创建分目录，并将应用程序的事件记录到此目录中。<br>可以将此属性设置为HDFS目录，以便history server读取历史记录文件。</br>      </td>
</tr>
<tr>
<td> spark.yarn.historyServer.address </td>
<td style="text-align:left;"> Spark history server的地址。 这个地址会在Spark应用程序完成后提交给YARN RM，然后RM将信息从RM UI写到history server UI上。<br><strong><font color=red>注意：hostname:port，前面不加http：//，末尾也不要加反斜杠。</font></strong> </br>     </td>
</tr>
</tbody>
</table>


<h3>服务端配置</h3>

<p>服务端主要需要在conf/spark-defaults.conf中配置如下属性：</p>

<table>
<thead>
<tr>
<th> 参数        </th>
<th style="text-align:left;"> 功能           </th>
<th style="text-align:left;"> 默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td> spark.history.ui.port     </td>
<td style="text-align:left;"> History Server的默认访问端口。<strong><font color=red>建议配置在8000~9000之间，以确保再内网浏览器能够正常显示。</strong><font color=red> </td>
<td style="text-align:left;"> 18080 </td>
</tr>
<tr>
<td> spark.history.fs.logDirectory    </td>
<td style="text-align:left;">   用于指定HistoryServer读取的eventlog存放的hdfs路径。  </td>
<td style="text-align:left;">无</td>
</tr>
<tr>
<td> spark.history.updateInterval </td>
<td style="text-align:left;">  History Server显示信息的刷新时间间隔，以秒为单位。每次更新都会检查持久层事件日志的任何变化。  </td>
<td style="text-align:left;"> 10 </td>
</tr>
<tr>
<td>spark.history.retainedApplications</td>
<td style="text-align:left;">在History Server上显示的最大应用程序数量，如果超过这个值，旧的应用程序信息将被删除。</td>
<td style="text-align:left;">250</td>
</tr>
</tbody>
</table>


<p>如果使用Kerberos认证可以配置如下参数：</p>

<table>
<thead>
<tr>
<th> 参数        </th>
<th style="text-align:left;"> 功能           </th>
<th style="text-align:left;"> 默认值</th>
</tr>
</thead>
<tbody>
<tr>
<td> spark.history.kerberos.enabled    </td>
<td style="text-align:left;"> 是否使用kerberos方式登录访问history server。</td>
<td style="text-align:left;">false</td>
</tr>
<tr>
<td>spark.history.kerberos.principal</td>
<td style="text-align:left;">spark.history.kerberos.enabled为true时使用，用于指定History Server的kerberos主体名称</td>
<td style="text-align:left;">空</td>
</tr>
<tr>
<td>spark.history.kerberos.keytab</td>
<td style="text-align:left;">spark.history.kerberos.enabled为true时使用，用于指定History Server的kerberos keytab文件位置</td>
<td style="text-align:left;">空</td>
</tr>
<tr>
<td>spark.history.ui.acls.enable</td>
<td style="text-align:left;">授权用户查看应用程序信息的时候是否检查acl。如果启用，无论应用程序的spark.ui.acls.enable怎么设置，都要进行授权检查。<br>只有应用程序所有者和spark.ui.view.acls指定的用户可以查看应用程序信息; 如果禁用，不做任何检查。</br></td>
<td style="text-align:left;">false</td>
</tr>
</tbody>
</table>


<p>另外，服务端可以配置以下环境变量：</p>

<table>
<thead>
<tr>
<th> 参数        </th>
<th style="text-align:left;"> 功能           </th>
</tr>
</thead>
<tbody>
<tr>
<td>SPARK_DAEMON_JAVA_OPTS</td>
<td style="text-align:left;">History Server的JVM参数，默认为空</td>
</tr>
<tr>
<td>SPARK_DAEMON_MEMORY</td>
<td style="text-align:left;">分配给History Server的内存大小，默认512M</td>
</tr>
<tr>
<td>SPARK_HISTORY_OPTS</td>
<td style="text-align:left;">History Server的属性设置，默认为空。</td>
</tr>
<tr>
<td>SPARK_PUBLIC_DNS</td>
<td style="text-align:left;">History Server的公网地址，如果不设置，可以用内网地址来访问，默认为空。</td>
</tr>
</tbody>
</table>


<h2>History Server 使用</h2>

<h3>启动</h3>

<p>一般将客户端运行生成的eventlog统一存放在一个HDFS路径下便于查询历史记录，然后History Server端 spark.history.fs.logDirectory的值设为该路径即可。</p>

<p>启动History Server命令：</p>

<pre><code>$sh sbin/start-history-server.sh
</code></pre>

<h3>停止</h3>

<p>停止History Server命令：</p>

<pre><code>$sh sbin/stop-history-server.sh
</code></pre>
]]></content>
  </entry>
  
</feed>
