<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tensorflow | Workspace of LionHeart]]></title>
  <link href="http://lionheartwang.github.io/blog/categories/tensorflow/atom.xml" rel="self"/>
  <link href="http://lionheartwang.github.io/"/>
  <updated>2018-06-05T12:14:43+08:00</updated>
  <id>http://lionheartwang.github.io/</id>
  <author>
    <name><![CDATA[Wang Yiguang]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tensorflow基础环境安装]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/05/17/tensorflowji-chu-huan-jing-an-zhuang/"/>
    <updated>2018-05-17T16:24:53+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/05/17/tensorflowji-chu-huan-jing-an-zhuang</id>
    <content type="html"><![CDATA[<p>本文介绍如何搭建tensorflow的基础运行环境，包括Cuda、Cudnn与tensorflow gpu版本的安装。</p>

<!-- More -->


<h2>安装Cuda</h2>

<p>在nvida官网寻找Cuda rpm安装包：</p>

<ul>
<li> <a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a></li>
</ul>


<p>选择对应版本和系统后下载。</p>

<p>以Cuda 9.0为例，下载后的安装包为：</p>

<ul>
<li>cuda-repo-rhel7-9-0-local-9.0.176-1.x86_64.rpm</li>
</ul>


<p>执行如下命令安装：</p>

<pre><code class="bash">sudo rpm -i cuda-repo-rhel7-9-0-local-9.0.176-1.x86_64.rpm
sudo yum clean all
sudo yum install cuda
</code></pre>

<p>安装nvida-driver：</p>

<pre><code class="bash">sudo yum install -y nvidia-kmod cuda-drivers
</code></pre>

<p>重新加载nvidia相关模块：</p>

<pre><code class="bash">sudo rmmod nvidia-uvm
sudo rmmod nvidia
sudo nvidia-modprobe -u -c=0
</code></pre>

<p>检验新的驱动是否生效，运行</p>

<pre><code class="bash">nvidia-smi
</code></pre>

<p>应当输出本机GPU信息。</p>

<h2>安装Cudnn</h2>

<p>在nvidia官网寻找Cudnn安装包：</p>

<ul>
<li><a href="https://developer.nvidia.com/rdp/cudnn-download">https://developer.nvidia.com/rdp/cudnn-download</a></li>
</ul>


<p>根据Cuda版本与系统选择对应版本下载。</p>

<p>以Cudnn 7.0为例，下载后的安装包为：</p>

<ul>
<li>cudnn-9.0-linux-x64-v7.tgz</li>
</ul>


<p>解压安装操作如下：</p>

<pre><code class="bash">tar zxvf cudnn-9.0-linux-x64-v7.tgz
sudo cp cuda/include/cudnn.h /usr/local/cuda/include
sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
</code></pre>

<p>设置环境变量：</p>

<pre><code>export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
</code></pre>

<h2>安装Tensorflow</h2>

<p>通过pip安装GPU版本的tensorflow。</p>

<pre><code>pip install tensorflow-gpu
</code></pre>

<p>在python中测试tensorflow是否能够正常加载。</p>

<pre><code class="python">import tensorflow as tf
print tf.__version__
</code></pre>

<p>目前最新版本为1.8.0</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tensorflow架构介绍]]></title>
    <link href="http://lionheartwang.github.io/blog/2018/03/25/tensorflowjia-gou-jie-shao/"/>
    <updated>2018-03-25T17:43:48+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2018/03/25/tensorflowjia-gou-jie-shao</id>
    <content type="html"><![CDATA[<p>Tensorflow的架构为大规模分布式训练和预测设计，单页为实验新的机器学习模型和进行系统层面上的优化提供了足够多的灵活性。</p>

<p>本文介绍Tensorflow的系统架构设计，展示Tensorflow如何将灵活性与扩展性结合。</p>

<p>本文内容主要从Tensorflow官方文档中翻译整理而得。</p>

<ul>
<li><a href="https://www.tensorflow.org/extend/architecture">https://www.tensorflow.org/extend/architecture</a></li>
</ul>


<p>阅读本文之前需要了解Tenflow变成中像计算流图、算子和session的概念。如果不了解可以参考如下文档：</p>

<ul>
<li><a href="https://www.tensorflow.org/programmers_guide/low_level_intro">https://www.tensorflow.org/programmers_guide/low_level_intro</a></li>
</ul>


<!-- More -->


<h2>概述</h2>

<p>Tensorflow的运行时是一个跨平台的库，下图展示了整体结构。用户级别的代码与核心的运行时的代码通过一套C API分隔开。</p>

<p><img src="/images/blog/43-tensorflow-layers.png"></p>

<p>本文主要介绍如下几层：</p>

<ul>
<li>Client:

<ul>
<li>将计算定义为数据流图的形式。</li>
<li>通过一个session初始化图的执行。</li>
</ul>
</li>
<li>Distributed Master

<ul>
<li>根据Session.run()传递的参数将图裁剪为子图。</li>
<li>将子图拆解为多个部分用于分发到不通的进程和设备上。</li>
<li>分发子图分片到Worker Services</li>
<li>初始化Woker Service待执行的子图分片计算。</li>
</ul>
</li>
<li>Worker Services (每个task一个)

<ul>
<li>调度子图算子的执行，这些算子会调用对应于可用硬件（CPU\GPU等）的kernal实现。</li>
<li>发送算子计算结果以及接收来自其他Worker Service的计算结果。</li>
</ul>
</li>
<li>Kernel Implementations

<ul>
<li>负责实现每个图算子的执行。</li>
</ul>
</li>
</ul>


<p>下图说明了这些组件是如何交互的。</p>

<p><img src="/images/blog/44-tensorflow-components-interact.svg"></p>

<p>解释如下：</p>

<ul>
<li>&ldquo;/job:worker/task:0&rdquo; 和 &ldquo;/job:ps/task:0&rdquo; 都是worker services上的task。</li>
<li>“PS”代表parameter server，是负责存储和更新模型参数的task。其他的task会将训练过程中优化的参数更新发送给parameter server。</li>
<li>这样的职能划分并非必须的，但在分布式训练场景中很常见。</li>
</ul>


<p>注意</p>

<ul>
<li>Distributed Master和Worker Service只在支持分布式版本的tensorflow中存在。</li>
<li>单进程版本的Tensorflow包含一个特殊的Session实现完成Distributed Master做的全部事情但只在本地进程中进行设备间的通信。</li>
</ul>


<h2>组件介绍</h2>

<p>下面以一个示例图的处理过程介绍TensorFlow核心层的更多实现细节。</p>

<h3>Client</h3>

<p>用户编写客户端tensorflow程序来构建计算流图。</p>

<p>客户端程序既可以直接调用各种算子操作组合而成，也可以调用更方便的API库如Estimator等直接构建神经网络和其他高层抽象实现。</p>

<p>Tensorflow支持多种客户端语言，主流的是Python和C++，因为Google的内部使用者对这两种语言更熟悉一些。</p>

<p>大多数训练库仍为Python实现，但C++更高效。</p>

<p>Client会创建一个session，将图的定义以<code>tf.GraphDef</code>这种protobuffer的形式发送给Distributed Master节点。</p>

<p>当client对流图中的一个或多个节点进行求值时，会触发调用让Distributed Master进行计算的初始化操作。</p>

<p>在下图中，Client构建了一个图将权值w附加到一个特征向量x并增加了偏移b，然后在变量中存储计算结果。</p>

<p><img src="/images/blog/45-tensorflow-graph_client.svg"></p>

<p> tf.Session相关代码：</p>

<ul>
<li><a href="https://www.tensorflow.org/api_docs/python/tf/Session">https://www.tensorflow.org/api_docs/python/tf/Session</a></li>
</ul>


<h3>Distributed Master</h3>

<p>Distributed Master负责以下工作：</p>

<ul>
<li>对图进行裁剪来获取满足Client对节点求值必须计算的子图。</li>
<li>将图进行拆分得到每个参与计算的设备负责的图分片。</li>
<li>缓存这些拆分的图分片，以备后续步骤中被重新使用。</li>
</ul>


<p>master节点能够看到每步计算的整体情况，因此可以对计算流程进行一些标准的优化，例如公共子表达式求值、常量折叠等。</p>

<p>随后master会负责协调一系列的task执行来计算优化后的子图。</p>

<p><img src="/images/blog/46-tensorflow-graph_master_cln.svg"></p>

<p>下图展示了示例graph的一种可能的分发方案。Distributed Master将模型参数进行分组从而在Parameter Server中将他们存储在一起。</p>

<p><img src="/images/blog/47-tensorflow-graph_split1.svg"></p>

<p>在图的边被切分的分片中，Distributed Master会插入send和receive节点来在不同的task间传递信息。</p>

<p><img src="/images/blog/48-tensorflow-graph_split2.svg"></p>

<p> 随后Distributed Master将图的分片分发给不同节点上的task。</p>

<p> <img src="/images/blog/50-tensorflow-graph_send_recv.svg"></p>

<p>相关代码：</p>

<ul>
<li>MasterService API 定义：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/protobuf/master_service.proto">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/protobuf/master_service.proto</a></li>
<li>Master接口：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/master_interface.h">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/master_interface.h</a></li>
</ul>


<h3>Worker Service</h3>

<p>每个task的Worker Service负责如下工作：</p>

<ul>
<li>处理Distributed Master发来的请求。</li>
<li>调度执行构成本地局部子图的算子的kernels实现。</li>
<li>协调task之间的直接通信。</li>
</ul>


<p>我们优化了Worker Service，支持以较低的开销运行大规模的流图。</p>

<p>我们目前的实现能够支持每秒数以万计的子图执行，这通常能够支持大量副本细粒度地进行训练。</p>

<p>Worker Service将算子的kernel实现分发给设备并尽可能并行化，例如使用多CPU核或者GPU。</p>

<p>我们针对每对源设备和目的设备的组合情况，定制了相应的Send和Recv算子。包括：</p>

<ul>
<li>本地CPU和GPU设备的数据交换使用cudaMemcpyAsync() API来配合计算和数据转移工作。</li>
<li>两个本地GPU使用端到端的DMA技术来避免通过本机CPU进行代价昂贵的数据拷贝操作。</li>
</ul>


<p>对于task之间的数据交换，tensorflow支持多种协议，包括：</p>

<ul>
<li>基于TCP协议的gRPC。</li>
<li>基于融合以太网的RDMA。</li>
</ul>


<p>另外我们也初步支持了NVIDIA为多GPU通信提供的NCCL库。</p>

<p>相关代码：</p>

<ul>
<li>WorkerService API 定义：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/protobuf/worker_service.proto">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/protobuf/worker_service.proto</a></li>
<li>Worker接口：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/worker_interface.h">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/worker_interface.h</a></li>
<li>远程通信相关(Send和Recv算子实现)：<a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/rpc/rpc_rendezvous_mgr.h">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/distributed_runtime/rpc/rpc_rendezvous_mgr.h</a></li>
</ul>


<h3>Kernel</h3>

<p>Tensorflow运行时包含了超过200种标准的算子实现，包括数学相关、数组相关、控制流相关以及状态管理相关的算子。</p>

<p>每一种算子在实现上都针对不同的设备种类进行了优化。</p>

<p>其中，许多算子是使用<code>Eigen::Tensor</code>实现的，它基于C++模板生成了高效的能在多核CPU以及GPU上执行的代码。</p>

<p>另一方面，我们也使用想cuDNN等第三方库尽可能地让kernel的实现更加高效。</p>

<p>如果现有算子无法满足需求，用户可以注册自定义的算子实现到kernel中。</p>

<p>相关代码：</p>

<ul>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/framework/op_kernel.h">https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/core/framework/op_kernel.h</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tensorflow模型保存与加载方法]]></title>
    <link href="http://lionheartwang.github.io/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa/"/>
    <updated>2017-12-10T16:57:13+08:00</updated>
    <id>http://lionheartwang.github.io/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa</id>
    <content type="html"><![CDATA[<p>本文档介绍如何保存和读取Tensorflow变量和模型。</p>

<p>官方文档可参考：</p>

<ul>
<li><a href="https://www.tensorflow.org/programmers_guide/saved_model">https://www.tensorflow.org/programmers_guide/saved_model</a></li>
</ul>


<!--More-->


<h2>保存/读取变量</h2>

<p>本节介绍如何存取Tensorflow变量。注意Estimators会自动在model_dir中存取变量。</p>

<p><code>tf.train.Saver</code> 提供了存取模型的接口，其构造函数会在图中增加存取模型的op。</p>

<p>示例化的Saver对象提供方法来运行这些op并，设置checkpoint文件用于保存和恢复模型信息。</p>

<p>Saver关于将模型中定义的所有变量保存，如果你不了解加载模型的graph结构，可以参考后面的保存恢复模型一节。</p>

<p>TensorFlow将变量以二进制形式存储在文件中，保存的信息主要是变量名以及对应的Tensor的值。</p>

<h3>保存变量</h3>

<p>调用tf.train.Saver()来管理模型变量，示例代码：</p>

<pre><code class="python"># Create some variables.
v1 = tf.get_variable("v1", shape=[3], initializer = tf.zeros_initializer)
v2 = tf.get_variable("v2", shape=[5], initializer = tf.zeros_initializer)

inc_v1 = v1.assign(v1+1)
dec_v2 = v2.assign(v2-1)

# Add an op to initialize the variables.
init_op = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, initialize the variables, do some work, and save the
# variables to disk.
with tf.Session() as sess:
  sess.run(init_op)
  # Do some work with the model.
  inc_v1.op.run()
  dec_v2.op.run()
  # Save the variables to disk.
  save_path = saver.save(sess, "/tmp/model.ckpt")
  print("Model saved in file: %s" % save_path)
</code></pre>

<h3>恢复变量</h3>

<p>同样使用tf.train.Saver来从checkpoint中恢复变量，示例代码：</p>

<pre><code class="python">tf.reset_default_graph()

# Create some variables.
v1 = tf.get_variable("v1", shape=[3])
v2 = tf.get_variable("v2", shape=[5])

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Later, launch the model, use the saver to restore variables from disk, and
# do some work with the model.
with tf.Session() as sess:
  # Restore variables from disk.
  saver.restore(sess, "/tmp/model.ckpt")
  print("Model restored.")
  # Check the values of the variables
  print("v1 : %s" % v1.eval())
  print("v2 : %s" % v2.eval())
</code></pre>

<h3>存取指定的变量</h3>

<p>有时可能只需要存取模型graph中的部分变量，可以list或者dict两种形式传给tf.train.Saver()来指定需要存取的变量。</p>

<ul>
<li>list形式，模型的变量列表。</li>
<li>dict形式，name为新的保存后的变量名，value为模型中的变量名。</li>
</ul>


<p>示例代码：</p>

<pre><code class="python">tf.reset_default_graph()
# Create some variables.
v1 = tf.get_variable("v1", [3], initializer = tf.zeros_initializer)
v2 = tf.get_variable("v2", [5], initializer = tf.zeros_initializer)

# Add ops to save and restore only `v2` using the name "v2"
saver = tf.train.Saver({"v2": v2})

# Use the saver object normally after that.
with tf.Session() as sess:
  # Initialize v1 since the saver will not.
  v1.initializer.run()
  saver.restore(sess, "/tmp/model.ckpt")

  print("v1 : %s" % v1.eval())
  print("v2 : %s" % v2.eval())
</code></pre>

<p>注意：</p>

<ul>
<li>可以创建多个模型Saver对象来保存模型变量，同样的变量可被不同的Saver保存多次，restore操作用于从checkpoint中恢复变量的值。</li>
<li>如果checkpoint中只保存了部分变量，那么恢复后，graph中其他的变量仍需要被初始化。</li>
<li>查看checkpoint中的变量可以使用inspect_checkpoint库，特别是print_tensors_in_checkpoint_file函数。</li>
<li>Saver默认使用tf.Variable.name属性作为每个变量的变量名，然而你可以在Saver对象中未变量指定存储在checkpoint中的新名字。</li>
</ul>


<h2>保存/读取模型概述</h2>

<p>当你想要保存整个模型(变量、模型graph以及graph的meta信息)时，我们推荐使用<code>SavedModel</code>。</p>

<p>SavedModel是一种面向多语言的，可恢复的高度序列化封装的格式。</p>

<p>SavedModel运行上层系统或工具来生产、消费或者转换Tensorflow模型。</p>

<p>Tensorflow提供了多种机制来同SavedModel进行交互，包括tf.saved_model API, Estimator API 以及 CLI方式。</p>

<h2>操作SavedModel API</h2>

<p>本节聚焦在使用底层Tensorflow API时需要用到的保存或加载SavedModel的API。</p>

<h3>构建SavedModel</h3>

<p>我们提供了SavedModel builder的python实现。SavedModelBuilder提供保存MetaGraphDef结构的功能。</p>

<ul>
<li>MetaGraphDef是MetaGraph的proto buffer表达形式。</li>
<li>MetaGraph是一个数据流图，以及相关的变量、资源和signatures。</li>
<li>signature是一个graph的输入与输出的集合。</li>
</ul>


<p>每个加入到 SavedModel中的MetaGraphDef需要以用户指定的tag标注。tag提供了区分特定MetaGraphDef的方法。通常这些tag会标注MetaGraphDef的功能以及一些可选的硬件相关的信息。</p>

<p>SavedModelBuilder的使用示例代码如下：</p>

<pre><code class="python">export_dir = ...
...
builder = tf.saved_model_builder.SavedModelBuilder(export_dir)
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph_and_variables(sess,
                                       [tag_constants.TRAINING],
                                       signature_def_map=foo_signatures,
                                       assets_collection=foo_assets)
...
# Add a second MetaGraphDef for inference.
with tf.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph([tag_constants.SERVING])
...
builder.save()
</code></pre>

<h3>加载SaveModel</h3>

<p>调用python版本的SaveModel loader需要提供一下信息：</p>

<ul>
<li>保存graph定义和变量的session。</li>
<li>用来标识MetaGraphDef的tag。</li>
<li>SavedModel对应的目录位置。</li>
</ul>


<p>示例代码如下：</p>

<pre><code class="python">export_dir = ...
...
with tf.Session(graph=tf.Graph()) as sess:
  tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)
  ...
</code></pre>

<p>C++版本的SavedModel loader也提供从指定目录恢复模型的API，并支持指定SessionOptions和RunOptions参数。</p>

<p>同样需要指定taq参数，被加载的SavedModel被SavedModelBundle引用，包含了MetaGraphDef以及加载它的session信息。</p>

<p>示例代码：</p>

<pre><code class="c++">const string export_dir = ...
SavedModelBundle bundle;
...
LoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain},
               &amp;bundle);
</code></pre>

<p>另外Tensorflow提供了一组MetaGraphDef和SignatureDef相关的常量供用户使用。</p>

<p>MetaGraphDef常量：</p>

<ul>
<li>python：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/tag_constants.py">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/tag_constants.py</a></li>
<li>c++：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/tag_constants.h">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/tag_constants.h</a></li>
</ul>


<p>SignatureDef常量</p>

<ul>
<li>python：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/signature_constants.py">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/signature_constants.py</a></li>
<li>c++：<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/signature_constants.h">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/saved_model/signature_constants.h</a></li>
</ul>


<h2>在Estimators中使用SaveModel</h2>

<p>当训练好Estimator模型后，你可能需要部署预测服务。你可以选择在本机启动一个本地服务，或在云端进行扩展。</p>

<p>部署Estimator训练出的模型需要先把模型导出为SaveModel格式，本节介绍：</p>

<ul>
<li>如何制定输出节点以及相关的API</li>
<li>使用SavedModel导出模型</li>
<li>发请求给本地模型预测服务</li>
</ul>


<h3>准备Serving输入</h3>

<p>训练过程中 <code>input_fn()</code>用于提供数据输入，类似地，预测阶段输入数据由 <code>serving_input_receiver_fn</code>提供。</p>

<p>serving_input_receiver_fn有如下两个功能：</p>

<ul>
<li>将预测需要输入数据的placeholder添加到graph中。</li>
<li>添加额外的用于将输入数据格式转换为feature Tensors格式的op。</li>
</ul>


<p>函数返回 <code>tf.estimator.export.ServingInputReceiver</code>对象，封装了placeholders以及feature Tensor。</p>

<p>当编写 <code>serving_input_receiver_fn</code>时，需要提供一个 tf.parse_example的特定parser描述来说明数据解析的方式。</p>

<p>Parser说明是一个dict的形式，包含：</p>

<ul>
<li>tf.FixedLenFeature</li>
<li>tf.VarLenFeature</li>
<li>tf.SparseFeature</li>
</ul>


<p>示例代码：</p>

<pre><code class="python">feature_spec = {'foo': tf.FixedLenFeature(...),
                'bar': tf.VarLenFeature(...)}

def serving_input_receiver_fn():
  """An input receiver that expects a serialized tf.Example."""
  serialized_tf_example = tf.placeholder(dtype=tf.string,
                                         shape=[default_batch_size],
                                         name='input_example_tensor')
  receiver_tensors = {'examples': serialized_tf_example}
  features = tf.parse_example(serialized_tf_example, feature_spec)
  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)
</code></pre>

<p>tf.estimator.export.build_parsing_serving_input_receiver_fn 工具函数给出了一个通用实现。</p>

<h3>导出模型</h3>

<p>调用 <code>tf.estimator.Estimator.export_savedmodel</code> ，提供导出路径以及serving_input_receiver_fn进行模型导出：</p>

<pre><code class="python">estimator.export_savedmodel(export_dir_base, serving_input_receiver_fn)
</code></pre>

<p>该方法创建一个新的graph并调用serving_input_receiver_fn来获取输入tensor，随后调用Estimator的<code>model_fn()</code> 来产生模型的graph。</p>

<p>最终会创建一个带时间戳的目录(export_dir_base/<timestamp>)并将模型导出为SavedModel。</p>

<h3>指定模型输出</h3>

<p>通过export_outputs指定，其类型为 <code>tf.estimator.EstimatorSpec</code>，是一个形如 {name: output} 的dict，用于描述预测阶段的输出。</p>

<p>预测输出的值类型必须为 <code>ExportOutput</code> 的某个实现，例如：</p>

<ul>
<li>tf.estimator.export.ClassificationOutput,</li>
<li>tf.estimator.export.RegressionOutput</li>
<li>tf.estimator.export.PredictOutput.</li>
</ul>


<h3>部署本地预测服务</h3>

<p>本地部署预测服务需要使用TensorFlow Serving。</p>

<p>TensorFlow Serving是一个独立的开源项目，功能是加载SavedModel模型并对外提供gRPC服务。</p>

<p>首先安装Tensorflow Serving。</p>

<p>部署服务命令如下，将 <code>$export_dir_base</code> 替换为SavedModel导出的目录。</p>

<pre><code class="bash">bazel build //tensorflow_serving/model_servers:tensorflow_model_server
bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_base_path=$export_dir_base
</code></pre>

<p>执行后在9000端口会启动一个gRPC预测服务。</p>

<h3>向本地Server发送请求</h3>

<p>发送预测请求需要通过PredictionService gRPC API。相关API依赖：</p>

<pre><code class="bash">  deps = [
    "//tensorflow_serving/apis:classification_proto_py_pb2",
    "//tensorflow_serving/apis:regression_proto_py_pb2",
    "//tensorflow_serving/apis:predict_proto_py_pb2",
    "//tensorflow_serving/apis:prediction_service_proto_py_pb2"
  ]
</code></pre>

<p>python代码中可以像如下示例使用：</p>

<pre><code class="python">from tensorflow_serving.apis import classification_pb2
from tensorflow_serving.apis import regression_pb2
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2
</code></pre>

<p>请求的数据会以proto buffer的形式发送，发送请求的示例代码：</p>

<pre><code class="python">from grpc.beta import implementations

channel = implementations.insecure_channel(host, int(port))
stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)

request = classification_pb2.ClassificationRequest()
example = request.input.example_list.examples.add()
example.features.feature['x'].float_list.value.extend(image[0].astype(float))

result = stub.Classify(request, 10.0)  # 10 secs timeout
</code></pre>

<p>本例中的返回值是一个 <code>ClassificationResponse</code>格式的PB数据。</p>

<h2>SavedModel目录结构</h2>

<p>Tensorflow为每个SavedModel组织目录结构形式如下：</p>

<pre><code class="bash">assets/
assets.extra/
variables/
    variables.data-?????-of-?????
    variables.index
saved_model.pb|saved_model.pbtxt
</code></pre>

<p>说明如下：</p>

<ul>
<li>assets：是一个子目录，包含了一些外部文件，例如词表等，这些资源文件会被特定的MetaGraphDef读取使用。</li>
<li>assets.extra：是一个子目录，用于上层应用或者用户添加一些自己的资源文件，但不会被模型的graph加载。该目录不由SavedModel管理。</li>
<li>variables：是一个子目录，用于存储tf.train.Saver的输出。</li>
<li>saved_model.pb/saved_model.pbtxt SavedModel的Proto Buffer描述。包含了MetaGraphDef的proto buffer形式的定义。</li>
</ul>


<p>一个单独的SavedModel可以表达多个graph，SavedModel的多个graph共享一组checkpoint(变量和资源文件)。</p>

<p>组织形式如下图所示：</p>

<p><img src="/images/blog/10-savemodel.png"></p>
]]></content>
  </entry>
  
</feed>
