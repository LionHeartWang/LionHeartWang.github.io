
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Spark日志配置方法 - Workspace of LionHeart</title>
  <meta name="author" content="Wang Yiguang">

  
  <meta name="description" content="Spark日志配置方法 Oct 19th, 2016 7:13 pm 导航[-] 本文介绍Spark日志级别控制方法。 Apache Spark 默认使用 log4j 作为日志工具。
Baidu Spark 根据不同的发布版本，使用 log4j 或者 logback 作为日志工具。 &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://lionheartwang.github.io/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Workspace of LionHeart" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<link href='https://fonts.googleapis.com/css?family=Tillana:400,500,600,700,800|Trade+Winds|Henny+Penny|Mountains+of+Christmas:400,700|Nothing+You+Could+Do' rel='stylesheet' type='text/css'>

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Workspace of LionHeart</a></h1>
  
    <h2>Focus on Spark/TensorFlow and other BigData and ML platforms</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://www.google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="lionheartwang.github.io">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/bigdata">大数据</a></li>
  <li><a href="/machinelearning">机器学习</a></li>
  <li><a href="/algorithm">算法</a></li>
  <!--li><a href="/">Blog</a></li-->
  <!--li><a href="/blog/archives">Archives</a></li-->
  <!--li><a href="/team">Team</a></li-->
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Spark日志配置方法</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2016-10-19T19:13:38+08:00'><span class='date'><span class='date-month'>Oct</span> <span class='date-day'>19</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>7:13 pm</span></time>
        
      </p>
    
  </header>


  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<div class="blogAnchor">
  <p id="anchorContentToggle" title="收起">导航[-]</p>
  <div class="anchorContent" id="anchorContent"> </div>
</div>

  <div class="entry-content"><p>本文介绍Spark日志级别控制方法。</p>

<p>Apache Spark 默认使用 log4j 作为日志工具。
Baidu Spark 根据不同的发布版本，使用 log4j 或者 logback 作为日志工具。</p>

<!--more-->


<h2>Driver日志级别设置</h2>

<p>日志配置默认在Spark客户端conf目录下，log4j配置log4j.properties文件，logback配置logback.xml文件。</p>

<h3>log4j配置示例</h3>

<p>示例配置文件log4j.properties如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># Set everything to be logged to the console</span>
</span><span class='line'>log4j.rootCategory<span class="o">=</span>INFO, DRFA
</span><span class='line'>log4j.appender.console<span class="o">=</span>org.apache.log4j.ConsoleAppender
</span><span class='line'>log4j.appender.console.target<span class="o">=</span>System.err
</span><span class='line'>log4j.appender.console.layout<span class="o">=</span>org.apache.log4j.PatternLayout
</span><span class='line'>log4j.appender.console.layout.ConversionPattern<span class="o">=</span>%d<span class="o">{</span>yy/MM/dd HH:mm:ss<span class="o">}</span> %p %c<span class="o">{</span>1<span class="o">}</span>: %m%n
</span><span class='line'>log4j.appender.DRFA<span class="o">=</span>org.apache.log4j.DailyRollingFileAppender
</span><span class='line'>log4j.appender.DRFA.File<span class="o">=</span>log/spark.log
</span><span class='line'>log4j.appender.DRFA.layout<span class="o">=</span>org.apache.log4j.PatternLayout
</span><span class='line'>log4j.appender.DRFA.layout.ConversionPattern<span class="o">=</span>%d<span class="o">{</span>yy/MM/dd HH:mm:ss<span class="o">}</span> <span class="o">[</span>%t<span class="o">]</span> %p %c<span class="o">{</span>1<span class="o">}</span>: %m%n
</span><span class='line'><span class="c"># Settings to quiet third party logs that are too verbose</span>
</span><span class='line'>log4j.logger.org.eclipse.jetty<span class="o">=</span>WARN
</span><span class='line'>log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle<span class="o">=</span>ERROR
</span><span class='line'>log4j.logger.org.apache.spark.repl.SparkIMain<span class="nv">$exprTyper</span><span class="o">=</span>INFO
</span><span class='line'>log4j.logger.org.apache.spark.repl.SparkILoop<span class="nv">$SparkILoopInterpreter</span><span class="o">=</span>INFO
</span><span class='line'>log4j.logger.org.apache.hadoop.fs.DfsInputStream<span class="o">=</span>INFO
</span><span class='line'>log4j.logger.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl<span class="o">=</span>INFO
</span><span class='line'>log4j.logger.org.apache.hadoop.hive.ql.io.*<span class="o">=</span>DEBUG
</span></code></pre></td></tr></table></div></figure>


<p>其中：</p>

<ul>
<li>log4j.rootCategory配置总体的默认日志级别。</li>
<li>log4j.appender.DRFA.File用来配置日志重定向的目标文件。</li>
<li>log4j.appender.DRFA.layout.ConversionPattern用来配置日志的输出格式。</li>
<li>log4j.logger.xxx.xxx.xxx用来配置指定类的日志级别，xxx.xxx.xxx代表类路径。</li>
</ul>


<h3>logback配置示例</h3>

<p>示例配置文件logback.xml如下：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="cp">&lt;?xml version=&quot;1.0&quot;?&gt;</span>
</span><span class='line'><span class="cp">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span>
</span><span class='line'><span class="nt">&lt;configuration&gt;</span>
</span><span class='line'>  <span class="nt">&lt;appender</span> <span class="na">name=</span><span class="s">&quot;RFILE&quot;</span> <span class="na">class=</span><span class="s">&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;file&gt;</span>log/spark.log<span class="nt">&lt;/file&gt;</span>
</span><span class='line'>    <span class="nt">&lt;rollingPolicy</span> <span class="na">class=</span><span class="s">&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>      <span class="nt">&lt;fileNamePattern&gt;</span>log/spark.%d{yyyy-MM-dd}.log<span class="nt">&lt;/fileNamePattern&gt;</span>
</span><span class='line'>      <span class="nt">&lt;maxHistory&gt;</span>30<span class="nt">&lt;/maxHistory&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/rollingPolicy&gt;</span>
</span><span class='line'>    <span class="nt">&lt;encoder&gt;</span>
</span><span class='line'>      <span class="nt">&lt;pattern&gt;</span> %date %level [%logger] - %msg%n<span class="nt">&lt;/pattern&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/encoder&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/appender&gt;</span>
</span><span class='line'>  <span class="nt">&lt;logger</span> <span class="na">name=</span><span class="s">&quot;org.apache.hadoop.hive.ql.io.*&quot;</span>  <span class="na">level=</span><span class="s">&quot;INFO&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;root</span> <span class="na">level=</span><span class="s">&quot;info&quot;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;appender-ref</span> <span class="na">ref=</span><span class="s">&quot;RFILE&quot;</span> <span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/root&gt;</span>
</span><span class='line'><span class="nt">&lt;/configuration&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>其中：</p>

<ul>
<li><root>用来配置总体的默认日志级别。</li>
<li><file>用来配置日志重定向的目标文件。</li>
<li><pattern>用来配置日志的输出格式。</li>
<li><logger>用来配置指定类的日志级别。</li>
</ul>


<h2>Executor日志级别设置</h2>

<p>executor日志级别配置同driver端类似，但需要将executor的日志配置文件上传，并通过executor的Java参数指定使用的配置文件名。</p>

<h3>log4j配置示例</h3>

<p>配置好executor的log4j配置文件，命名随意，假设为executor-log4j.properties，放在conf目录下。</p>

<p>配置spark-conf.defaults文件，在 spark.executor.extraJavaOptions 中增加:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>spark.executor.extraJavaOptions -Dlog4j.configuration<span class="o">=</span>executor-log4j.properties
</span></code></pre></td></tr></table></div></figure>


<p>启动Spark作业时增加–files参数。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$spark</span>-submit --files /path/to/conf/executor-log4j.properties xxx
</span><span class='line'>则作业运行时的日志级别将由executor-log4j.properties文件控制。
</span></code></pre></td></tr></table></div></figure>


<h3>logback配置示例</h3>

<p>配置好executor的logback配置文件，命名随意，假设为executor-logback.xml，放在conf目录下。</p>

<p>配置spark-conf.defaults文件，在 spark.executor.extraJavaOptions 中增加:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>spark.executor.extraJavaOptions -Dlogback.configurationFile<span class="o">=</span>executor-logback.xml
</span></code></pre></td></tr></table></div></figure>


<p>启动Spark作业时增加–files参数。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="nv">$spark</span>-submit --files /path/to/conf/executor-logback.xml xxx
</span></code></pre></td></tr></table></div></figure>


<p>则作业运行时的日志级别将由executor-logback.xml文件控制。</p>
</div>
  <script src="/javascripts/blogDirectory.js" type="text/javascript"></script>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Wang Yiguang</span></span>

      




<time class='entry-date' datetime='2016-10-19T19:13:38+08:00'><span class='date'><span class='date-month'>Oct</span> <span class='date-day'>19</span><span class='date-suffix'>th</span>, <span class='date-year'>2016</span></span> <span class='time'>7:13 pm</span></time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/spark/'>spark</a>, <a class='category' href='/blog/categories/大数据/'>大数据</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://lionheartwang.github.io/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa/" data-via="" data-counturl="http://lionheartwang.github.io/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2016/09/13/javascriptshi-xian-bo-wen-mu-lu-zi-dong-sheng-cheng/" title="Previous Post: JavaScript实现博文目录自动生成">&laquo; JavaScript实现博文目录自动生成</a>
      
      
        <a class="basic-alignment right" href="/blog/2016/11/28/spark-core-message-queue-mechanism/" title="Next Post: Spark Core 消息队列机制">Spark Core 消息队列机制 &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section id="recent_posts">
  <h1>Recent Posts</h1>
  
	  <p><a href="/blog/2018/03/11/spark-shuffle-implementation/">Spark Shuffle Implementation</a></p>
  
	  <p><a href="/blog/2018/03/05/flink-framwork-introduction/">Flink架构及工作原理介绍</a></p>
  
	  <p><a href="/blog/2018/02/08/da-jian-rayji-qun-bu-zou/">搭建Ray集群步骤</a></p>
  
	  <p><a href="/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa/">Tensorflow模型保存与加载方法</a></p>
  
	  <p><a href="/blog/2017/11/30/on-line-random-forest-paper/">论文阅读: On-line Random Forest</a></p>
  
</section>

<section id="categories_aside">
  <h1>Categories</h1>
  
	<p><a href='/blog/categories/技术资源'>技术资源 <span>(12)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2017/11/26/shi-yong-landslideji-yu-markdownzhi-zuo-zai-xian-slide/">使用Landslide基于MarkDown制作在线Slide</a> 26/11/2017</li>
    
		<li><a href="/blog/2017/11/14/gua-zai-hdfsdao-ben-di-mu-lu-fang-fa/">使用Fuse挂载HDFS到本地目录方法</a> 14/11/2017</li>
    
		<li><a href="/blog/2017/10/24/linux-nfs-configuration/">Linux NFS配置及使用方法</a> 24/10/2017</li>
    
		<li><a href="/blog/2017/06/18/dockerchang-yong-ming-ling-jie-shao/">Docker常用命令介绍</a> 18/06/2017</li>
    
		<li><a href="/blog/2017/04/17/remote-debug-via-intelij/">使用InteliJ远程调试程序</a> 17/04/2017</li>
    
		<li><a href="/blog/2017/03/14/intall-thrift-on-mac/">Mac安装Thrift方法</a> 14/03/2017</li>
    
		<li><a href="/blog/2017/01/20/zookeeperan-zhuang-shi-yong-zhi-nan/">ZooKeeper安装使用指南</a> 20/01/2017</li>
    
		<li><a href="/blog/2016/08/18/no-pass-world-ssh-method/">免密码远程登录/拷贝方法</a> 18/08/2016</li>
    
		<li><a href="/blog/2016/07/06/mysqlchang-yong-cao-zuo-fang-fa/">mysql常用操作方法</a> 06/07/2016</li>
    
		<li><a href="/blog/2016/05/27/linux-shellpei-se-zhi-nan/">Linux Shell配色指南</a> 27/05/2016</li>
    
		<li><a href="/blog/2016/04/07/fei-rootzhang-hu-kuai-su-an-zhuang-mysql/">非Root账户快速安装mySQL</a> 07/04/2016</li>
    
		<li><a href="/blog/2016/02/03/%E4%BD%BF%E7%94%A8Octopress%E6%90%AD%E5%BB%BAGitHub%E5%8D%9A%E5%AE%A2/">使用Octopress搭建Github博客</a> 03/02/2016</li>
    
  </ul-->
  
	<p><a href='/blog/categories/设计模式'>设计模式 <span>(1)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2016/02/05/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%A6%82%E8%BF%B0/">设计模式概述</a> 05/02/2016</li>
    
  </ul-->
  
	<p><a href='/blog/categories/读书笔记'>读书笔记 <span>(4)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2016/03/12/jian-suo-mo-xing-yu-sou-suo-pai-xu/">检索模型与搜索排序</a> 12/03/2016</li>
    
		<li><a href="/blog/2016/02/20/sou-suo-yin-qing-suo-yin/">搜索引擎索引</a> 20/02/2016</li>
    
		<li><a href="/blog/2016/02/08/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%8F%8A%E5%85%B6%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/">搜索引擎及其技术架构</a> 08/02/2016</li>
    
		<li><a href="/blog/2016/02/05/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%A6%82%E8%BF%B0/">设计模式概述</a> 05/02/2016</li>
    
  </ul-->
  
	<p><a href='/blog/categories/搜索引擎'>搜索引擎 <span>(3)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2016/03/12/jian-suo-mo-xing-yu-sou-suo-pai-xu/">检索模型与搜索排序</a> 12/03/2016</li>
    
		<li><a href="/blog/2016/02/20/sou-suo-yin-qing-suo-yin/">搜索引擎索引</a> 20/02/2016</li>
    
		<li><a href="/blog/2016/02/08/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E5%8F%8A%E5%85%B6%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84/">搜索引擎及其技术架构</a> 08/02/2016</li>
    
  </ul-->
  
	<p><a href='/blog/categories/spark'>spark <span>(4)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2018/03/11/spark-shuffle-implementation/">Spark Shuffle Implementation</a> 11/03/2018</li>
    
		<li><a href="/blog/2016/11/28/spark-core-message-queue-mechanism/">Spark Core 消息队列机制</a> 28/11/2016</li>
    
		<li><a href="/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa/">Spark日志配置方法</a> 19/10/2016</li>
    
		<li><a href="/blog/2016/08/24/spark-historyserver-configuration/">Spark HistoryServer 配置和使用方法</a> 24/08/2016</li>
    
  </ul-->
  
	<p><a href='/blog/categories/大数据'>大数据 <span>(8)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2018/03/11/spark-shuffle-implementation/">Spark Shuffle Implementation</a> 11/03/2018</li>
    
		<li><a href="/blog/2018/03/05/flink-framwork-introduction/">Flink架构及工作原理介绍</a> 05/03/2018</li>
    
		<li><a href="/blog/2018/02/08/da-jian-rayji-qun-bu-zou/">搭建Ray集群步骤</a> 08/02/2018</li>
    
		<li><a href="/blog/2017/11/14/gua-zai-hdfsdao-ben-di-mu-lu-fang-fa/">使用Fuse挂载HDFS到本地目录方法</a> 14/11/2017</li>
    
		<li><a href="/blog/2017/01/20/zookeeperan-zhuang-shi-yong-zhi-nan/">ZooKeeper安装使用指南</a> 20/01/2017</li>
    
		<li><a href="/blog/2016/11/28/spark-core-message-queue-mechanism/">Spark Core 消息队列机制</a> 28/11/2016</li>
    
		<li><a href="/blog/2016/10/19/sparkri-zhi-pei-zhi-fang-fa/">Spark日志配置方法</a> 19/10/2016</li>
    
		<li><a href="/blog/2016/08/24/spark-historyserver-configuration/">Spark HistoryServer 配置和使用方法</a> 24/08/2016</li>
    
  </ul-->
  
	<p><a href='/blog/categories/javascript'>javascript <span>(1)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2016/09/13/javascriptshi-xian-bo-wen-mu-lu-zi-dong-sheng-cheng/">JavaScript实现博文目录自动生成</a> 13/09/2016</li>
    
  </ul-->
  
	<p><a href='/blog/categories/前端'>前端 <span>(1)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2016/09/13/javascriptshi-xian-bo-wen-mu-lu-zi-dong-sheng-cheng/">JavaScript实现博文目录自动生成</a> 13/09/2016</li>
    
  </ul-->
  
	<p><a href='/blog/categories/论文笔记'>论文笔记 <span>(1)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2017/11/30/on-line-random-forest-paper/">论文阅读: On-line Random Forest</a> 30/11/2017</li>
    
  </ul-->
  
	<p><a href='/blog/categories/机器学习'>机器学习 <span>(2)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa/">Tensorflow模型保存与加载方法</a> 10/12/2017</li>
    
		<li><a href="/blog/2017/11/30/on-line-random-forest-paper/">论文阅读: On-line Random Forest</a> 30/11/2017</li>
    
  </ul-->
  
	<p><a href='/blog/categories/tensorflow'>tensorflow <span>(1)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2017/12/10/tensorflowmo-xing-bao-cun-yu-jia-zai-fang-fa/">Tensorflow模型保存与加载方法</a> 10/12/2017</li>
    
  </ul-->
  
	<p><a href='/blog/categories/ray'>ray <span>(1)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2018/02/08/da-jian-rayji-qun-bu-zou/">搭建Ray集群步骤</a> 08/02/2018</li>
    
  </ul-->
  
	<p><a href='/blog/categories/flink'>flink <span>(1)</span></a></p>
  <!--ul class="arc-list">
    
		<li><a href="/blog/2018/03/05/flink-framwork-introduction/">Flink架构及工作原理介绍</a> 05/03/2018</li>
    
  </ul-->
  
</section>




  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2018 - Wang Yiguang -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

<script src="/javascripts/libs/blogDirectory.js" type="text/javascript"></script>
</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
